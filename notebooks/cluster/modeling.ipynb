{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modeling: Cluster Start Here\n",
        "============================\n",
        "\n",
        "This script models an example strong lens on the 'cluster' scale, where there is a Brightest Cluster Galaxy (BCG),\n",
        "large dark matter halo, 20 extra galaxies in the cluster whose collective mass contributes significantly to the\n",
        "ray-tracing and 5 background source galaxies.\n",
        "\n",
        "The primary method for modeling cluster scale strong lenses uses `point` source modeling, where each source is modeled\n",
        "as a point source, where the positions of its multiple images are fitted (but not the extended emission observed at a\n",
        "pixel level).\n",
        "\n",
        "__Scaling Relations__\n",
        "\n",
        "This example models the mass of the cluster galaxies by putting them on a scaling relation which links light (measured\n",
        "luminosity) to mass. This means the number of dimensions of the model does not increase as we add more and more\n",
        "galaxies to the cluster lens model. Given the largest clusters have 100+ galaxies, this avoids our model complexity\n",
        "blowing up to 100 of free parameter and is therefore key.\n",
        "\n",
        "__Example__\n",
        "\n",
        "This script fits a `PointDataset` dataset of a 'cluster-scale' strong lens where:\n",
        "\n",
        " - There is a main Brightest Cluster Galaxy lens whose total mass distribution is an `Isothermal` and `ExternalShear`.\n",
        " - There is a large scale dark matter halo modeled as an `NFWSph`.\n",
        " - There are ten extra lens galaxies in the cluster whose total mass distributions are `DPIEPotential` models where\n",
        "   their mass is linked to their light via a scaling relation.\n",
        " - There are 5 source galaxies modeled as point sources.\n",
        "\n",
        "The point-source dataset used in this example consists of the positions of every lensed source's multiple images\n",
        "(their fluxes are not used).\n",
        "\n",
        "__Plotters__\n",
        "\n",
        "To produce images of the data `Plotter` objects are used, which are high-level wrappers of matplotlib\n",
        "code which produce high quality visualization of strong lenses.\n",
        "\n",
        "The `PLotter` API is described in the script `autolens_workspace/*/plot/start_here.py`.\n",
        "\n",
        "__Simulation__\n",
        "\n",
        "This script fits a simulated cluster dataset of a strong lens, which is produced in the\n",
        "script `autolens_workspace/*/cluster/simulator.py`\n",
        "\n",
        "__Data Preparation__\n",
        "\n",
        "The `Imaging` dataset fitted in this example confirms to a number of standard that make it suitable to be fitted in\n",
        "**PyAutoLens**.\n",
        "\n",
        "If you are intending to fit your own strong lens data, you will need to ensure it conforms to these standards, which are\n",
        "described in the script `autolens_workspace/*/data_preparation/imaging/start_here.ipynb`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import autofit as af\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "Load the strong lens dataset `cluster`, which is the dataset we will use to perform lens modeling.\n",
        "\n",
        "We begin by loading a CCD image of the dataset. Although we perform point-source modeling and will not use this data in \n",
        "the model-fit, it is useful to load it for visualization. By passing this dataset to the model-fit at the\n",
        "end of the script it will be used when visualizing the results. \n",
        "\n",
        "The use of an image in this way is entirely optional, and if it were not included in the model-fit visualization would \n",
        "performed without the image.\n",
        "\n",
        "This is loaded via .fits files, which is a data format used by astronomers to store images.\n",
        "\n",
        "The `pixel_scales` define the arc-second to pixel conversion factor of the image, which for the dataset we are using \n",
        "is 0.1\" / pixel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"simple\"\n",
        "dataset_path = Path(\"dataset\", \"cluster\", dataset_name)\n",
        "\n",
        "data = al.Array2D.from_fits(file_path=dataset_path / \"data.fits\", pixel_scales=0.1)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now load the point source datasets we will fit using point source modeling. \n",
        "\n",
        "We load this data as a list of `PointDataset` object, which contains the positions of every point source. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_list = []\n",
        "\n",
        "for i in range(5):\n",
        "\n",
        "    dataset = al.from_json(\n",
        "        file_path=Path(dataset_path, f\"point_dataset_{i}.json\"),\n",
        "    )\n",
        "\n",
        "    dataset_list.append(dataset)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can print this dictionary to see the dataset's `name` and `positions` and noise-map values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for dataset in dataset_list:\n",
        "\n",
        "    print(\"Point Dataset Info:\")\n",
        "    print(dataset.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can plot the positions of each dataset over the observed image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "positions_list = []\n",
        "\n",
        "for dataset in dataset_list:\n",
        "\n",
        "    positions_list.append(dataset.positions)\n",
        "\n",
        "visuals = aplt.Visuals2D(positions=positions_list)\n",
        "\n",
        "array_plotter = aplt.Array2DPlotter(array=data, visuals_2d=visuals)\n",
        "array_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also just plot the positions, omitting the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grid_plotter = aplt.Grid2DPlotter(grid=positions_list)\n",
        "grid_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Centres__\n",
        "\n",
        "The centre of every extra lens galaxy is used to compose the lens model, fixing their mass distributions\n",
        "to their centres of light.\n",
        "\n",
        "We load these centres below and plot them on the image to confirm they are located correctly and\n",
        "cover all galaxies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "extra_galaxies_centre_list = al.Grid2DIrregular(\n",
        "    al.from_json(file_path=Path(dataset_path, \"extra_galaxies_centre_list.json\"))\n",
        ")\n",
        "\n",
        "visuals = aplt.Visuals2D(light_profile_centres=extra_galaxies_centre_list)\n",
        "\n",
        "array_plotter = aplt.Array2DPlotter(array=data, visuals_2d=visuals)\n",
        "array_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Luminosities__\n",
        "\n",
        "We also need the luminosity of each galaxy, which in this example is the measured property we relate to mass via\n",
        "the scaling relation.\n",
        "\n",
        "We again uses the true values of the luminosities from the simulated dataset, but in a real analysis we would have\n",
        "to determine these luminosities beforehand (see discussion above).\n",
        "\n",
        "This could be other measured properties, like stellar mass or velocity dispersion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "extra_galaxies_luminosity_list = al.from_json(\n",
        "    file_path=Path(dataset_path, \"extra_galaxies_luminosities.json\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Point Solver__\n",
        "\n",
        "For point-source modeling we require a `PointSolver`, which determines the multiple-images of the mass model for a \n",
        "point source at location (y,x) in the source plane. \n",
        "\n",
        "It does this by ray tracing triangles from the image-plane to the source-plane and calculating if the \n",
        "source-plane (y,x) centre is inside the triangle. The method gradually ray-traces smaller and smaller triangles so \n",
        "that the multiple images can be determine with sub-pixel precision.\n",
        "\n",
        "The `PointSolver` requires an initial grid of (y, x) coordinates in the image plane (defined above), which defines the \n",
        "first set of triangles to ray trace spanning the whole cluster.It also requires that a `pixel_scale_precision` is input, \n",
        "which is the resolution up to which the multiple images are computed. The lower the `pixel_scale_precision`, the\n",
        "longer the calculation, with the value of 0.001 below balancing efficiency with precision.\n",
        "\n",
        "Strong lens mass models have a multiple image called the \"central image\". However, the image is nearly always \n",
        "significantly demagnified, meaning that it is not observed and cannot constrain the lens model. As this image is a\n",
        "valid multiple image, the `PointSolver` will locate it irrespective of whether its so demagnified it is not observed.\n",
        "To ensure this does not occur, we set a `magnification_threshold=0.1`, which discards this image because its\n",
        "magnification will be well below this threshold.\n",
        "\n",
        "If your dataset contains a central image that is observed you should reduce to include it in\n",
        "the analysis.\n",
        "\n",
        "__Chi Squared__\n",
        "\n",
        "For point-source modeling, there are many different ways to define the likelihood function, broadly referred to a\n",
        "an `image-plane chi-squared` or `source-plane chi-squared`. This determines whether the multiple images of the point\n",
        "source are used to compute the likelihood in the source-plane or image-plane.\n",
        "\n",
        "We will use an \"image-plane chi-squared\", which uses the `PointSolver` to determine the multiple images of the point\n",
        "source in the image-plane for the given mass model and compares the positions of these model images to the observed\n",
        "images to compute the chi-squared and likelihood.\n",
        "\n",
        "The `point_source` package provides full details of how the `PointSolver` works and the different\n",
        "chi squared definitions available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grid = al.Grid2D.uniform(\n",
        "    shape_native=(100, 100),\n",
        "    pixel_scales=1.0,  # <- The pixel-scale describes the conversion from pixel units to arc-seconds.\n",
        ")\n",
        "\n",
        "solver = al.PointSolver.for_grid(\n",
        "    grid=grid, pixel_scale_precision=0.001, magnification_threshold=0.1, xp=jnp\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Main Galaxies and Extra Galaxies__\n",
        "\n",
        "For a cluster-scale lens, we designate there to be the following lensing objects in the system:\n",
        "\n",
        " - `main_galaxies`: The main lens galaxies which are the brightest and highest mass galaxies in the lens system. In\n",
        " clusters they are often BCGs. These are modeled individually with a unique name for each, with their mass distributions \n",
        " modeled using parametric models. The cluster scale dark matter halo is also tied to the BCG.\n",
        " \n",
        " - `extra_galaxies`: The extra galaxies which make up the cluster, whose masses individually don't contirbute too much\n",
        " lensing but they collectively contribute to the lensing of the source galaxies a lot. These are modeled with a\n",
        "  more restrictive model, for example with their centres fixed to the observed centre of light and their mass \n",
        "  distributions modeled using a scaling relation. These are grouped into a single  `extra_galaxies` collection.\n",
        "  \n",
        "In this simple example cluster scale lens, there is one main lens galaxy and ten extra galaxies. \n",
        "\n",
        "for point source modeling, we do not model the light of the lens galaxies, as it is not necessary when only the \n",
        "positions of the multiple images are used to fit the model.\n",
        "\n",
        "__Centres__\n",
        "\n",
        "If the centres of the extra galaxies are treated as free parameters, there are too many \n",
        "parameters and the model may not be fitted accurately.\n",
        "\n",
        "For cluster-scale lenses we therefore manually specify the centres of the extra galaxies (which we loaded above) which \n",
        "are fixed to the observed centres of light of the galaxies.\n",
        "\n",
        "In a real analysis, one must determine the centres of the galaxies before modeling them, which can be done as follows:\n",
        "\n",
        " - Use the GUI tool in the `data_preparation/point_source/gui/extra_galaxies_centre_list.py` script to determine the centres\n",
        "   of the extra galaxies. \n",
        "\n",
        " - Use image processing software like Source Extractor (https://sextractor.readthedocs.io/en/latest/).\n",
        "\n",
        " - Fit every galaxy individually with a parametric light profile (e.g. an `Sersic`).\n",
        "\n",
        "__Redshifts__\n",
        "\n",
        "In this example all galaxies are at the same redshift in the image-plane, meaning multi-plane lensing is not used.\n",
        "\n",
        "If you have redshift information on the line of sight galaxies and some of their redshifts are different to the lens\n",
        "galaxy, you can easily extend this example below to perform multi-plane lensing.\n",
        "\n",
        "You would simply define a `redshift_list` and use this to set up the extra `Galaxy` redshifts.\n",
        "\n",
        "__Model__\n",
        "\n",
        "We compose a lens model where:\n",
        "\n",
        " - The main lens galaxy's total mass distribution is an `Isothermal` and `ExternalShear`  with a large\n",
        " - `NFWSph` dark matter halo [9 parameters].\n",
        " \n",
        " - There are ten extra lens galaxies with `DPIEPotentialSph` total mass distributions, with centres fixed to the \n",
        "   observed centres of light and masses linked to light via a scaling relation whose parameters are fitted \n",
        "   for [3 parameters].\n",
        " \n",
        " - There are five source galaxies whose light is a `Point` [10 parameters].\n",
        "\n",
        "The number of free parameters and therefore the dimensionality of non-linear parameter space is N=22."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Main Lens:\n",
        "\n",
        "lens_centre = (0.0, 0.0)\n",
        "\n",
        "mass = af.Model(al.mp.Isothermal)\n",
        "\n",
        "mass.centre.centre_0 = af.GaussianPrior(mean=lens_centre[0], sigma=0.3)\n",
        "mass.centre.centre_1 = af.GaussianPrior(mean=lens_centre[1], sigma=0.3)\n",
        "\n",
        "shear = af.Model(al.mp.ExternalShear)\n",
        "dark = af.Model(al.mp.NFWSph)\n",
        "\n",
        "dark.centre.centre_0 = af.GaussianPrior(mean=lens_centre[0], sigma=0.3)\n",
        "dark.centre.centre_1 = af.GaussianPrior(mean=lens_centre[1], sigma=0.3)\n",
        "\n",
        "lens = af.Model(al.Galaxy, redshift=0.5, mass=mass, shear=shear, dark=dark)\n",
        "\n",
        "# Extra Galaxies\n",
        "\n",
        "ra_star = af.LogUniformPrior(lower_limit=1e8, upper_limit=1e11)\n",
        "rs_star = af.UniformPrior(lower_limit=-1.0, upper_limit=1.0)\n",
        "b0_star = af.LogUniformPrior(lower_limit=1e5, upper_limit=1e7)\n",
        "luminosity_star = 1e9\n",
        "\n",
        "extra_galaxies_dict = {}\n",
        "\n",
        "for i, extra_galaxy_centre, extra_galaxy_luminosity in enumerate(\n",
        "    zip(extra_galaxies_centre_list, extra_galaxies_luminosity_list)\n",
        "):\n",
        "\n",
        "    mass = af.Model(al.mp.dPIEMassSph)\n",
        "    mass.centre = extra_galaxy_centre\n",
        "    mass.ra = ra_star * (extra_galaxy_luminosity / luminosity_star) ** 0.5\n",
        "    mass.rs = rs_star * (extra_galaxy_luminosity / luminosity_star) ** 0.5\n",
        "    mass.b0 = b0_star * (extra_galaxy_luminosity / luminosity_star) ** 0.25\n",
        "\n",
        "    extra_galaxy = af.Model(al.Galaxy, redshift=0.5, mass=mass)\n",
        "\n",
        "    extra_galaxies_dict[f\"extra_galaxy_{i}\"] = extra_galaxy\n",
        "\n",
        "# Source:\n",
        "\n",
        "source_galaxies_dict = {}\n",
        "\n",
        "for i, positions in enumerate(positions_list):\n",
        "\n",
        "    positions_centre_y = np.mean(positions, axis=0)\n",
        "    positions_centre_x = np.mean(positions, axis=1)\n",
        "\n",
        "    point = af.Model(al.ps.Point)\n",
        "    point.centre_0 = af.GaussianPrior(mean=positions_centre_y, sigma=3.0)\n",
        "    point.centre_1 = af.GaussianPrior(mean=positions_centre_x, sigma=3.0)\n",
        "\n",
        "    source = af.Model(al.Galaxy, redshift=1.0, **{f\"point_{i}\": point})\n",
        "\n",
        "    source_galaxies_dict[f\"source_{i}\"] = source\n",
        "\n",
        "# Overall Lens Model:\n",
        "\n",
        "model = af.Collection(\n",
        "    galaxies=af.Collection(lens=lens, **source_galaxies_dict),\n",
        "    extra_galaxies=af.Collection(**extra_galaxies_dict),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `info` attribute shows the model in a readable format.\n",
        "\n",
        "This shows the cluster scale model, with separate entries for the main lens galaxy, the source galaxies and the \n",
        "extra galaxies.\n",
        "\n",
        "The `info` below may not display optimally on your computer screen, for example the whitespace between parameter\n",
        "names on the left and parameter priors on the right may lead them to appear across multiple lines. This is a\n",
        "common issue in Jupyter notebooks.\n",
        "\n",
        "The`info_whitespace_length` parameter in the file `config/general.yaml` in the [output] section can be changed to \n",
        "increase or decrease the amount of whitespace (The Jupyter notebook kernel will need to be reset for this change to \n",
        "appear in a notebook)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Name Pairing__\n",
        "\n",
        "Every point-source dataset in the `PointDataset` has a name, (e.g. `point_0`, `point_1`). This `name` pairs \n",
        "the dataset to the `Point` in the model below. Because the name of the dataset is `point_0`, the \n",
        "only `Point` object that is used to fit it must have the name `point_0`.\n",
        "\n",
        "If there is no point-source in the model that has the same name as a `PointDataset`, that data is not used in\n",
        "the model-fit. If a point-source is included in the model whose name has no corresponding entry in \n",
        "the `PointDataset` **PyAutoLens** will raise an error.\n",
        "\n",
        "In cluster lenses, point-source datasets may have many source galaxies in them, and name pairing is necessary to \n",
        "ensure every point source in the lens model is  fitted to its particular lensed images in the `PointDataset`!\n",
        "\n",
        "The model fitting default settings assume that the BCG lens galaxy centre is near the coordinates (0.0\", 0.0\"). \n",
        "\n",
        "If for your dataset the  lens is not centred at (0.0\", 0.0\"), we recommend that you either: \n",
        "\n",
        " - Reduce your data so that the centre is (`autolens_workspace/*/data_preparation`). \n",
        " - Manually override the lens model priors (`autolens_workspace/*/guides/modeling/customize`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "The lens model is fitted to the data using the nested sampling algorithm Nautilus (see `start.here.py` for a \n",
        "full description).\n",
        "\n",
        "The folders: \n",
        "\n",
        " - `autolens_workspace/*/guides/modeling/searches`.\n",
        " - `autolens_workspace/*/guides/modeling/customize`\n",
        "  \n",
        "Give overviews of the non-linear searches **PyAutoLens** supports and more details on how to customize the\n",
        "model-fit, including the priors on the model.\n",
        "\n",
        "The `name` and `path_prefix` below specify the path where results ae stored in the output folder:  \n",
        "\n",
        " `/autolens_workspace/output/group/simple/mass[sie]_source[point]/unique_identifier`.\n",
        "\n",
        "__Unique Identifier__\n",
        "\n",
        "In the path above, the `unique_identifier` appears as a collection of characters, where this identifier is generated \n",
        "based on the model, search and dataset that are used in the fit.\n",
        "\n",
        "An identical combination of model, search and dataset generates the same identifier, meaning that rerunning the\n",
        "script will use the existing results to resume the model-fit. In contrast, if you change the model, search or dataset,\n",
        "a new unique identifier will be generated, ensuring that the model-fit results are output into a separate folder. \n",
        "\n",
        "__Parallel Script__\n",
        "\n",
        "Depending on the operating system (e.g. Linux, Mac, Windows), Python version, if you are running a Jupyter notebook \n",
        "and other factors, this script may not run a successful parallel fit (e.g. running the script \n",
        "with `number_of_cores` > 1 will produce an error). It is also common for Jupyter notebooks to not run in parallel \n",
        "correctly, requiring a Python script to be run, often from a command line terminal.\n",
        "\n",
        "To fix these issues, the Python script needs to be adapted to use an `if __name__ == \"__main__\":` API, as this allows\n",
        "the Python `multiprocessing` module to allocate threads and jobs correctly. An adaptation of this example script \n",
        "is provided at `autolens_workspace/scripts/guides/modeling/customize`, which will hopefully run \n",
        "successfully in parallel on your computer!\n",
        "\n",
        "Therefore if paralellization for this script doesn't work, check out the `parallel.py` example. You will need to update\n",
        "all scripts you run to use the this format and API. \n",
        "\n",
        "__Iterations Per Update__\n",
        "\n",
        "Every N iterations, the non-linear search outputs the maximum likelihood model and its best fit image to the \n",
        "Notebook visualizer and to hard-disk.\n",
        "\n",
        "This process takes around ~10 seconds, so we don't want it to happen too often so as to slow down the overall\n",
        "fit, but we also want it to happen frequently enough that we can track the progress.\n",
        "\n",
        "On GPU, a value of ~2500 will see this output happens every minute, a good balance. On CPU it'll be a little\n",
        "longer, but still a good balance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.Nautilus(\n",
        "    path_prefix=Path(\"cluster\", \"modeling\"),\n",
        "    name=\"start_here\",\n",
        "    unique_tag=dataset_name,\n",
        "    n_live=100,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "We next create  `AnalysisPoint` objects, which can be given many inputs customizing how the lens model is \n",
        "fitted to the data (in this example they are omitted for simplicity).\n",
        "\n",
        "Internally, this object defines the `log_likelihood_function` used by the non-linear search to fit the model to \n",
        "the `Imaging` dataset. \n",
        "\n",
        "We create a list of analysis objects, one for each dataset, which means that the lens modeling will fit each\n",
        "set of multiple images one-by-one and then sum their likelihoods. \n",
        "\n",
        "It is not vital that you as a user understand the details of how the `log_likelihood_function` fits a lens model to \n",
        "data, but interested readers can find a step-by-step guide of the likelihood \n",
        "function at ``autolens_workspace/*/point/log_likelihood_function`\n",
        "\n",
        "__JAX__\n",
        "\n",
        "PyAutoLens uses JAX under the hood for fast GPU/CPU acceleration. If JAX is installed with GPU\n",
        "support, your fits will run much faster (around 10 minutes instead of an hour). If only a CPU is available,\n",
        "JAX will still provide a speed up via multithreading, with fits taking around 20-30 minutes.\n",
        "\n",
        "If you don\u2019t have a GPU locally, consider Google Colab which provides free GPUs, so your modeling runs are much faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_list = [\n",
        "    al.AnalysisPoint(\n",
        "        dataset=dataset,\n",
        "        solver=solver,\n",
        "        use_jax=True,  # JAX will use GPUs for acceleration if available, else JAX will use multithreaded CPUs.\n",
        "    )\n",
        "    for dataset in dataset_list\n",
        "]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis Factor__\n",
        "\n",
        "Each analysis object is wrapped in an `AnalysisFactor`, which pairs each analysis it with the model.\n",
        "\n",
        "For this simple cluster examples, the API below in a very simple way. However, the factor graph API below is used for\n",
        "many advanced lens modeling tasks elsewhere in the workspace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_factor_list = []\n",
        "\n",
        "for analysis in analysis_list:\n",
        "\n",
        "    analysis_factor = af.AnalysisFactor(prior_model=model_analysis, analysis=analysis)\n",
        "\n",
        "    analysis_factor_list.append(analysis_factor)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Factor Graph__\n",
        "\n",
        "All `AnalysisFactor` objects are combined into a `FactorGraphModel`, which represents a global model fit to \n",
        "multiple datasets using a graphical model structure.\n",
        "\n",
        "The key outcomes of this setup are:\n",
        "\n",
        " - The individual log likelihoods from each `Analysis` object are summed to form the total log likelihood \n",
        "   evaluated during the model-fitting process.\n",
        "\n",
        " - Results from all datasets are output to a unified directory, with subdirectories for visualizations \n",
        "   from each analysis object, as defined by their `visualize` methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "factor_graph = af.FactorGraphModel(*analysis_factor_list)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To inspect this new model, with extra parameters for each dataset created, we \n",
        "print `factor_graph.global_prior_model.info`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(factor_graph.global_prior_model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Run Times__\n",
        "\n",
        "Lens modeling can be a computationally expensive process. When fitting complex models to high resolution datasets \n",
        "run times can be of order hours, days, weeks or even months.\n",
        "\n",
        "Run times are dictated by two factors:\n",
        "\n",
        " - The log likelihood evaluation time: the time it takes for a single `instance` of the lens model to be fitted to \n",
        "   the dataset such that a log likelihood is returned.\n",
        " \n",
        " - The number of iterations (e.g. log likelihood evaluations) performed by the non-linear search: more complex lens\n",
        "   models require more iterations to converge to a solution.\n",
        "   \n",
        "For this analysis, the log likelihood evaluation time is < 1 seconds on CPU, < 0.02 seconds on GPU, which is \n",
        "fast for cluster scale lens modeling. \n",
        "\n",
        "To estimate the expected overall run time of the model-fit we multiply the log likelihood evaluation time by an \n",
        "estimate of the number of iterations the non-linear search will perform. \n",
        "\n",
        "For this model, this is typically around > iterations, meaning that this script takes < ? seconds, \n",
        "or ? minutes on CPU, or < ? seconds, or ? minute on GPU.\n",
        "\n",
        "__Model-Fit__\n",
        "\n",
        "We begin the model-fit by passing the model and analysis object to the non-linear search (checkout the output folder\n",
        "for on-the-fly visualization and results)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_list = search.fit(model=factor_graph.global_prior_model, analysis=factor_graph)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Output Folder__\n",
        "\n",
        "Now this is running you should checkout the `autolens_workspace/output` folder. This is where the results of the \n",
        "search are written to hard-disk (in the `start_here` folder), where all outputs are human readable (e.g. as .json,\n",
        ".csv or text files).\n",
        "\n",
        "As the fit progresses, results are written to the `output` folder on the fly using the highest likelihood model found\n",
        "by the non-linear search so far. This means you can inspect the results of the model-fit as it runs, without having to\n",
        "wait for the non-linear search to terminate.\n",
        "\n",
        "The `output` folder includes:\n",
        "\n",
        " - `model.info`: Summarizes the lens model, its parameters and their priors discussed in the next tutorial.\n",
        "\n",
        " - `model.results`: Summarizes the highest likelihood lens model inferred so far including errors.\n",
        "\n",
        " - `images`: Visualization of the highest likelihood model-fit to the dataset, (e.g. a fit subplot showing the lens \n",
        " and source galaxies, model data and residuals).\n",
        "\n",
        " - `files`: A folder containing .fits files of the dataset, the model as a human-readable .json file, \n",
        " a `.csv` table of every non-linear search sample and other files containing information about the model-fit.\n",
        "\n",
        " - search.summary: A file providing summary statistics on the performance of the non-linear search.\n",
        "\n",
        " - `search_internal`: Internal files of the non-linear search (in this case Nautilus) used for resuming the fit and\n",
        "  visualizing the search.\n",
        "\n",
        "__Result__\n",
        "\n",
        "The search returns a result object, which whose `info` attribute shows the result in a readable format.\n",
        "\n",
        "[Above, we discussed that the `info_whitespace_length` parameter in the config files could b changed to make \n",
        "the `model.info` attribute display optimally on your computer. This attribute also controls the whitespace of the\n",
        "`result.info` attribute.]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result_list[0].info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Result` object also contains:\n",
        "\n",
        " - The model corresponding to the maximum log likelihood solution in parameter space.\n",
        " - The corresponding maximum log likelihood `Tracer` and `FitImaging` objects.\n",
        "\n",
        "Checkout `autolens_workspace/*/results` for a full description of analysing results in **PyAutoLens**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result_list[0].max_log_likelihood_instance)\n",
        "\n",
        "tracer_plotter = aplt.TracerPlotter(\n",
        "    tracer=result_list[0].max_log_likelihood_tracer, grid=result_list[0].grids.lp\n",
        ")\n",
        "tracer_plotter.subplot_tracer()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It also contains information on the posterior as estimated by the non-linear search (in this example `Nautilus`). \n",
        "\n",
        "Below, we make a corner plot of the \"Probability Density Function\" of every parameter in the model-fit.\n",
        "\n",
        "The plot is labeled with short hand parameter names (e.g. `sersic_index` is mapped to the short hand \n",
        "parameter `n`). These mappings ate specified in the `config/notation.yaml` file and can be customized by users.\n",
        "\n",
        "The superscripts of labels correspond to the name each component was given in the model (e.g. for the `Isothermal`\n",
        "mass its name `mass` defined when making the `Model` above is used)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plotter = aplt.NestPlotter(samples=result_list[0].samples)\n",
        "plotter.corner_anesthetic()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This script gives a concise overview of the basic cluster modeling API, fitting one the simplest lens models possible.\n",
        "\n",
        "Lets now consider what features you should read about to improve your cluster lens modeling, especially if you are aiming\n",
        "to fit more complex models to your data.\n",
        "\n",
        "__Data Preparation__\n",
        "\n",
        "If you are looking to fit your own point source data of a strong lens, checkout  \n",
        "the `autolens_workspace/*/data_preparation/point_source/README.rst` script for an overview of how data should be \n",
        "prepared before being modeled.\n",
        "\n",
        "__HowToLens__\n",
        "\n",
        "This `start_here.py` script, and the features examples above, do not explain many details of how lens modeling is \n",
        "performed, for example:\n",
        "\n",
        " - How does PyAutoLens perform ray-tracing and lensing calculations in order to fit a lens model?\n",
        " - How is a lens model fitted to data? What quantifies the goodness of fit (e.g. how is a log likelihood computed?).\n",
        " - How does Nautilus find the highest likelihood lens models? What exactly is a \"non-linear search\"?\n",
        "\n",
        "You do not need to be able to answer these questions in order to fit lens models with PyAutoLens and do science.\n",
        "However, having a deeper understanding of how it all works is both interesting and will benefit you as a scientist\n",
        "\n",
        "This deeper insight is offered by the **HowToLens** Jupyter notebook lectures, found \n",
        "at `autolens_workspace/*/howtolens`. \n",
        "\n",
        "I recommend that you check them out if you are interested in more details!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}