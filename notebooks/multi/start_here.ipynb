{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Start Here: Multi Wavelength\n",
        "============================\n",
        "\n",
        "Strong gravitational lenses are often observed with CCD imaging, for example using HST, JWST,\n",
        "or ground-based telescopes.\n",
        "\n",
        "The examples `start_here_imaging.ipynb` illustrates how to perform lens modeling of CCD imaging\n",
        "of single lenses, it is recommend you read that example before reading this one.\n",
        "\n",
        "This script shows you how to model multiple images of a strong lens, taken at different wavelengths,\n",
        "with as little setup as possible. In about 15 minutes you\u2019ll be able to point the code at your own\n",
        "FITS files and fit your first lens.\n",
        "\n",
        "Multi-wavelength lens modeling is an advanced feature and it is recommend you become more familiar with\n",
        "**PyAutoLens** and lens modeling before using it for your own science. Nevertheless, this script\n",
        "should make it quick and easy to at least have a go doing multi-wavelength modeling of your own data.\n",
        "\n",
        "We focus on a *galaxy-scale* lens (a single lens galaxy). If you have multiple lens galaxies,\n",
        "see the `start_here_group.ipynb` and `start_here_cluster.ipynb` examples.\n",
        "\n",
        "__JAX__\n",
        "\n",
        "PyAutoLens uses JAX under the hood for fast GPU/CPU acceleration. If JAX is installed with GPU\n",
        "support, your fits will run much faster (around 10 minutes instead of an hour). If only a CPU is available,\n",
        "JAX will still provide a speed up via multithreading, with fits taking around 20-30 minutes.\n",
        "\n",
        "If you don\u2019t have a GPU locally, consider Google Colab which provides free GPUs, so your modeling runs are much faster.\n",
        "\n",
        "We also show how to simulate strong lens imaging. This is useful for building machine learning training datasets,\n",
        "or for investigating lensing effects in a controlled way.\n",
        "\n",
        "__Google Colab Setup__\n",
        "\n",
        "The introduction `start_here` examples are available on Google Colab, which allows you to run them in a web browser\n",
        "without manual local PyAutoLens installation.\n",
        "\n",
        "The code below sets up your environment if you are using Google Colab, including installing autolens and downloading\n",
        "files required to run the notebook. If you are running this script not in Colab (e.g. locally on your own computer),\n",
        "running the code will still check correctly that your environment is set up and ready to go."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "\n",
        "    subprocess.check_call(\n",
        "        [sys.executable, \"-m\", \"pip\", \"install\", \"autoconf\", \"--no-deps\"]\n",
        "    )\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "from autoconf import setup_colab\n",
        "\n",
        "setup_colab.for_autolens(\n",
        "    raise_error_if_not_gpu=False  # Switch to False for CPU Google Colab\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Imports__\n",
        "\n",
        "Lets first import autolens, its plotting module and the other libraries we'll need.\n",
        "\n",
        "You'll see these imports in the majority of workspace examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "import autofit as af\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "We begin by loading the dataset. Three ingredients are needed for lens modeling:\n",
        "\n",
        "1. The image itself (CCD counts).\n",
        "2. A noise-map (per-pixel RMS noise).\n",
        "3. The PSF (Point Spread Function).\n",
        "\n",
        "Here we use multi-wavelength James Webb Space Telescope imaging of a strong lens called the COSMOS-Web ring. Replace \n",
        "these FITS paths with your own to immediately try modeling your data.\n",
        "\n",
        "The `pixel_scales` value converts pixel units into arcseconds. It is critical you set this\n",
        "correctly for your data.\n",
        "\n",
        "**Multi-wavelength Specific**: Note how each waveband and its corresponding pixel scale are put into a list and dictionary, \n",
        "which we use to load all 4 wavelength images in a list of imaging datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "waveband_list = [\n",
        "    #    \"F115W\",  # Commented out to make code run fast, but can be included to show 4 wavebad modeling.\n",
        "    #    \"F150W\",\n",
        "    \"F277W\",\n",
        "    \"F444W\",\n",
        "]\n",
        "pixel_scale_dict = {\n",
        "    \"F115W\": 0.03,\n",
        "    \"F150W\": 0.03,\n",
        "    \"F277W\": 0.06,\n",
        "    \"F444W\": 0.06,\n",
        "}\n",
        "\n",
        "dataset_name = \"cosmos_web_ring\"\n",
        "dataset_path = Path(\"dataset\") / \"imaging\" / dataset_name / \"wavebands\"\n",
        "\n",
        "dataset_list = []\n",
        "\n",
        "for dataset_waveband in waveband_list:\n",
        "\n",
        "    dataset_waveband_path = dataset_path / dataset_waveband\n",
        "\n",
        "    pixel_scale = pixel_scale_dict[dataset_waveband]\n",
        "\n",
        "    dataset = al.Imaging.from_fits(\n",
        "        data_path=dataset_waveband_path / \"data.fits\",\n",
        "        psf_path=dataset_waveband_path / \"psf.fits\",\n",
        "        noise_map_path=dataset_waveband_path / \"noise_map.fits\",\n",
        "        pixel_scales=pixel_scale,\n",
        "    )\n",
        "\n",
        "    dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "    dataset_plotter.subplot_dataset()\n",
        "\n",
        "    dataset_list.append(dataset)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Extra Galaxy Removal__\n",
        "\n",
        "There may be regions of an image that have signal near the lens and source that is from other galaxies not associated\n",
        "with the strong lens we are studying. The emission from these images will impact our model fitting and needs to be\n",
        "removed from the analysis.\n",
        "\n",
        "This `mask_extra_galaxies` is used to prevent them from impacting a fit by scaling the RMS noise map values to\n",
        "large values. This mask may also include emission from objects which are not technically galaxies,\n",
        "but blend with the galaxy we are studying in a similar way. Common examples of such objects are foreground stars\n",
        "or emission due to the data reduction process.\n",
        "\n",
        "In this example, the noise is scaled over all regions of the image, even those quite far away from the strong lens\n",
        "in the centre. We are next going to apply a 2.5\" circular mask which means we only analyse the central region of\n",
        "the image. It only in these central regions where for the actual lens analysis it matters that we scaled the noise.\n",
        "\n",
        "After performing lens modeling to this strong lens, the script further down provides a GUI to create such a mask\n",
        "for your own data, if necessary.\n",
        "\n",
        "**Multi-wavelength Specific**: The RMS noise map scaling is applied to all datasets one-by-one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_scaled_list = []\n",
        "\n",
        "for dataset, dataset_waveband in zip(dataset_list, waveband_list):\n",
        "\n",
        "    dataset_waveband_path = dataset_path / dataset_waveband\n",
        "\n",
        "    mask_extra_galaxies = al.Mask2D.from_fits(\n",
        "        file_path=dataset_waveband_path / \"mask_extra_galaxies.fits\",\n",
        "        pixel_scales=dataset.pixel_scales,\n",
        "        invert=True,\n",
        "    )\n",
        "\n",
        "    dataset = dataset.apply_noise_scaling(mask=mask_extra_galaxies)\n",
        "\n",
        "    dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "    dataset_plotter.subplot_dataset()\n",
        "\n",
        "    dataset_scaled_list.append(dataset)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Masking__\n",
        "\n",
        "Lens modeling does not need to fit the entire image, only the region containing lens and\n",
        "source light. We therefore define a circular mask around the lens.\n",
        "\n",
        "- Make sure the mask fully encloses the lensed arcs and the lens galaxy.\n",
        "- Avoid masking too much empty sky, as this slows fitting without adding information.\n",
        "\n",
        "We\u2019ll also oversample the central pixels, which improves modeling accuracy without adding\n",
        "unnecessary cost far from the lens.\n",
        "\n",
        "**Multi-wavelength Specific**: The mask is applied to each wavelength of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mask_radius = 2.5\n",
        "\n",
        "dataset_masked_list = []\n",
        "\n",
        "for dataset in dataset_scaled_list:\n",
        "\n",
        "    mask = al.Mask2D.circular(\n",
        "        shape_native=dataset.shape_native,\n",
        "        pixel_scales=dataset.pixel_scales,\n",
        "        radius=mask_radius,\n",
        "    )\n",
        "\n",
        "    dataset = dataset.apply_mask(mask=mask)\n",
        "\n",
        "    # Over sampling is important for accurate lens modeling, but details are omitted\n",
        "    # for simplicity here, so don't worry about what this code is doing yet!\n",
        "\n",
        "    over_sample_size = al.util.over_sample.over_sample_size_via_radial_bins_from(\n",
        "        grid=dataset.grid,\n",
        "        sub_size_list=[4, 2, 1],\n",
        "        radial_list=[0.3, 0.6],\n",
        "        centre_list=[(0.0, 0.0)],\n",
        "    )\n",
        "\n",
        "    dataset = dataset.apply_over_sampling(over_sample_size_lp=over_sample_size)\n",
        "\n",
        "    dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "    dataset_plotter.subplot_dataset()\n",
        "\n",
        "    dataset_masked_list.append(dataset)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "To perform lens modeling we must define a lens model, describing the light profiles of \n",
        "the lens and source galaxies, and the mass profile of the lens galaxy.\n",
        "\n",
        "A brilliant lens model to start with is one which uses a Multi Gaussian Expansion (MGE) \n",
        "to model the lens and source light, and a Singular Isothermal Ellipsoid (SIE) plus \n",
        "shear to model the lens mass. \n",
        "\n",
        "Full details of why this models is so good are provided in the main workspace docs, \n",
        "but in a nutshell it  provides an excellent balance of being fast to fit, flexible \n",
        "enough to capture complex galaxy morphologies and providing accurate fits to the vast \n",
        "majority of strong lenses.\n",
        "\n",
        "The MGE model composition API is quite long and technical, so we simply load the MGE \n",
        "models for the lens and source below via a utility function `mge_model_from` which \n",
        "hides the API to make the code in this introduction example ready to read. We then \n",
        "use the PyAutoLens Model API to compose the over lens model.\n",
        "\n",
        "**Multi-wavelength Specific**: The main lens model composition does not change for \n",
        "multi wavelength, however it is worth emphaising that the MGE will infer a unique\n",
        "lens and source solution for each wavelength whereby the Gaussians have different\n",
        "intensities, meaning that effects like colour gradients will be captured accurately.\n",
        "\n",
        "Multi wavelength data may also have small offsets between each band, often smaller\n",
        "than a pixel and thus below standard astrometric precision. We therefore include\n",
        "a `dataset_model` composition which models these offsets as free parameters during\n",
        "the lens modeling. Slightly further down in the script we will tell autolens\n",
        "to make a different between each dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Lens:\n",
        "\n",
        "bulge = al.model_util.mge_model_from(\n",
        "    mask_radius=mask_radius, total_gaussians=20, centre_prior_is_uniform=True\n",
        ")\n",
        "\n",
        "mass = af.Model(al.mp.Isothermal)\n",
        "\n",
        "shear = af.Model(al.mp.ExternalShear)\n",
        "\n",
        "lens = af.Model(al.Galaxy, redshift=0.5, bulge=bulge, mass=mass, shear=shear)\n",
        "\n",
        "# Source:\n",
        "\n",
        "bulge = al.model_util.mge_model_from(\n",
        "    mask_radius=mask_radius, total_gaussians=20, centre_prior_is_uniform=False\n",
        ")\n",
        "\n",
        "source = af.Model(al.Galaxy, redshift=1.0, bulge=bulge)\n",
        "\n",
        "# Dataset Model\n",
        "\n",
        "dataset_model = af.Model(al.DatasetModel)\n",
        "\n",
        "# Overall Lens Model:\n",
        "\n",
        "model = af.Collection(\n",
        "    dataset_model=dataset_model, galaxies=af.Collection(lens=lens, source=source)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can print the model to show the parameters that the model is composed of, which shows many of the MGE's fixed\n",
        "parameter values the API above hided the composition of."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "In other examples, a single `Analysis` object is passed the dataset and used to perform lens modeling.\n",
        "\n",
        "When there are multiple datasets, a list of analysis objects is created, once for each dataset.\n",
        "\n",
        "__JAX__\n",
        "\n",
        "PyAutoLens uses JAX under the hood for fast GPU/CPU acceleration. If JAX is installed with GPU\n",
        "support, your fits will run much faster (around 10 minutes instead of an hour). If only a CPU is available,\n",
        "JAX will still provide a speed up via multithreading, with fits taking around 20-30 minutes.\n",
        "\n",
        "If you don\u2019t have a GPU locally, consider Google Colab which provides free GPUs, so your modeling runs are much faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_list = [\n",
        "    al.AnalysisImaging(\n",
        "        dataset=dataset,\n",
        "        use_jax=True,  # JAX will use GPUs for acceleration if available, else JAX will use multithreaded CPUs.\n",
        "    )\n",
        "    for dataset in dataset_masked_list\n",
        "]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each analysis object is wrapped in an `AnalysisFactor`, which pairs it with the model and prepares it for use in a \n",
        "factor graph. This step allows us to flexibly define how each dataset relates to the model.\n",
        "\n",
        "Whilst not illustrates here, note that the API below is extremely customizeable and allows us to\n",
        "make the model vary on a per dataset basis. We use this below to make it so the dataset offset of the second,\n",
        "third and fourth datasets are included."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_factor_list = []\n",
        "\n",
        "for i, analysis in enumerate(analysis_list):\n",
        "    model_analysis = model.copy()\n",
        "\n",
        "    if i > 0:\n",
        "        model_analysis.dataset_model.grid_offset.grid_offset_0 = af.UniformPrior(\n",
        "            lower_limit=-1.0, upper_limit=1.0\n",
        "        )\n",
        "        model_analysis.dataset_model.grid_offset.grid_offset_1 = af.UniformPrior(\n",
        "            lower_limit=-1.0, upper_limit=1.0\n",
        "        )\n",
        "\n",
        "    analysis_factor = af.AnalysisFactor(prior_model=model, analysis=analysis)\n",
        "\n",
        "    analysis_factor_list.append(analysis_factor)\n",
        "\n",
        "# Required to set up a fit with mutliple datasets.\n",
        "factor_graph = af.FactorGraphModel(*analysis_factor_list)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fit__\n",
        "\n",
        "We now fit the data with the lens model using the non-linear fitting method and nested sampling algorithm Nautilus.\n",
        "\n",
        "This requires an `AnalysisImaging` object, which defines the `log_likelihood_function` used by Nautilus to fit\n",
        "the model to the imaging data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.Nautilus(\n",
        "    path_prefix=Path(\n",
        "        \"multi_wavelength\"\n",
        "    ),  # The path where results and output are stored.\n",
        "    name=\"start_here\",  # The name of the fit and folder results are output to.\n",
        "    unique_tag=dataset_name,  # A unique tag which also defines the folder.\n",
        "    n_live=150,  # The number of Nautilus \"live\" points, increase for more complex models.\n",
        "    n_batch=50,  # For fast GPU fitting lens model fits are batched and run simultaneously.\n",
        "    iterations_per_quick_update=5000,  # Every N iterations the max likelihood model is visualized and written to output folder.\n",
        ")\n",
        "\n",
        "result_list = search.fit(model=factor_graph.global_prior_model, analysis=factor_graph)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "The result object returned by this model-fit is a list of `Result` objects, because we used a factor graph.\n",
        "Each result corresponds to each analysis, and therefore corresponds to the model-fit at that wavelength.\n",
        "\n",
        "For example, close inspection of the `max_log_likelihood_instance` of the two results shows that all parameters,\n",
        "except the `effective_radius` of the source galaxy's `bulge`, are identical."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result_list[0].max_log_likelihood_instance)\n",
        "print(result_list[1].max_log_likelihood_instance)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result also contains the maximum likelihood lens model which can be used to plot the best-fit lensing information\n",
        "and fit to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for result in result_list:\n",
        "    tracer_plotter = aplt.TracerPlotter(\n",
        "        tracer=result.max_log_likelihood_tracer, grid=result.grids.lp\n",
        "    )\n",
        "    tracer_plotter.subplot_tracer()\n",
        "\n",
        "    fit_plotter = aplt.FitImagingPlotter(fit=result.max_log_likelihood_fit)\n",
        "    fit_plotter.subplot_fit()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Your Own Lens__\n",
        "\n",
        "If you have your own strong lens imaging data, you are now ready to model it yourself by adapting the code above\n",
        "and simply inputting the path to your own .fits files into the `Imaging.from_fits()` function.\n",
        "\n",
        "A few things to note, with full details on data preparation provided in the main workspace documentation:\n",
        "\n",
        "- Supply your own CCD image, PSF, and RMS noise-map.\n",
        "- Ensure the lens galaxy is roughly centered in the image.\n",
        "- Double-check `pixel_scales` for your telescope/detector.\n",
        "- Adjust the mask radius to include all relevant light.\n",
        "- Remove extra light from galaxies and other objects using the extra galaxies mask GUI above.\n",
        "- Start with the default model \u2014 it works very well for pretty much all galaxy scale lenses!\n",
        "\n",
        "__Simulator__\n",
        "\n",
        "In the example `start_here_imaging.ipynb`, we showed how to simulate CCD imaging of a strong lens.\n",
        "\n",
        "We do not give a full description of the simulation API for multi wavelength lens imaging here,\n",
        "but it is fully described in the main workspace documentation.\n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "This script has shown how to model CCD imaging data of strong lenses, and simulate your own strong lens images.\n",
        "\n",
        "Details of the **PyAutoLens** API and how lens modeling and simulations actually work were omitted for simplicity,\n",
        "but everything you need to know is described throughout the main workspace documentation. You should check it out,\n",
        "but maybe you want to try and model your own lens first!\n",
        "\n",
        "The following locations of the workspace are good places to checkout next:\n",
        "\n",
        "- `autolens_workspace/*/advanced/multiimaging`: A full description of the multi wavelength and multi image fitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}