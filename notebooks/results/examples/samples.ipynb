{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results: Samples\n",
        "================\n",
        "\n",
        "After a non-linear search has completed, it returns a `Result` object that contains information on samples of\n",
        "the non-linear search, such as the maximum likelihood model instance, the errors on each parameter and the \n",
        "Bayesian evidence.\n",
        "\n",
        "This script illustrates how to use the result to inspect the non-linear search samples.\n",
        "\n",
        "__Units__\n",
        "\n",
        "In this example, all quantities are **PyAutoLens**'s internal unit coordinates, with spatial coordinates in\n",
        "arc seconds, luminosities in electrons per second and mass quantities (e.g. convergence) are dimensionless.\n",
        "\n",
        "The guide `guides/units_and_cosmology.ipynb` illustrates how to convert these quantities to physical units like\n",
        "kiloparsecs, magnitudes and solar masses.\n",
        "\n",
        "__Start Here Notebook__\n",
        "\n",
        "If any code in this script is unclear, refer to the `results/start_here.ipynb` notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import autofit as af\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fit__\n",
        "\n",
        "To illustrate results, we need to perform a model-fit in order to create a `Result` object.\n",
        "\n",
        "The code below performs a model-fit using Nautilus. \n",
        "\n",
        "You should be familiar with modeling already, if not read the `modeling/start_here.py` script before reading this one!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"simple__no_lens_light\"\n",
        "dataset_path = path.join(\"dataset\", \"imaging\", dataset_name)\n",
        "\n",
        "dataset = al.Imaging.from_fits(\n",
        "    data_path=path.join(dataset_path, \"data.fits\"),\n",
        "    psf_path=path.join(dataset_path, \"psf.fits\"),\n",
        "    noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "    pixel_scales=0.1,\n",
        ")\n",
        "\n",
        "mask = al.Mask2D.circular(\n",
        "    shape_native=dataset.shape_native, pixel_scales=dataset.pixel_scales, radius=3.0\n",
        ")\n",
        "\n",
        "dataset = dataset.apply_mask(mask=mask)\n",
        "\n",
        "model = af.Collection(\n",
        "    galaxies=af.Collection(\n",
        "        lens=af.Model(al.Galaxy, redshift=0.5, mass=al.mp.Isothermal),\n",
        "        source=af.Model(\n",
        "            al.Galaxy, redshift=1.0, bulge=al.lp_linear.SersicCore, disk=None\n",
        "        ),\n",
        "    ),\n",
        ")\n",
        "\n",
        "search = af.Nautilus(\n",
        "    path_prefix=path.join(\"results_folder\"),\n",
        "    name=\"results\",\n",
        "    unique_tag=dataset_name,\n",
        "    n_live=100,\n",
        "    number_of_cores=1,\n",
        ")\n",
        "\n",
        "analysis = al.AnalysisImaging(dataset=dataset)\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Info__\n",
        "\n",
        "As seen throughout the workspace, the `info` attribute shows the result in a readable format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Plot__\n",
        "\n",
        "We now have the `Result` object we will cover in this script. \n",
        "\n",
        "As a reminder, in the `modeling` scripts we use the `max_log_likelihood_tracer` and `max_log_likelihood_fit` to plot \n",
        "the results of the fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tracer_plotter = aplt.TracerPlotter(\n",
        "    tracer=result.max_log_likelihood_tracer, grid=mask.derive_grid.all_false\n",
        ")\n",
        "tracer_plotter.subplot_tracer()\n",
        "fit_plotter = aplt.FitImagingPlotter(fit=result.max_log_likelihood_fit)\n",
        "fit_plotter.subplot_fit()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results tutorials `tracer.py` and `fit.py` expand on the `max_log_likelihood_tracer` and `max_log_likelihood_fit`, \n",
        "showing how  they can be used to inspect many aspects of a model.\n",
        "\n",
        "__Samples__\n",
        "\n",
        "The result contains a `Samples` object, which contains all samples of the non-linear search.\n",
        "\n",
        "Each sample corresponds to a set of model parameters that were evaluated and accepted by the non linear search, \n",
        "in this example `Nautilus`. \n",
        "\n",
        "This includes their log likelihoods, which are used for computing additional information about the model-fit,\n",
        "for example the error on every parameter. \n",
        "\n",
        "Our model-fit used the nested sampling algorithm Nautilus, so the `Samples` object returned is a `SamplesNest` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "print(\"Nest Samples: \\n\")\n",
        "print(samples)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Parameters__\n",
        "\n",
        "The parameters are stored as a list of lists, where:\n",
        "\n",
        " - The outer list is the size of the total number of samples.\n",
        " - The inner list is the size of the number of free parameters in the fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"All parameters of the very first sample\")\n",
        "print(samples.parameter_lists[0])\n",
        "print(\"The fourth parameter of the tenth sample\")\n",
        "print(samples.parameter_lists[9][3])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Figures of Merit__\n",
        "\n",
        "The `Samples` class contains the log likelihood, log prior, log posterior and weight_list of every accepted sample, where:\n",
        "\n",
        "- The `log_likelihood` is the value evaluated in the `log_likelihood_function`.\n",
        "\n",
        "- The `log_prior` encodes information on how parameter priors map log likelihood values to log posterior values.\n",
        "\n",
        "- The `log_posterior` is `log_likelihood + log_prior`.\n",
        "\n",
        "- The `weight` gives information on how samples are combined to estimate the posterior, which depends on type of search\n",
        "  used (for `Nautilus` they are all non-zero values which sum to 1).\n",
        "\n",
        "Lets inspect these values for the tenth sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"log(likelihood), log(prior), log(posterior) and weight of the tenth sample.\")\n",
        "print(samples.log_likelihood_list[9])\n",
        "print(samples.log_prior_list[9])\n",
        "print(samples.log_posterior_list[9])\n",
        "print(samples.weight_list[9])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Instances__\n",
        "\n",
        "Many results can be returned as an instance of the model, using the Python class structure of the model composition.\n",
        "\n",
        "For example, we can return the model parameters corresponding to the maximum log likelihood sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = samples.max_log_likelihood()\n",
        "print(\"Maximum Log Likelihood Model Instance: \\n\")\n",
        "print(instance, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The attributes of the `instance` (e.g. `galaxies`, `lens`) have these names due to how we composed the `Galaxy` and\n",
        "its light and mass profiles via the `Collection` and `Model` above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(instance.galaxies)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These galaxies will be named according to the model fitted by the search (in this case, `lens` and `source`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(instance.galaxies.lens)\n",
        "print(instance.galaxies.source)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Their light profiles are also named according to model composition allowing individual parameters to be printed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(instance.galaxies.lens.mass.einstein_radius)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use this list of galaxies to create the maximum log likelihood `Tracer`, which is the property of the result \n",
        "we've used up to now!\n",
        "\n",
        "Using this tracer is expanded upon in the `tracer.py` results tutorial.\n",
        "\n",
        "(If we had the `Imaging` available we could easily use this to create the maximum log likelihood `FitImaging`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "max_lh_tracer = al.Tracer(galaxies=instance.galaxies)\n",
        "\n",
        "print(max_lh_tracer)\n",
        "print(mask.derive_grid.all_false)\n",
        "\n",
        "# Input to FitImaging to solve for linear light profile intensities, see `start_here.py` for details.\n",
        "fit = al.FitImaging(dataset=dataset, tracer=max_lh_tracer)\n",
        "max_lh_tracer = fit.tracer_linear_light_profiles_to_light_profiles\n",
        "\n",
        "tracer_plotter = aplt.TracerPlotter(\n",
        "    tracer=max_lh_tracer, grid=mask.derive_grid.all_false\n",
        ")\n",
        "tracer_plotter.subplot_tracer()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Posterior / PDF__\n",
        "\n",
        "The result contains the full posterior information of our non-linear search, which can be used for parameter \n",
        "estimation. \n",
        "\n",
        "PDF stands for \"Probability Density Function\" and it quantifies probability of each model parameter having values\n",
        "that are sampled. It therefore enables error estimation via a process called marginalization.\n",
        "\n",
        "The median pdf vector is available, which estimates every parameter via 1D marginalization of their PDFs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = samples.median_pdf()\n",
        "\n",
        "print(\"Median PDF Model Instances: \\n\")\n",
        "print(instance, \"\\n\")\n",
        "print(instance.galaxies.source.bulge)\n",
        "print()\n",
        "\n",
        "vector = samples.median_pdf(as_instance=False)\n",
        "\n",
        "print(\"Median PDF Model Parameter Lists: \\n\")\n",
        "print(vector, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Errors__\n",
        "\n",
        "Methods for computing error estimates on all parameters are provided. \n",
        "\n",
        "This again uses 1D marginalization, now at an input sigma confidence limit. \n",
        "\n",
        "By inputting `sigma=3.0` margnialization find the values spanning 99.7% of 1D PDF. Changing this to `sigma=1.0`\n",
        "would give the errors at the 68.3% confidence limit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance_upper_sigma = samples.values_at_upper_sigma(sigma=3.0)\n",
        "instance_lower_sigma = samples.values_at_lower_sigma(sigma=3.0)\n",
        "\n",
        "print(\"Errors Instances: \\n\")\n",
        "print(instance_upper_sigma.galaxies.source.bulge, \"\\n\")\n",
        "print(instance_lower_sigma.galaxies.source.bulge, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "They can also be returned at the values of the parameters at their error values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance_upper_values = samples.errors_at_upper_sigma(sigma=3.0)\n",
        "instance_lower_values = samples.errors_at_lower_sigma(sigma=3.0)\n",
        "\n",
        "print(\"Errors Instances: \\n\")\n",
        "print(instance_upper_values.galaxies.source.bulge, \"\\n\")\n",
        "print(instance_lower_values.galaxies.source.bulge, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Sample Instance__\n",
        "\n",
        "A non-linear search retains every model that is accepted during the model-fit.\n",
        "\n",
        "We can create an instance of any model -- below we create an instance of the last accepted model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = samples.from_sample_index(sample_index=-1)\n",
        "\n",
        "print(instance.galaxies.lens.mass)\n",
        "print(instance.galaxies.lens.mass)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search Plots__\n",
        "\n",
        "The Probability Density Functions (PDF's) of the results can be plotted using the non-linear search in-built \n",
        "visualization tools.\n",
        "\n",
        "This fit used `Nautilus` therefore we use the `NestPlotter` for visualization, which wraps in-built\n",
        "visualization tools.\n",
        "\n",
        "The `autofit_workspace/*/plots` folder illustrates other packages that can be used to make these plots using\n",
        "the standard output results formats (e.g. `GetDist.py`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plotter = aplt.NestPlotter(samples=result.samples)\n",
        "plotter.corner_anesthetic()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Maximum Likelihood__\n",
        "\n",
        "The maximum log likelihood value of the model-fit can be estimated by simple taking the maximum of all log\n",
        "likelihoods of the samples.\n",
        "\n",
        "If different models are fitted to the same dataset, this value can be compared to determine which model provides\n",
        "the best fit (e.g. which model has the highest maximum likelihood)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Maximum Log Likelihood: \\n\")\n",
        "print(max(samples.log_likelihood_list))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Bayesian Evidence__\n",
        "\n",
        "Nested sampling algorithms like Nautilus also estimate the Bayesian evidence (estimated via the nested sampling \n",
        "algorithm).\n",
        "\n",
        "The Bayesian evidence accounts for \"Occam's Razor\", whereby it penalizes models for being more complex (e.g. if a model\n",
        "has more parameters it needs to fit the da\n",
        "\n",
        "The Bayesian evidence is a better quantity to use to compare models, because it penalizes models with more parameters\n",
        "for being more complex (\"Occam's Razor\"). Comparisons using the maximum likelihood value do not account for this and\n",
        "therefore may unjustly favour more complex models.\n",
        "\n",
        "Using the Bayesian evidence for model comparison is well documented on the internet, for example the following\n",
        "wikipedia page: https://en.wikipedia.org/wiki/Bayes_factor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Maximum Log Likelihood and Log Evidence: \\n\")\n",
        "print(samples.log_evidence)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Lists__\n",
        "\n",
        "All results can alternatively be returned as a 1D list of values, by passing `as_instance=False`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "max_lh_list = samples.max_log_likelihood(as_instance=False)\n",
        "print(\"Max Log Likelihood Model Parameters: \\n\")\n",
        "print(max_lh_list, \"\\n\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The list above does not tell us which values correspond to which parameters.\n",
        "\n",
        "The following quantities are available in the `Model`, where the order of their entries correspond to the parameters \n",
        "in the `ml_vector` above:\n",
        "\n",
        " - `paths`: a list of tuples which give the path of every parameter in the `Model`.\n",
        " - `parameter_names`: a list of shorthand parameter names derived from the `paths`.\n",
        " - `parameter_labels`: a list of parameter labels used when visualizing non-linear search results (see below).\n",
        "\n",
        "For simple models like the one fitted in this tutorial, the quantities below are somewhat redundant. For the\n",
        "more complex models they are important for tracking the parameters of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = samples.model\n",
        "\n",
        "print(model.paths)\n",
        "print(model.parameter_names)\n",
        "print(model.parameter_labels)\n",
        "print(model.model_component_and_parameter_names)\n",
        "print(\"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All the methods above are available as lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = samples.median_pdf(as_instance=False)\n",
        "values_at_upper_sigma = samples.values_at_upper_sigma(sigma=3.0, as_instance=False)\n",
        "values_at_lower_sigma = samples.values_at_lower_sigma(sigma=3.0, as_instance=False)\n",
        "errors_at_upper_sigma = samples.errors_at_upper_sigma(sigma=3.0, as_instance=False)\n",
        "errors_at_lower_sigma = samples.errors_at_lower_sigma(sigma=3.0, as_instance=False)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Latex__\n",
        "\n",
        "If you are writing modeling results up in a paper, you can use inbuilt latex tools to create latex table code which \n",
        "you can copy to your .tex document.\n",
        "\n",
        "By combining this with the filtering tools below, specific parameters can be included or removed from the latex.\n",
        "\n",
        "Remember that the superscripts of a parameter are loaded from the config file `notation/label.yaml`, providing high\n",
        "levels of customization for how the parameter names appear in the latex table. This is especially useful if your model\n",
        "uses the same model components with the same parameter, which therefore need to be distinguished via superscripts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "latex = af.text.Samples.latex(\n",
        "    samples=result.samples,\n",
        "    median_pdf_model=True,\n",
        "    sigma=3.0,\n",
        "    name_to_label=True,\n",
        "    include_name=True,\n",
        "    include_quickmath=True,\n",
        "    prefix=\"Example Prefix \",\n",
        "    suffix=r\"\\\\[-2pt]\",\n",
        ")\n",
        "\n",
        "print(latex)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Derived Errors (Advanced)__\n",
        "\n",
        "Computing the errors of a quantity like the `einstein_radius` is simple, because it is sampled by the non-linear \n",
        "search. Errors are accessible using the `Samples` object's `errors_from` methods, which marginalize over the \n",
        "parameters via the 1D Probability Density Function (PDF).\n",
        "\n",
        "Computing errors on derived quantities is more tricky, because they are not sampled directly by the non-linear search. \n",
        "For example, what if we want the error on the axis-ratio of the mass model? In order to do this we need to create the \n",
        "PDF of that derived quantity, which we can then marginalize over using the same function we use to marginalize model \n",
        "parameters.\n",
        "\n",
        "Below, we compute the axis-ratio of every accepted model sampled by the non-linear search and use this determine the PDF \n",
        "of the axis-ratio. When combining the axis-ratio's we weight each value by its `weight`. For Nautilus, a nested sampling \n",
        "algorithm, the weight of every sample is different and thus must be included.\n",
        "\n",
        "In order to pass these samples to the function `marginalize`, which marginalizes over the PDF of the axis-ratio to \n",
        "compute its error, we also pass the weight list of the samples.\n",
        "\n",
        "Note again how because when creating the model above using the input names `lens` and `mass` we access the instance\n",
        "below using these."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "axis_ratio_list = []\n",
        "\n",
        "for sample in samples.sample_list:\n",
        "    instance = sample.instance_for_model(model=samples.model, ignore_assertions=True)\n",
        "\n",
        "    ell_comps = instance.galaxies.lens.mass.ell_comps\n",
        "\n",
        "    axis_ratio = al.convert.axis_ratio_from(ell_comps=ell_comps)\n",
        "\n",
        "    axis_ratio_list.append(axis_ratio)\n",
        "\n",
        "median_axis_ratio, lower_axis_ratio, upper_axis_ratio = af.marginalize(\n",
        "    parameter_list=axis_ratio_list, sigma=3.0, weight_list=samples.weight_list\n",
        ")\n",
        "\n",
        "print(f\"axis_ratio = {median_axis_ratio} ({upper_axis_ratio} {lower_axis_ratio}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The calculation above could be computationally expensive, if there are many samples and the derived quantity is\n",
        "slow to compute.\n",
        "\n",
        "An alternative approach, which will provide comparable accuracy provided enough draws are used, is to sample \n",
        "points randomy from the PDF of the model and use these to compute the derived quantity.\n",
        "\n",
        "Draws are from the PDF of the model, so the weights of the samples are accounted for and we therefore do not\n",
        "pass them to the `marginalize` function (it essentially treats all samples as having equal weight)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "random_draws = 50\n",
        "\n",
        "axis_ratio_list = []\n",
        "\n",
        "for i in range(random_draws):\n",
        "    instance = samples.draw_randomly_via_pdf()\n",
        "\n",
        "    ell_comps = instance.galaxies.lens.mass.ell_comps\n",
        "\n",
        "    axis_ratio = al.convert.axis_ratio_from(ell_comps=ell_comps)\n",
        "\n",
        "    axis_ratio_list.append(axis_ratio)\n",
        "\n",
        "median_axis_ratio, lower_axis_ratio, upper_axis_ratio = af.marginalize(\n",
        "    parameter_list=axis_ratio_list,\n",
        "    sigma=3.0,\n",
        ")\n",
        "\n",
        "print(f\"axis_ratio = {median_axis_ratio} ({upper_axis_ratio} {lower_axis_ratio}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Samples Filtering (Advanced)__\n",
        "\n",
        "Our samples object has the results for all three parameters in our model. However, we might only be interested in the\n",
        "results of a specific parameter.\n",
        "\n",
        "The basic form of filtering specifies parameters via their path, which was printed above via the model and is printed \n",
        "again below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "print(\"Parameter paths in the model which are used for filtering:\")\n",
        "print(samples.model.paths)\n",
        "\n",
        "print(\"All parameters of the very first sample\")\n",
        "print(samples.parameter_lists[0])\n",
        "\n",
        "samples = samples.with_paths(\n",
        "    [\n",
        "        (\"galaxies\", \"lens\", \"mass\", \"einstein_radius\"),\n",
        "        (\"galaxies\", \"source\", \"bulge\", \"sersic_index\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"All parameters of the very first sample (containing only the lens mass's einstein radius and \"\n",
        "    \"source bulge's sersic index).\"\n",
        ")\n",
        "print(samples.parameter_lists[0])\n",
        "\n",
        "print(\n",
        "    \"Maximum Log Likelihood Model Instances (containing only the lens mass's einstein radius and \"\n",
        "    \"source bulge's sersic index):\\n\"\n",
        ")\n",
        "print(samples.max_log_likelihood(as_instance=False))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We specified each path as a list of tuples of strings. \n",
        "\n",
        "This is how the source code internally stores the path to different components of the model, but it is not \n",
        "consistent with the API used to compose a model.\n",
        "\n",
        "We can alternatively use the following API:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "samples = samples.with_paths(\n",
        "    [\"galaxies.lens.mass.einstein_radius\", \"galaxies.source.bulge.sersic_index\"]\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"All parameters of the very first sample (containing only the lens mass's einstein radius and \"\n",
        "    \"source bulge's sersic index).\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can alternatively filter the `Samples` object by removing all parameters with a certain path. Below, we remove\n",
        "the centres of the mass model to be left with 10 parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "print(\"Parameter paths in the model which are used for filtering:\")\n",
        "print(samples.model.paths)\n",
        "\n",
        "print(\"Parameters of first sample\")\n",
        "print(samples.parameter_lists[0])\n",
        "\n",
        "print(samples.model.total_free_parameters)\n",
        "\n",
        "samples = samples.without_paths(\n",
        "    [\n",
        "        # \"galaxies.lens.mass.centre\"),\n",
        "        \"galaxies.lens.mass.centre.centre_0\",\n",
        "        # \"galaxies.lens.mass.centre.centre_1),\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Parameters of first sample without the lens mass centre.\")\n",
        "print(samples.parameter_lists[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can keep and remove entire paths of the samples, for example keeping only the parameters of the lens or \n",
        "removing all parameters of the source's bulge."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "samples = samples.with_paths([\"galaxies.lens\"])\n",
        "print(\"Parameters of the first sample of the lens galaxy\")\n",
        "print(samples.parameter_lists[0])\n",
        "\n",
        "samples = result.samples\n",
        "samples = samples.without_paths([\"galaxies.source.bulge\"])\n",
        "print(\"Parameters of the first sample without the source's bulge\")\n",
        "print(samples.parameter_lists[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fin."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}