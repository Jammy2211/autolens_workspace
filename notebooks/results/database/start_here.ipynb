{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Database: Introduction\n",
        "======================\n",
        "\n",
        "The default behaviour of model-fitting results output is to be written to hard-disc in folders. These are simple to\n",
        "navigate and manually check.\n",
        "\n",
        "For small model-fitting tasks this is sufficient, however it does not scale well when performing many model fits to\n",
        "large datasets, because manual inspection of results becomes time consuming.\n",
        "\n",
        "All results can therefore be output to an sqlite3 (https://docs.python.org/3/library/sqlite3.html) relational database,\n",
        "meaning that results can be loaded into a Jupyter notebook or Python script for inspection, analysis and interpretation.\n",
        "This database supports advanced querying, so that specific model-fits (e.g., which fit a certain model or dataset) can\n",
        "be loaded.\n",
        "\n",
        "This script fits a sample of three simulated strong lenses using the same non-linear search. The results will be used\n",
        "to illustrate the database in the database tutorials that follow.\n",
        "\n",
        "__Model__\n",
        "\n",
        "The search fits each lens with:\n",
        "\n",
        " - An `Isothermal` `MassProfile` for the lens galaxy's mass.\n",
        " - An `Sersic` `LightProfile` for the source galaxy's light."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import json\n",
        "from os import path\n",
        "import autofit as af\n",
        "import autolens as al"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Unique Identifiers__\n",
        "\n",
        "Results output to hard-disk are contained in a folder named via a unique identifier (a \n",
        "random collection of characters, e.g. `8hds89fhndlsiuhnfiusdh`). The unique identifier changes if the model or \n",
        "search change, to ensure different fits to not overwrite one another on hard-disk.\n",
        "\n",
        "Each unique identifier is used to define every entry of the database as it is built. Unique identifiers therefore play \n",
        "the same vital role for the database of ensuring that every set of results written to it are unique.\n",
        "\n",
        "In this example, we fit 3 different datasets with the same search and model. Each `dataset_name` is therefore passed\n",
        "in as the search's `unique_tag` to ensure 3 separate sets of results for each model-fit are written to the .sqlite\n",
        "database.\n",
        "\n",
        "__Dataset__\n",
        "\n",
        "For each dataset we load it from hard-disc, set up its `Analysis` class and fit it with a non-linear search. \n",
        "\n",
        "We want each results to be stored in the database with an entry specific to the dataset. We'll use the `Dataset`'s name \n",
        "string to do this, so lets create a list of the 3 dataset names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_names = [\n",
        "    \"simple\",\n",
        "    \"lens_sersic\",\n",
        "    \"mass_power_law\",\n",
        "]\n",
        "\n",
        "pixel_scales = 0.1"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Results From Hard Disk__\n",
        "\n",
        "In this example, results will be first be written to hard-disk using the standard output directory structure and we\n",
        "will then build the database from these results. This behaviour is governed by us inputting `session=None`.\n",
        "\n",
        "If you have existing results you wish to build a database for, you can therefore adapt this example you to do this.\n",
        "\n",
        "Later in this example we show how results can also also be output directly to an .sqlite database, saving on hard-disk \n",
        "space. This will be acheived by setting `session` to something that is not `None`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "session = None\n",
        "\n",
        "for dataset_name in dataset_names:\n",
        "    \"\"\"\n",
        "    __Paths__\n",
        "\n",
        "    Set up the config and output paths.\n",
        "    \"\"\"\n",
        "    dataset_path = path.join(\"dataset\", \"imaging\", dataset_name)\n",
        "\n",
        "    \"\"\"\n",
        "    __Dataset__\n",
        "\n",
        "    Using the dataset path, load the data (image, noise-map, PSF) as an `Imaging` object from .fits files.\n",
        "\n",
        "    This `Imaging` object will be available via the aggregator. Note also that we give the dataset a `name` via the\n",
        "    command `name=dataset_name`. we'll use this name in the aggregator tutorials.\n",
        "    \"\"\"\n",
        "    dataset = al.Imaging.from_fits(\n",
        "        data_path=path.join(dataset_path, \"data.fits\"),\n",
        "        psf_path=path.join(dataset_path, \"psf.fits\"),\n",
        "        noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "        pixel_scales=pixel_scales,\n",
        "    )\n",
        "\n",
        "    \"\"\"\n",
        "    __Mask__\n",
        "\n",
        "    The `Mask2D` we fit this data-set with, which will be available via the aggregator.\n",
        "    \"\"\"\n",
        "    mask = al.Mask2D.circular(\n",
        "        shape_native=dataset.shape_native, pixel_scales=dataset.pixel_scales, radius=3.0\n",
        "    )\n",
        "\n",
        "    dataset = dataset.apply_mask(mask=mask)\n",
        "\n",
        "    \"\"\"\n",
        "    __Info__\n",
        "\n",
        "    Information about the model-fit that is not part included in the model-fit itself can be made accessible via the \n",
        "    database by passing an `info` dictionary. \n",
        "\n",
        "    Below we write info on the dataset`s (hypothetical) data of observation and exposure time, which we will later show\n",
        "    the database can access. \n",
        "\n",
        "    For fits to large datasets this ensures that all relevant information for interpreting results is accessible.\n",
        "    \"\"\"\n",
        "    with open(path.join(dataset_path, \"info.json\")) as json_file:\n",
        "        info = json.load(json_file)\n",
        "\n",
        "    \"\"\"\n",
        "    __Model__\n",
        "\n",
        "    Set up the model as per usual, and will see in tutorial 3 why we have included `disk=None`.\n",
        "    \"\"\"\n",
        "    model = af.Collection(\n",
        "        galaxies=af.Collection(\n",
        "            lens=af.Model(al.Galaxy, redshift=0.5, mass=al.mp.Isothermal),\n",
        "            source=af.Model(\n",
        "                al.Galaxy, redshift=1.0, bulge=al.lp_linear.SersicCore, disk=None\n",
        "            ),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    \"\"\"\n",
        "    The `unique_tag` below uses the `dataset_name` to alter the unique identifier, which as we have seen is also \n",
        "    generated depending on the search settings and model. In this example, all three model fits use an identical \n",
        "    search and model, so this `unique_tag` is key for ensuring 3 separate sets of results for each model-fit are \n",
        "    stored in the output folder and written to the .sqlite database. \n",
        "    \"\"\"\n",
        "    search = af.Nautilus(\n",
        "        path_prefix=path.join(\"database\"),\n",
        "        name=\"database_example\",\n",
        "        unique_tag=dataset_name,  # This makes the unique identifier use the dataset name\n",
        "        session=session,  # This can instruct the search to write to the .sqlite database.\n",
        "        n_live=100,\n",
        "        number_of_cores=6,\n",
        "        n_like_max=5000,\n",
        "    )\n",
        "\n",
        "    analysis = al.AnalysisImaging(dataset=dataset)\n",
        "\n",
        "    search.fit(analysis=analysis, model=model, info=info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Building a Database File From an Output Folder__\n",
        "\n",
        "The fits above wrote the results to hard-disk in folders, not as an .sqlite database file. \n",
        "\n",
        "We build the database below, where the `database_name` corresponds to the name of your output folder and is also the \n",
        "name of the `.sqlite` database file that is created.\n",
        "\n",
        "If you are fitting a relatively small number of datasets (e.g. 10-100) having all results written to hard-disk (e.g. \n",
        "for quick visual inspection) and using the database for sample wide analysis is beneficial.\n",
        "\n",
        "We can optionally only include completed model-fits but setting `completed_only=True`.\n",
        "\n",
        "If you inspect the `output` folder, you will see a `database.sqlite` file which contains the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "database_name = \"database\"\n",
        "\n",
        "agg = af.Aggregator.from_database(\n",
        "    filename=f\"{database_name}.sqlite\", completed_only=False\n",
        ")\n",
        "\n",
        "agg.add_directory(directory=path.join(\"output\", database_name))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Writing Directly To Database__\n",
        "\n",
        "Results can be written directly to the .sqlite database file, skipping output to hard-disk entirely, by creating\n",
        "a session and passing this to the non-linear search.\n",
        "\n",
        "The code below shows how to do this, but it is commented out to avoid rerunning the non-linear searches.\n",
        "\n",
        "This is ideal for tasks where model-fits to hundreds or thousands of datasets are performed, as it becomes unfeasible\n",
        "to inspect the results of all fits on the hard-disk. \n",
        "\n",
        "Our recommended workflow is to set up database analysis scripts using ~10 model-fits, and then scaling these up\n",
        "to large samples by writing directly to the database."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# session = af.db.open_database(\"database.sqlite\")\n",
        "#\n",
        "# search = af.Nautilus(\n",
        "#     path_prefix=path.join(\"database\"),\n",
        "#     name=\"database_example\",\n",
        "#     unique_tag=dataset_name,  # This makes the unique identifier use the dataset name\n",
        "#     session=session,  # This can instruct the search to write to the .sqlite database.\n",
        "#     n_live=100,\n",
        "# )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Files__\n",
        "\n",
        "When performing fits which output results to hard-disc, a `files` folder is created containing .json / .csv files of \n",
        "the model, samples, search, etc.\n",
        "\n",
        "These are the files that are written to the database, which the aggregator loads via the database in order to make \n",
        "them accessible in a Python script or Jupyter notebook.\n",
        "\n",
        "You can checkout the output folder created by this fit to see these files.\n",
        "\n",
        "Below, we will access these results using the aggregator's `values` method. A full list of what can be loaded is\n",
        "as follows:\n",
        "\n",
        " - `model`: The `model` defined above and used in the model-fit (`model.json`).\n",
        " - `search`: The non-linear search settings (`search.json`).\n",
        " - `samples`: The non-linear search samples (`samples.csv`).\n",
        " - `samples_info`: Additional information about the samples (`samples_info.json`).\n",
        " - `samples_summary`: A summary of key results of the samples (`samples_summary.json`).\n",
        " - `info`: The info dictionary passed to the search (`info.json`).\n",
        " - `covariance`: The inferred covariance matrix (`covariance.csv`).\n",
        " - `cosmology`: The cosmology used by the fit (`cosmology.json`).\n",
        " - `settings_inversion`: The settings associated with a inversion if used (`settings_inversion.json`).\n",
        " - `dataset/data`: The data that is fitted (`data.fits`).\n",
        " - `dataset/noise_map`: The noise-map (`noise_map.fits`).\n",
        " - `dataset/psf`: The Point Spread Function (`psf.fits`).\n",
        " - `dataset/mask`: The mask applied to the data (`mask.fits`).\n",
        " - `dataset/settings`: The settings associated with the dataset (`settings.json`).\n",
        "\n",
        "The `samples` and `samples_summary` results contain a lot of repeated information. The `samples` result contains\n",
        "the full non-linear search samples, for example every parameter sample and its log likelihood. The `samples_summary`\n",
        "contains a summary of the results, for example the maximum log likelihood model and error estimates on parameters\n",
        "at 1 and 3 sigma confidence.\n",
        "\n",
        "Accessing results via the `samples_summary` is much faster, because as it does reperform calculations using the full \n",
        "list of samples. Therefore, if the result you want is accessible via the `samples_summary` you should use it\n",
        "but if not you can revert to the `samples.\n",
        "\n",
        "__Generators__\n",
        "\n",
        "Before using the aggregator to inspect results, lets discuss Python generators. \n",
        "\n",
        "A generator is an object that iterates over a function when it is called. The aggregator creates all of the objects \n",
        "that it loads from the database as generators (as opposed to a list, or dictionary, or another Python type).\n",
        "\n",
        "This is because generators are memory efficient, as they do not store the entries of the database in memory \n",
        "simultaneously. This contrasts objects like lists and dictionaries, which store all entries in memory all at once. \n",
        "If you fit a large number of datasets, lists and dictionaries will use a lot of memory and could crash your computer!\n",
        "\n",
        "Once we use a generator in the Python code, it cannot be used again. To perform the same task twice, the \n",
        "generator must be remade it. This cookbook therefore rarely stores generators as variables and instead uses the \n",
        "aggregator to create each generator at the point of use.\n",
        "\n",
        "To create a generator of a specific set of results, we use the `values` method. This takes the `name` of the\n",
        "object we want to create a generator of, for example inputting `name=samples` will return the results `Samples`\n",
        "object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples_gen = agg.values(\"samples\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By converting this generator to a list and printing it, it is a list of 3 `SamplesNest` objects, corresponding to \n",
        "the 3 model-fits performed above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Samples:\\n\")\n",
        "print(samples_gen)\n",
        "print(\"Total Samples Objects = \", len(agg), \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "The model used to perform the model fit for each of the 3 datasets can be loaded via the aggregator and printed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_gen = agg.values(\"model\")\n",
        "\n",
        "for model in model_gen:\n",
        "    print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "The non-linear search used to perform the model fit can be loaded via the aggregator and printed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search_gen = agg.values(\"search\")\n",
        "\n",
        "for search in search_gen:\n",
        "    print(search)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Samples__\n",
        "\n",
        "The `Samples` class contains all information on the non-linear search samples, for example the value of every parameter\n",
        "sampled using the fit or an instance of the maximum likelihood model.\n",
        "\n",
        "The `Samples` class is described fully in the results cookbook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for samples in agg.values(\"samples\"):\n",
        "    print(\"The tenth sample`s third parameter\")\n",
        "    print(samples.parameter_lists[9][2], \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Therefore, by loading the `Samples` via the database we can now access the results of the fit to each dataset.\n",
        "\n",
        "For example, we can plot the maximum likelihood model for each of the 3 model-fits performed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ml_vector = [\n",
        "    samps.max_log_likelihood(as_instance=False) for samps in agg.values(\"samples\")\n",
        "]\n",
        "\n",
        "print(\"Max Log Likelihood Model Parameter Lists: \\n\")\n",
        "print(ml_vector, \"\\n\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All remaining methods accessible by `agg.values` are described in the other database examples.\n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "This example illustrates how to use the database.\n",
        "\n",
        "The API above can be combined with the `results/examples` scripts in order to use the database to load results and\n",
        "perform analysis on them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}