{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results: Fits Make\n",
        "==================\n",
        "\n",
        "This example is a results workflow example, which means it provides tool to set up an effective workflow inspecting\n",
        "and interpreting the large libraries of modeling results.\n",
        "\n",
        "In this tutorial, we use the aggregator to load .fits files output by a model-fit, extract hdu images and create\n",
        "new .fits files, for example all to a single folder on your hard-disk.\n",
        "\n",
        "For example, a common use case is extracting an image from `model_galaxy_images.fits` of many fits and putting them\n",
        "into a single .fits file on your hard-disk. If you have modeled 100+ datasets, you can then inspect all model images\n",
        "in DS9 in .fits format n a single folder, which is more efficient than clicking throughout the `output` open each\n",
        ".fits file one-by-one.\n",
        "\n",
        "The most common use of .fits splciing is where multiple observations of the same galaxy are analysed, for example\n",
        "at different wavelengths, where each fit outputs a different .fits files. The model images of each fit to each\n",
        "wavelength can then be packaged up into a single .fits file.\n",
        "\n",
        "This enables the results of many model-fits to be concisely visualized and inspected, which can also be easily passed\n",
        "on to other collaborators.\n",
        "\n",
        "Internally, splicing uses standard Astorpy functions to open, edit and save .fit files.\n",
        "\n",
        "__CSV, Png and Fits__\n",
        "\n",
        "Workflow functionality closely mirrors the `png_make.py` and `fits_make.py`  examples, which load results of\n",
        "model-fits and output th em as .png files and .fits files to quickly summarise results.\n",
        "\n",
        "The same initial fit creating results in a folder called `results_folder_csv_png_fits` is therefore used.\n",
        "\n",
        "__Interferometer__\n",
        "\n",
        "This script can easily be adapted to analyse the results of charge injection imaging model-fits.\n",
        "\n",
        "The only entries that needs changing are:\n",
        "\n",
        " - `ImagingAgg` -> `InterferometerAgg`.\n",
        " - `FitImagingAgg` -> `FitInterferometerAgg`.\n",
        " - `ImagingPlotter` -> `InterferometerPlotter`.\n",
        " - `FitImagingPlotter` -> `FitInterferometerPlotter`.\n",
        "\n",
        "Quantities specific to an interfometer, for example its uv-wavelengths real space mask, are accessed using the same API\n",
        "(e.g. `values(\"dataset.uv_wavelengths\")` and `.values{\"dataset.real_space_mask\")).\n",
        "\n",
        "__Database File__\n",
        "\n",
        "The aggregator can also load results from a `.sqlite` database file.\n",
        "\n",
        "This is beneficial when loading results for large numbers of model-fits (e.g. more than hundreds)\n",
        "because it is optimized for fast querying of results.\n",
        "\n",
        "See the package `results/database` for a full description of how to set up the database and the benefits it provides,\n",
        "especially if loading results from hard-disk is slow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from pathlib import Path\n",
        "\n",
        "import autofit as af\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fit__\n",
        "\n",
        "The code below performs a model-fit using nautilus. \n",
        "\n",
        "You should be familiar with modeling already, if not read the `modeling/start_here.py` script before reading this one!\n",
        "\n",
        "__Unique Tag__\n",
        "\n",
        "One thing to note is that the `unique_tag` of the search is given the name of the dataset with an index for the\n",
        "fit of 0 and 1. \n",
        "\n",
        "This `unique_tag` names the fit in a descriptive and human-readable way, which we will exploit to make our .fits files\n",
        "more descriptive and easier to interpret."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for i in range(2):\n",
        "    dataset_name = \"simple__no_lens_light\"\n",
        "    dataset_path = Path(\"dataset\") / \"imaging\" / dataset_name\n",
        "\n",
        "    dataset = al.Imaging.from_fits(\n",
        "        data_path=dataset_path / \"data.fits\",\n",
        "        psf_path=dataset_path / \"psf.fits\",\n",
        "        noise_map_path=dataset_path / \"noise_map.fits\",\n",
        "        pixel_scales=0.1,\n",
        "    )\n",
        "\n",
        "    mask_radius = 3.0\n",
        "\n",
        "    mask = al.Mask2D.circular(\n",
        "        shape_native=dataset.shape_native,\n",
        "        pixel_scales=dataset.pixel_scales,\n",
        "        radius=mask_radius,\n",
        "    )\n",
        "\n",
        "    dataset = dataset.apply_mask(mask=mask)\n",
        "\n",
        "    bulge = al.model_util.mge_model_from(\n",
        "        mask_radius=mask_radius,\n",
        "        total_gaussians=20,\n",
        "        gaussian_per_basis=1,\n",
        "        centre_prior_is_uniform=False,\n",
        "    )\n",
        "\n",
        "    model = af.Collection(\n",
        "        galaxies=af.Collection(\n",
        "            lens=af.Model(\n",
        "                al.Galaxy,\n",
        "                redshift=0.5,\n",
        "                mass=al.mp.Isothermal,\n",
        "                shear=al.mp.ExternalShear,\n",
        "            ),\n",
        "            source=af.Model(al.Galaxy, redshift=1.0, bulge=bulge, disk=None),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    search = af.Nautilus(\n",
        "        path_prefix=Path(\"results_folder_csv_png_fits\"),\n",
        "        name=\"results\",\n",
        "        unique_tag=f\"simple__no_lens_light_{i}\",\n",
        "        n_live=100,\n",
        "        iterations_per_quick_update=10000,\n",
        "    )\n",
        "\n",
        "    class AnalysisLatent(al.AnalysisImaging):\n",
        "\n",
        "        LATENT_KEYS = [\n",
        "            \"galaxies.lens.shear.magnitude\",\n",
        "            \"galaxies.lens.shear.angle\",\n",
        "        ]\n",
        "\n",
        "        def compute_latent_variables(self, parameters, model):\n",
        "            \"\"\"\n",
        "            A latent variable is not a model parameter but can be derived from the model. Its value and errors may be\n",
        "            of interest and aid in the interpretation of a model-fit.\n",
        "\n",
        "            This code implements a simple example of a latent variable, the magn\n",
        "\n",
        "            By overwriting this method we can manually specify latent variables that are calculated and output to\n",
        "            a `latent.csv` file, which mirrors the `samples.csv` file.\n",
        "\n",
        "            In the example below, the `latent.csv` file will contain at least two columns with the shear magnitude and\n",
        "            angle sampled by the non-linear search.\n",
        "\n",
        "            This function is called for every non-linear search sample, where the `instance` passed in corresponds to\n",
        "            each sample.\n",
        "\n",
        "            You can add your own custom latent variables here, if you have particular quantities that you\n",
        "            would like to output to the `latent.csv` file.\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            parameters : array-like\n",
        "                The parameter vector of the model sample. This will typically come from the non-linear search.\n",
        "                Inside this method it is mapped back to a model instance via `model.instance_from_vector`.\n",
        "            model : Model\n",
        "                The model object defining how the parameter vector is mapped to an instance. Passed explicitly\n",
        "                so that this function can be used inside JAX transforms (`vmap`, `jit`) with `functools.partial`.\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            A dictionary mapping every latent variable name to its value.\n",
        "\n",
        "            \"\"\"\n",
        "            instance = model.instance_from_vector(vector=parameters)\n",
        "\n",
        "            if hasattr(instance.galaxies.lens, \"shear\"):\n",
        "                magnitude, angle = al.convert.shear_magnitude_and_angle_from(\n",
        "                    gamma_1=instance.galaxies.lens.shear.gamma_1,\n",
        "                    gamma_2=instance.galaxies.lens.shear.gamma_2,\n",
        "                )\n",
        "\n",
        "            return (magnitude, angle)\n",
        "\n",
        "    analysis = AnalysisLatent(dataset=dataset)\n",
        "\n",
        "    result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Workflow Paths__\n",
        "\n",
        "The workflow examples are designed to take large libraries of results and distill them down to the key information\n",
        "required for your science, which are therefore placed in a single path for easy access.\n",
        "\n",
        "The `workflow_path` specifies where these files are output, in this case the .fits files containing the key \n",
        "results we require."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "workflow_path = Path(\"output\") / \"results_folder_csv_png_fits\" / \"workflow_make_example\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Aggregator__\n",
        "\n",
        "Set up the aggregator as shown in `start_here.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from autofit.aggregator.aggregator import Aggregator\n",
        "\n",
        "agg = Aggregator.from_directory(\n",
        "    directory=Path(\"output\") / \"results_folder_csv_png_fits\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract the `AggregateFITS` object, which has specific functions for loading .fits files and outputting results in \n",
        ".fits format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg_fits = af.AggregateFITS(aggregator=agg)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Extract Images__\n",
        "\n",
        "We now extract 2 images from the `fit.fits` file and combine them together into a single .fits file.\n",
        "\n",
        "We will extract the `model_image` and `residual_map` images, which are images you are used to\n",
        "plotting and inspecting in the `output` folder of a model-fit and can load and inspect in DS9 from the file\n",
        "`fit.fits`.\n",
        "\n",
        "By inspecting `fit.fits` you will see it contains four images which each have a an `ext_name`: `model_image`,\n",
        "`residual_map`, `normalized_residual_map`, `chi_squared_map`.\n",
        "\n",
        "We do this by simply passing the `agg_fits.extract_fits` method the name of the fits file we load from `fits.fit`\n",
        "and the `ext_name` of what we extract.\n",
        "\n",
        "This runs on all results the `Aggregator` object has loaded from the `output` folder, meaning that for this example\n",
        "where two model-fits are loaded, the `image` object contains two images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hdu_list = agg_fits.extract_fits(\n",
        "    hdus=[al.agg.fits_fit.model_data, al.agg.fits_fit.residual_map],\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Output Single Fits__\n",
        "\n",
        "The `image` object which has been extracted is an `astropy` `Fits` object, which we use to save the .fits to the \n",
        "hard-disk.\n",
        "\n",
        "The .fits has 4 hdus, the `model_image` and `residual_map` for the two datasets fitted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hdu_list.writeto(\"fits_make_single.fits\", overwrite=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Output to Folder__\n",
        "\n",
        "An alternative way to output the .fits files is to output them as single .fits files for each model-fit in a single \n",
        "folder, which is done using the `output_to_folder` method.\n",
        "\n",
        "It can sometimes be easier and quicker to inspect the results of many model-fits when they are output to individual\n",
        "files in a folder, as using an IDE you can click load and flick through the images. This contrasts a single .png\n",
        "file you scroll through, which may be slower to load and inspect.\n",
        "\n",
        "__Naming Convention__\n",
        "\n",
        "We require a naming convention for the output files. In this example, we have two model-fits, therefore two .fits\n",
        "files are going to be output.\n",
        "\n",
        "One way to name the .fits files is to use the `unique_tag` of the search, which is unique to every model-fit. For\n",
        "the search above, the `unique_tag` was `simple_0` and `simple_1`, therefore this will informatively name the .fits\n",
        "files the names of the datasets.\n",
        "\n",
        "We achieve this behaviour by inputting `name=\"unique_tag\"` to the `output_to_folder` method. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg_fits.output_to_folder(\n",
        "    folder=workflow_path,\n",
        "    name=\"unique_tag\",\n",
        "    hdus=[al.agg.fits_fit.model_data, al.agg.fits_fit.residual_map],\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `name` can be any search attribute, for example the `name` of the search, the `path_prefix` of the search, etc,\n",
        "if they will give informative names to the .fits files.\n",
        "\n",
        "You can also manually input a list of names, one for each fit, if you want to name the .fits files something else.\n",
        "However, the list must be the same length as the number of fits in the aggregator, and you may not be certain of the\n",
        "order of fits in the aggregator and therefore will need to extract this information, for example by printing the\n",
        "`unique_tag` of each search (or another attribute containing the dataset name)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print([search.unique_tag for search in agg.values(\"search\")])\n",
        "\n",
        "agg_fits.output_to_folder(\n",
        "    folder=workflow_path,\n",
        "    name=[\"hi_0.fits\", \"hi_1.fits\"],\n",
        "    hdus=[al.agg.fits_fit.model_data, al.agg.fits_fit.residual_map],\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__CSV Files__\n",
        "\n",
        "In the results `image` folder .csv files containing the information to visualize aspects of a result may be present.\n",
        "\n",
        "A common example is the file `source_plane_reconstruction_0.csv`, which contains the y and x coordinates of the \n",
        "pixelization mesh, the reconstruct values and the noise map of these values.\n",
        "\n",
        "The `AggregateFITS` object has a method `extract_csv` which extracts this table from each .csv file in the results,\n",
        "returning the extracted data as a list of dictionaries. This can then be used to visualize the data, and output\n",
        "it to a .fits file elsewhere.\n",
        "\n",
        "Below, we demonstrate a common use case for a pixelization. Each .csv file is loaded, benefitting from the fact\n",
        "that because it stores the irregular mesh values it is the most accurate way to store the data whilst also using\n",
        "much less hard-disk space than, for example. converting it to a 2D array and .fits file. We then use the\n",
        "loaded values to interpolate the data onto a regular grid and output it to .fits files in a folder.\n",
        "\n",
        "The code below is commented out because the model does not use a pixelization, but it does work if a\n",
        "pixelization is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# reconstruction_dict_list = agg_fits.extract_csv(\n",
        "#     filename=\"source_plane_reconstruction_0\",\n",
        "# )\n",
        "#\n",
        "# from scipy.interpolate import griddata\n",
        "#\n",
        "# for i, reconstruction_dict in enumerate(reconstruction_dict_list):\n",
        "#\n",
        "#     y = reconstruction_dict[\"y\"]\n",
        "#     x = reconstruction_dict[\"x\"]\n",
        "#     values = reconstruction_dict[\"reconstruction\"]\n",
        "#\n",
        "#     points = np.stack(\n",
        "#         arrays=(reconstruction_dict[\"x\"], reconstruction_dict[\"y\"]), axis=-1\n",
        "#     )\n",
        "#\n",
        "#     interpolation_grid = al.Grid2D.from_extent(\n",
        "#         extent=(-1.0, 1.0, -1.0, 1.0), shape_native=(201, 201)\n",
        "#     )\n",
        "#\n",
        "#     interpolated_array = griddata(points=points, values=values, xi=interpolation_grid)\n",
        "#\n",
        "#     al.output_to_fits(\n",
        "#         values=interpolated_array,\n",
        "#         file_path=workflow_path / f\"interpolated_reconstruction_{i}.fits\",\n",
        "#     )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Add Extra Fits__\n",
        "\n",
        "We can also add an extra .fits image to the extracted .fits file, for example an RGB image of the dataset.\n",
        "\n",
        "We create an image of shape (1, 2) and add the RGB image to the left of the subplot, so that the new subplot has\n",
        "shape (1, 3).\n",
        "\n",
        "When we add a single .png, we cannot extract or make it, it simply gets added to the subplot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# image_rgb = Image.open(Path(dataset_path, \"rgb.png\"))\n",
        "#\n",
        "# image = agg_fits.extract_fits(\n",
        "#     al.agg.subplot_dataset.data,\n",
        "#     al.agg.subplot_dataset.psf_log_10,\n",
        "#     subplot_shape=(1, 2),\n",
        "# )\n",
        "\n",
        "# image = al.add_image_to_left(image, additional_img)\n",
        "\n",
        "# image.save(\"png_make_with_rgb.png\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Custom Fits Files in Analysis__\n",
        "\n",
        "Describe how a user can extend the `Analysis` class to compute custom images that are output to the .png files,\n",
        "which they can then extract and make together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# %%\n",
        "'''\n",
        "__Path Navigation__\n",
        "\n",
        "Example combinig `fit.fits` from `source_lp[1]` and `mass_total[0]`.\n",
        "'''"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}