{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results: CSV\n",
        "============\n",
        "\n",
        "This example is a results workflow example, which means it provides tool to set up an effective workflow inspecting\n",
        "and interpreting the large libraries of modeling results.\n",
        "\n",
        "In this tutorial, we use the aggregator to load the results of model-fits and output them in a single .csv file.\n",
        "\n",
        "This enables the results of many model-fits to be concisely summarised and inspected in a single table, which\n",
        "can also be easily passed on to other collaborators.\n",
        "\n",
        "__CSV, Png and Fits__\n",
        "\n",
        "Workflow functionality closely mirrors the `png_make.py` and `fits_make.py`  examples, which load results of\n",
        "model-fits and output th em as .png files and .fits files to quickly summarise results. \n",
        "\n",
        "The same initial fit creating results in a folder called `results_folder_csv_png_fits` is therefore used.\n",
        "\n",
        "__Interferometer__\n",
        "\n",
        "This script can easily be adapted to analyse the results of charge injection imaging model-fits.\n",
        "\n",
        "The only entries that needs changing are:\n",
        "\n",
        " - `ImagingAgg` -> `InterferometerAgg`.\n",
        " - `FitImagingAgg` -> `FitInterferometerAgg`.\n",
        " - `ImagingPlotter` -> `InterferometerPlotter`.\n",
        " - `FitImagingPlotter` -> `FitInterferometerPlotter`.\n",
        "\n",
        "Quantities specific to an interfometer, for example its uv-wavelengths real space mask, are accessed using the same API\n",
        "(e.g. `values(\"dataset.uv_wavelengths\")` and `.values{\"dataset.real_space_mask\")).\n",
        "\n",
        "__Database File__\n",
        "\n",
        "The aggregator can also load results from a `.sqlite` database file.\n",
        "\n",
        "This is beneficial when loading results for large numbers of model-fits (e.g. more than hundreds)\n",
        "because it is optimized for fast querying of results.\n",
        "\n",
        "See the package `results/database` for a full description of how to set up the database and the benefits it provides,\n",
        "especially if loading results from hard-disk is slow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from pathlib import Path\n",
        "from os import path\n",
        "\n",
        "import autofit as af\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fit__\n",
        "\n",
        "The code below performs a model-fit using nautilus. \n",
        "\n",
        "You should be familiar with modeling already, if not read the `modeling/start_here.py` script before reading this one!\n",
        "\n",
        "__Unique Tag__\n",
        "\n",
        "One thing to note is that the `unique_tag` of the search is given the name of the dataset with an index for the\n",
        "fit of 0 and 1. \n",
        "\n",
        "This `unique_tag` names the fit in a descriptive and human-readable way, which we will exploit to make our .csv files\n",
        "more descriptive and easier to interpret."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for i in range(2):\n",
        "    dataset_name = \"simple__no_lens_light\"\n",
        "    dataset_path = path.join(\"dataset\", \"imaging\", dataset_name)\n",
        "\n",
        "    dataset = al.Imaging.from_fits(\n",
        "        data_path=path.join(dataset_path, \"data.fits\"),\n",
        "        psf_path=path.join(dataset_path, \"psf.fits\"),\n",
        "        noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "        pixel_scales=0.1,\n",
        "    )\n",
        "\n",
        "    mask = al.Mask2D.circular(\n",
        "        shape_native=dataset.shape_native, pixel_scales=dataset.pixel_scales, radius=3.0\n",
        "    )\n",
        "\n",
        "    dataset = dataset.apply_mask(mask=mask)\n",
        "\n",
        "    model = af.Collection(\n",
        "        galaxies=af.Collection(\n",
        "            lens=af.Model(al.Galaxy, redshift=0.5, mass=al.mp.Isothermal),\n",
        "            source=af.Model(\n",
        "                al.Galaxy, redshift=1.0, bulge=al.lp_linear.SersicCore, disk=None\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    search = af.Nautilus(\n",
        "        path_prefix=path.join(\"results_folder_csv_png_fits\"),\n",
        "        name=\"results\",\n",
        "        unique_tag=f\"simple__no_lens_light_{i}\",\n",
        "        n_live=100,\n",
        "        number_of_cores=1,\n",
        "    )\n",
        "\n",
        "    class AnalysisLatent(al.AnalysisImaging):\n",
        "        def compute_latent_variables(self, instance):\n",
        "            return {\"example_latent\": instance.galaxies.lens.mass.einstein_radius * 2.0}\n",
        "\n",
        "    analysis = AnalysisLatent(dataset=dataset)\n",
        "\n",
        "    result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Workflow Paths__\n",
        "\n",
        "The workflow examples are designed to take large libraries of results and distill them down to the key information\n",
        "required for your science, which are therefore placed in a single path for easy access.\n",
        "\n",
        "The `workflow_path` specifies where these files are output, in this case the .csv files which summarise the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "workflow_path = Path(\"output\") / \"results_folder_csv_png_fits\" / \"workflow_make_example\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Aggregator__\n",
        "\n",
        "Set up the aggregator as shown in `start_here.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from autofit.aggregator.aggregator import Aggregator\n",
        "\n",
        "agg = Aggregator.from_directory(\n",
        "    directory=path.join(\"output\", \"results_folder_csv_png_fits\"),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract the `AggregateCSV` object, which has specific functions for outputting results in a CSV format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg_csv = af.AggregateCSV(aggregator=agg)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Adding CSV Columns_\n",
        "\n",
        "We first make a simple .csv which contains two columns, corresponding to the inferred median PDF values for\n",
        "the y centre of the mass of the lens galaxy and its einstein radius.\n",
        "\n",
        "To do this, we use the `add_column` method, which adds a column to the .csv file we write at the end. Every time\n",
        "we call `add_column` we add a new column to the .csv file.\n",
        "\n",
        "Note the API for the `centre`, which is a tuple parameter and therefore needs for `centre_0` to be specified.\n",
        "\n",
        "The `results_folder_csv_png_fits` contained two model-fits to two different datasets, meaning that each `add_column` \n",
        "call will add three rows, corresponding to the three model-fits.\n",
        "\n",
        "This adds the median PDF value of the parameter to the .csv file, we show how to add other values later in this script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg_csv.add_column(argument=\"galaxies.lens.mass.centre.centre_0\")\n",
        "agg_csv.add_column(argument=\"galaxies.lens.mass.einstein_radius\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Saving the CSV__\n",
        "\n",
        "We can now output the results of all our model-fits to the .csv file, using the `save` method.\n",
        "\n",
        "This will output in your current working directory (e.g. the `autolens_workspace/output.results_folder_csv_png_fits`) \n",
        "as a .csv file containing the median PDF values of the parameters, have a quick look now to see the format of \n",
        "the .csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg_csv.save(path=workflow_path / \"csv_simple.csv\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Customizing CSV Headers__\n",
        "\n",
        "The headers of the .csv file are by default the argument input above based on the model. \n",
        "\n",
        "However, we can customize these headers using the `name` input of the `add_column` method, for example making them\n",
        "shorter or more readable.\n",
        "\n",
        "We recreate the `agg_csv` first, so that we begin adding columns to a new .csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg_csv = af.AggregateCSV(aggregator=agg)\n",
        "\n",
        "agg_csv.add_column(\n",
        "    argument=\"galaxies.lens.mass.centre.centre_0\",\n",
        "    name=\"mass_centre_0\",\n",
        ")\n",
        "agg_csv.add_column(\n",
        "    argument=\"galaxies.lens.mass.einstein_radius\",\n",
        "    name=\"mass_einstein_radius\",\n",
        ")\n",
        "\n",
        "agg_csv.save(path=workflow_path / \"csv_simple_custom_headers.csv\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Maximum Likelihood Values__\n",
        "\n",
        "We can also output the maximum likelihood values of each parameter to the .csv file, using the `use_max_log_likelihood`\n",
        "input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg_csv = af.AggregateCSV(aggregator=agg)\n",
        "\n",
        "agg_csv.add_column(\n",
        "    argument=\"galaxies.lens.mass.einstein_radius\",\n",
        "    name=\"mass_einstein_radius_max_lh\",\n",
        "    use_max_log_likelihood=True,\n",
        ")\n",
        "\n",
        "agg_csv.save(path=workflow_path / \"csv_simple_max_likelihood.csv\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Errors__\n",
        "\n",
        "We can also output PDF values at a given sigma confidence of each parameter to the .csv file, using \n",
        "the `use_values_at_sigma` input and specifying the sigma confidence.\n",
        "\n",
        "Below, we add the values at 3.0 sigma confidence to the .csv file, in order to compute the errors you would \n",
        "subtract the median value from these values.\n",
        "\n",
        "The method below adds two columns to the .csv file, corresponding to the values at the lower and upper sigma values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# agg_csv = af.AggregateCSV(aggregator=agg)\n",
        "#\n",
        "# agg_csv.add_column(\n",
        "#     argument=\"galaxies.lens.mass.effective_radius\",\n",
        "#     name=\"bulge_effective_radius_sigma\",\n",
        "#     use_values_at_sigma=3.0,\n",
        "# )\n",
        "#\n",
        "# agg_csv.save(path=\"csv_simple_errors.csv\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Column Label List__\n",
        "\n",
        "We can add a list of values to the .csv file, provided the list is the same length as the number of model-fits\n",
        "in the aggregator.\n",
        "\n",
        "A useful example is adding the name of every dataset to the .csv file in a column on the left, indicating \n",
        "which dataset each row corresponds to.\n",
        "\n",
        "To make this list, we use the `Aggregator` to loop over the `search` objects and extract their `unique_tag`'s, which \n",
        "when we fitted the model above used the dataset names. This API can also be used to extract the `name` or `path_prefix`\n",
        "of the search and build an informative list for the names of the subplots.\n",
        "\n",
        "We then pass the column `name` and this list to the `add_label_column` method, which will add a column to the .csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg_csv = af.AggregateCSV(aggregator=agg)\n",
        "\n",
        "unique_tag_list = [search.unique_tag for search in agg.values(\"search\")]\n",
        "\n",
        "agg_csv.add_label_column(\n",
        "    name=\"lens_name\",\n",
        "    values=unique_tag_list,\n",
        ")\n",
        "\n",
        "agg_csv.save(path=workflow_path / \"csv_simple_dataset_name.csv\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Latent Variables__\n",
        "\n",
        "Latent variables are not free model parameters but can be derived from the model, and they are described fully in\n",
        "?.\n",
        "\n",
        "This example was run with a latent variable called `example_latent`, and below we show that this latent variable\n",
        "can be added to the .csv file using the same API as above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# agg_csv = af.AggregateCSV(aggregator=agg)\n",
        "#\n",
        "# agg_csv.add_column(\n",
        "#     argument=\"example_latent\",\n",
        "# )\n",
        "#\n",
        "# agg_csv.save(path=workflow_path / \"csv_example_latent.csv\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Computed Columns__\n",
        "\n",
        "We can also add columns to the .csv file that are computed from the non-linear search samples (e.g. the nested sampling\n",
        "samples), for example a value derived from the median PDF instance values of the model.\n",
        "\n",
        "To do this, we write a function which is input into the `add_computed_column` method, where this function takes the\n",
        "median PDF instance as input and returns the computed value.\n",
        "\n",
        "Below, we add a trivial example of a computed column, where the median PDF value that is twice lens Einstein radius\n",
        "is computed and added to the .csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg_csv = af.AggregateCSV(aggregator=agg)\n",
        "\n",
        "\n",
        "def einstein_radius_x2_from(samples):\n",
        "    instance = samples.median_pdf()\n",
        "\n",
        "    return 2.0 * instance.galaxies.lens.mass.einstein_radius\n",
        "\n",
        "\n",
        "agg_csv.add_computed_column(\n",
        "    name=\"bulge_einstein_radius_x2_computed\",\n",
        "    compute=einstein_radius_x2_from,\n",
        ")\n",
        "\n",
        "agg_csv.save(path=workflow_path / \"csv_computed_columns.csv\")\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}