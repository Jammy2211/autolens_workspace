{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Misc: Custom Analysis\n",
        "=====================\n",
        "\n",
        "Users familiar with **PyAutoLens** will have seen that `Analysis` classes are used to performed lens modeling of\n",
        "different datasets. For example, the `AnalysisImaging` class fits imaging datasets, the `AnalysisInterferometer` class\n",
        "fits interferometer datasets.\n",
        "\n",
        "You may have a dataset which you want to use **PyAutoLenss**'s lensing capabilities to model, but which does not fit\n",
        "into one of the standard `Analysis` classes.\n",
        "\n",
        "A good example (at the time of writing this script) is fitting a weak lensing shear catalogue with a model of the lens\n",
        "galaxy's mass. **PyAutoLens** as the lensing capabilities to produce the shears of a mass model, but does not have an\n",
        "`Analysis` class to fit these shears to a dataset.\n",
        "\n",
        "This example demonstrates how you can write your own `Analysis` class to fit a dataset with **PyAutoLens**.\n",
        "\n",
        "__PyAutoFit__\n",
        "\n",
        "The `Analysis` class is the interface between the data and model, whereby a `log_likelihood_function` is defined\n",
        "and called by the non-linear search to fit the model.\n",
        "\n",
        "You may have performed a similar task yourself, for example by taking a fitting library (e.g. an MCMC method like\n",
        "Emcee or nested sampler like Dynesty) and writing a likelihood function that calls it to fit a model to a dataset.\n",
        "If you haven't done this, this script will explain how!\n",
        "\n",
        "**PyAutoLens** uses a library called **PyAutoFit** to set up this interfacebetween the data, model,\n",
        "`log_likelihood_function` and non-linear search. **PyAutoFit** is a general purpose library for model fitting,\n",
        "and we will see that it has a lot of powerful tools that we can use to customize our `Analysis` class.\n",
        "\n",
        "You can checkout the **PyAutoFit** readthedocs here:\n",
        "\n",
        " https://pyautofit.readthedocs.io/en/latest/\n",
        "\n",
        "The following analysis cookbook provides a concise reference guide to `Analysis` objects, and once you have completed\n",
        "this example will be a useful resource for writing your own `Analysis` class:\n",
        "\n",
        " https://pyautofit.readthedocs.io/en/latest/cookbooks/analysis.html\n",
        "\n",
        "__Source Code__\n",
        "\n",
        "This example contains URLs to the locations of the source code of the classes used when creating light and mass\n",
        "profiles.\n",
        "\n",
        "The example itself is standalone and should by the end allow you to implement a custom profile without diving into\n",
        "the **PyAutoLens** source code.\n",
        "\n",
        "We still recommend you take a look to see how things are structured!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import numpy as np\n",
        "from os import path\n",
        "import autofit as af\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Example Analysis Class__\n",
        "\n",
        "The `Analysis` classes available in **PyAutoLens** are actually located in both **PyAutoLens** and its parent \n",
        "package, **PyAutoGalaxy**: \n",
        "\n",
        " https://github.com/Jammy2211/PyAutoGalaxy\n",
        " https://github.com/Jammy2211/PyAutoLens\n",
        "\n",
        "All classes used for lens modeling are found in the following packages:\n",
        "\n",
        " https://github.com/Jammy2211/PyAutoGalaxy/tree/main/autogalaxy/imaging/model\n",
        " https://github.com/Jammy2211/PyAutoLens/tree/main/autolens/imaging/model\n",
        "\n",
        "The `AnalysisImaging` classes are found in the following modules:\n",
        "\n",
        " https://github.com/Jammy2211/PyAutoGalaxy/blob/main/autogalaxy/imaging/model/analysis.py\n",
        " https://github.com/Jammy2211/PyAutoLens/blob/main/autolens/imaging/model/analysis.py\n",
        "\n",
        "__Lens Model__\n",
        "\n",
        "To illustrate how to write a custom `Analysis` class, we require an example lens model that we will use to fit\n",
        "the dataset.\n",
        "\n",
        "We compose a simple lens model with an `IsothermalSph` mass model for the lens and an `Sersic` for the source."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Lens:\n",
        "mass = af.Model(al.mp.IsothermalSph)\n",
        "\n",
        "lens = af.Model(al.Galaxy, redshift=0.5, mass=mass)\n",
        "\n",
        "# Source:\n",
        "\n",
        "source = af.Model(al.Galaxy, redshift=1.0, bulge=al.lp.ExponentialCoreSph)\n",
        "\n",
        "# Overall Lens Model:\n",
        "\n",
        "model = af.Collection(galaxies=af.Collection(lens=lens, source=source))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `info` attribute shows the model in a readable format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Instances__\n",
        "\n",
        "Instances of the model above can be created, where an input `vector` of parameters is mapped to create an instance of \n",
        "the Python class of the model.\n",
        "\n",
        "This is used internally by the `Analysis` class we are about to write, and will be used in \n",
        "our `log_likelihood_function`. Therefore, we are quickly highlighting it here.\n",
        "\n",
        "We first need to know the order of parameters in the model, so we know how to define the input `vector`. This\n",
        "information is contained in the models `paths` attribute:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.paths)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We input values for the parameters of our model following the order of paths above.\n",
        "\n",
        "This creates an `instance` of the lens model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = model.instance_from_vector(vector=[0.0, 0.0, 1.6, 0.1, 0.1, 0.01, 2.0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This `instance` contains each of the model components we defined above. \n",
        "\n",
        "The argument names input into each `Collection` define the attribute names of the `instance`. \n",
        "\n",
        "For example, when composing the `model` above, we used a `Collection` called `galaxies` which had a `lens` and `source` \n",
        "attribute. These `lens` and `source` attributes each contained components called `mass` and `bulge` respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Lens Centre = {instance.galaxies.lens.mass.centre}\")\n",
        "print(f\"Lens Einstein Radius = {instance.galaxies.lens.mass.einstein_radius}\")\n",
        "print(f\"Source Centre = {instance.galaxies.source.bulge.centre}\")\n",
        "print(f\"Source Intensity = {instance.galaxies.source.bulge.intensity}\")\n",
        "print(f\"Source Effective Radius = {instance.galaxies.source.bulge.effective_radius}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Simple Analysis Example__\n",
        "\n",
        "For simplicity, a shortened version of an `AnalysisImaging` class is shown below where certain functions have been \n",
        "edited to make them easy to read and understand. \n",
        "\n",
        "This has docstrings updated to focus on the key aspects of implementing a new `Analysis` class and simplifies the \n",
        "inheritance structure of the profile."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class AnalysisImaging(af.Analysis):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset: al.Imaging,\n",
        "        cosmology: al.cosmo.LensingCosmology = al.cosmo.Planck15(),\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Fits a lens model to an imaging dataset via a non-linear search.\n",
        "\n",
        "        The `Analysis` class defines the `log_likelihood_function` which fits the model to the dataset and returns the\n",
        "        log likelihood value defining how well the model fitted the data.\n",
        "\n",
        "        It handles many other tasks, such as visualization, outputting results to hard-disk and storing results in\n",
        "        a format that can be loaded after the model-fit is complete.\n",
        "\n",
        "        This class is used for model-fits which fit strong lenses composed via a `Tracer` to an imaging dataset.\n",
        "        Parameters\n",
        "        ----------\n",
        "        dataset\n",
        "            The `Imaging` dataset that the model is fitted to.\n",
        "        cosmology\n",
        "            The Cosmology assumed for this analysis.\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.cosmology = cosmology\n",
        "\n",
        "    def log_likelihood_function(self, instance: af.ModelInstance) -> float:\n",
        "        \"\"\"\n",
        "        Given an instance of the model, where the model parameters are set via a non-linear search, fit the model\n",
        "        instance to the imaging dataset.\n",
        "\n",
        "        This function returns a log likelihood which is used by the non-linear search to guide the model-fit.\n",
        "\n",
        "        For this analysis class, this function performs the following steps:\n",
        "\n",
        "        1) Extracts all galaxies from the model instance and set up a `Tracer`, which includes ordering the galaxies\n",
        "           by redshift to set up each `Plane`.\n",
        "\n",
        "        2) Use the `Tracer` and other attributes to create a `FitImaging` object, which performs steps such as creating\n",
        "           model images of every galaxy in the tracer, blurring them with the imaging dataset's PSF and computing\n",
        "           residuals, a chi-squared statistic and the log likelihood.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        instance\n",
        "            An instance of the model that is being fitted to the data by this analysis (whose parameters have been set\n",
        "            via a non-linear search).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        float\n",
        "            The log likelihood indicating how well this model instance fitted the imaging data.\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        The `instance` that comes into this method is an instance of the lens model above, which we illustrated\n",
        "        via print statements how it is structured.\n",
        "\n",
        "        The parameter values are chosen by the non-linear search, based on where it thinks the high likelihood regions \n",
        "        of parameter space are.\n",
        "\n",
        "        The lines of Python code are commented out below to prevent excessive print statements when we run the\n",
        "        non-linear search, but feel free to uncomment them and run the search to see the parameters of every instance\n",
        "        that it fits.\n",
        "        \"\"\"\n",
        "\n",
        "        print(f\"Lens Centre = {instance.galaxies.lens.mass.centre}\")\n",
        "        print(f\"Lens Einstein Radius = {instance.galaxies.lens.mass.einstein_radius}\")\n",
        "        print(f\"Source Centre = {instance.galaxies.source.bulge.centre}\")\n",
        "        print(f\"Source Intensity = {instance.galaxies.source.bulge.intensity}\")\n",
        "        print(f\"Source Effective Radius = {instance.galaxies.source.bulge.effective_radius}\")\n",
        "\n",
        "        \"\"\"\n",
        "        You should be familiar with the `Tracer` object, given a list of galaxies it provides all the functionality\n",
        "        necessary to perform ray-tracing and strong lensing calculations.\n",
        "        \n",
        "        One aspect of its design you may not have considered is that the input galaxies can be any size, and it does\n",
        "        not matter what the galaxies, light or mass profiles are called (e.g. it does not depend on the lens mass\n",
        "        have the path `galaxies.lens.mass`).\n",
        "        \n",
        "        This means that a user can compose a lens model using any combination of light and mass profiles, and the\n",
        "        `log_likelihood_function` below will still work. You should ensure your `Analysis` class is written generically\n",
        "        like this.        \n",
        "        \"\"\"\n",
        "\n",
        "        tracer = al.Tracer(\n",
        "            galaxies=instance.galaxies,\n",
        "            cosmology=self.cosmology,\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        You should also be familiar with the `FitImaging` object, which given a tracer and imaging dataset fits the\n",
        "        tracer's model image to the data, using a chi-squared map to compute the residuals and likelihood.\n",
        "        \"\"\"\n",
        "\n",
        "        fit = al.FitImaging(\n",
        "            dataset=self.dataset,\n",
        "            tracer=tracer,\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        To get your custom analysis class, running quickly, you may not want to define your own `Fit` class but\n",
        "        instead just write out manually how the `log_likelihood` is computed. \n",
        "        \n",
        "        The commented out code below shows the simplest way to do this, and it is probably suitable for most \n",
        "        use-cases.\n",
        "        \n",
        "        At step-by-step description of what the code is doing is as follows:\n",
        "        \n",
        "         1) Creates an image of the lens and source galaxies from the tracer using its `image_2d_from()` method.\n",
        "\n",
        "         2) Blurs the tracer`s image with the data's PSF, ensuring the telescope optics are included in the fit. This \n",
        "         creates what is called the `model_image`.\n",
        "        \n",
        "         3) Computes the difference between this model-image and the observed image, creating the fit`s `residual_map`.\n",
        "        \n",
        "         4) Divides the residual-map by the noise-map, creating the fit`s `normalized_residual_map`.\n",
        "        \n",
        "         5) Squares every value in the normalized residual-map, creating the fit's `chi_squared_map`.\n",
        "        \n",
        "         6) Sums up these chi-squared values and converts them to a `log_likelihood`, which quantifies how good \n",
        "         this tracer`s fit to the data was (higher log_likelihood = better fit).\n",
        "         \n",
        "        Quantities like the `chi_squared_map` and `log_likelihood` are standard quantities used by all model-fitting\n",
        "        approaches.\n",
        "        \"\"\"\n",
        "        # model_data = tracer.blurred_image_2d_from(\n",
        "        #    grid=self.dataset.grid,\n",
        "        #    convolver=self.dataset.convolver,\n",
        "        #    blurring_grid=self.dataset.grids.blurring,\n",
        "        # )\n",
        "\n",
        "        # residual_map = self.dataset.data - model_data\n",
        "        # chi_squared_map = (residual_map / self.dataset.noise_map) ** 2.0\n",
        "        # chi_squared = sum(chi_squared_map)\n",
        "        # noise_normalization = np.sum(np.log(2 * np.pi * self.dataset.noise_map**2.0))\n",
        "        # log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "        \"\"\"\n",
        "        The `log_likelihood` is returned to the non-linear search, informing it how good a fit this lens model\n",
        "        was and whether to continue sampling this region of parameter space.\n",
        "        \"\"\"\n",
        "\n",
        "        return fit.log_likelihood\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis Class Considerations__\n",
        "\n",
        "Lets quickly think about the design of an `Analysis` class and how this can help us to set up any model-fit we can\n",
        "imagine:\n",
        "\n",
        " - The `__init__` method can be extended to include any data structures needed to perform the analysis. For example, \n",
        "   the `AnalysisImaging` object in the autolens source code has a `settings_inversion` object that customize \n",
        "   how fits using a `Pixelization` are performed.\n",
        "   \n",
        " - The `log_likelihood_function` can be written in any way that is desired to fit the data. The example above uses\n",
        "   the `FitImaging` object, but this is not necessary. Furthermore, you could customize this function to assume a \n",
        "   likelihood function defined by Poisson statistics (the example above assumes Gaussian statistics) or to include\n",
        "   additional constraints on the model that are specific to your dataset.\n",
        "\n",
        "__Model Fit__\n",
        "\n",
        "The standard API for choosing a non-linear search and performing a model-fit can now be used with this `Analysis`\n",
        "class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"simple__no_lens_light\"\n",
        "dataset_path = path.join(\"dataset\", \"imaging\", dataset_name)\n",
        "\n",
        "dataset = al.Imaging.from_fits(\n",
        "    data_path=path.join(dataset_path, \"data.fits\"),\n",
        "    psf_path=path.join(dataset_path, \"psf.fits\"),\n",
        "    noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "    pixel_scales=0.1,\n",
        ")\n",
        "\n",
        "mask = al.Mask2D.circular(\n",
        "    shape_native=dataset.shape_native, pixel_scales=dataset.pixel_scales, radius=3.0\n",
        ")\n",
        "\n",
        "dataset = dataset.apply_mask(mask=mask)\n",
        "\n",
        "search = af.Nautilus(\n",
        "    path_prefix=path.join(\"custom_analysis\"),\n",
        "    name=\"strong_lensing_example\",\n",
        "    unique_tag=dataset_name,\n",
        "    n_live=150,\n",
        "    number_of_cores=1,\n",
        "    iterations_per_update=10000,\n",
        ")\n",
        "\n",
        "# We are using the Analysis class above here!\n",
        "\n",
        "analysis = AnalysisImaging(dataset=dataset)\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Weak Lensing Example__\n",
        "\n",
        "Now lets consider how to write our own custom `Analysis` class, for the example of performing a weak lensing analysis.\n",
        "\n",
        "If you are unfamiliar with weak lensing, a brief summary is as follows:\n",
        "\n",
        " - Weak lensing is the small lensing signal induced into galaxies by lensing due to large-scale structure in the\n",
        "   universe. \n",
        "   \n",
        "- This signal is much smaller than the strong lensing regime and is often summarized as the small change in the \n",
        "  ellipticity of a source galaxy's light. \n",
        "  \n",
        "- This change in ellipticity can be measured and is called the `shear`, with the dataset our `Analysis` class will\n",
        "  fit called a shear catalogue.\n",
        "\n",
        " - In strong lensing, we typically use the deflection angles of a mass profile to fit the data. For weak lensing\n",
        " analysis we compute its shear (via the function `shear_yx_2d_from`) and compare this to the observed shear in the\n",
        " shear catalogue data.\n",
        "\n",
        "__Lens Model__\n",
        "\n",
        "We first compose our lens model for weak lensing analysis.\n",
        "\n",
        "This can reuse the **PyAutoLens** API for model composition, but does not require a source galaxy to be included as\n",
        "we are simply comparing the mass model shears.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Lens:\n",
        "mass = af.Model(al.mp.IsothermalSph)\n",
        "\n",
        "lens = af.Model(al.Galaxy, redshift=0.5, mass=mass)\n",
        "\n",
        "# Overall Lens Model:\n",
        "\n",
        "model = af.Collection(galaxies=af.Collection(lens=lens))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is our example `Analysis` class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class AnalysisShearCatalogue(af.Analysis):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data,  # You may wish to group these into a `ShearCatalogue` dataset.\n",
        "        noise_map,\n",
        "        grid,\n",
        "        cosmology: al.cosmo.LensingCosmology = al.cosmo.Planck15(),\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Fits a lens model to a shear catalogue dataset via a non-linear search.\n",
        "\n",
        "        The `Analysis` class defines the `log_likelihood_function` which fits the model to the dataset and returns the\n",
        "        log likelihood value defining how well the model fitted the data.\n",
        "\n",
        "        It handles many other tasks, such as visualization, outputting results to hard-disk and storing results in\n",
        "        a format that can be loaded after the model-fit is complete.\n",
        "\n",
        "        This class is used for model-fits which fit strong lenses composed via a `Tracer` to a weak lensing\n",
        "        shear catalogue dataset.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data\n",
        "            The shear catalogue data.\n",
        "        noise_map\n",
        "            An array describing the RMS standard deviation error in each shear measurement point (e.g. the noise-map).\n",
        "        grid\n",
        "            The (y,x) coordinates defining where the shears are measured and evaluated (e.g. the locations of the\n",
        "            galaxies in the shear catalogue).\n",
        "        cosmology\n",
        "            The Cosmology assumed for this analysis.\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.noise_map = noise_map\n",
        "        self.grid = grid\n",
        "        self.cosmology = cosmology\n",
        "\n",
        "    def log_likelihood_function(self, instance: af.ModelInstance) -> float:\n",
        "        \"\"\"\n",
        "        Given an instance of the model, where the model parameters are set via a non-linear search, fit the model\n",
        "        instance to the imaging dataset.\n",
        "\n",
        "        This function returns a log likelihood which is used by the non-linear search to guide the model-fit.\n",
        "\n",
        "        For this analysis class, this function performs the following steps:\n",
        "\n",
        "        1) Extracts all galaxies from the model instance and set up a `Tracer`, which includes ordering the galaxies\n",
        "           by redshift to set up each `Plane`.\n",
        "\n",
        "        2) Use the `Tracer` to compute the model shear field of the entire strong lensing system.\n",
        "\n",
        "        3) Compute the shear residuals, a chi-squared statistic and the log likelihood.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        instance\n",
        "            An instance of the model that is being fitted to the data by this analysis (whose parameters have been set\n",
        "            via a non-linear search).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        float\n",
        "            The log likelihood indicating how well this model instance fitted the imaging data.\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        For this example, its very easy to compute the model shear field as the `Tracer` object already has this\n",
        "        functionality built in.   \n",
        "        \"\"\"\n",
        "        tracer = al.Tracer(\n",
        "            galaxies=instance.galaxies,\n",
        "            cosmology=self.cosmology,\n",
        "        )\n",
        "\n",
        "        model_data = tracer.shear_yx_2d_via_hessian_from(grid=self.grid)\n",
        "\n",
        "        \"\"\"\n",
        "        We then use this model data and the data itself to compute the residuals, chi-squared and log likelihood.\n",
        "        \"\"\"\n",
        "        residual_map = self.data - model_data\n",
        "        chi_squared_map = (residual_map / self.noise_map) ** 2.0\n",
        "        chi_squared = sum(chi_squared_map)\n",
        "        noise_normalization = np.sum(np.log(2 * np.pi * self.noise_map**2.0))\n",
        "        log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "\n",
        "        return log_likelihood\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fit__\n",
        "\n",
        "The standard API for choosing a non-linear search and performing a model-fit can now be used with this `Analysis`\n",
        "class.\n",
        "\n",
        "NOTE: Felix can you send me an example shear catalogue so I can get this to run :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"example_shear_catalogue\"\n",
        "dataset_path = path.join(\"dataset\", \"weak_lensing\", dataset_name)\n",
        "\n",
        "# data = load_shear()\n",
        "# noise_map = load_noise_map()\n",
        "# grid = load_grid()\n",
        "\n",
        "search = af.Nautilus(\n",
        "    path_prefix=path.join(\"custom_analysis\"),\n",
        "    name=\"weak_lensing_example\",\n",
        "    unique_tag=dataset_name,\n",
        "    n_live=150,\n",
        "    number_of_cores=1,\n",
        "    iterations_per_update=10000,\n",
        ")\n",
        "\n",
        "# We are using the Analysis class above here!\n",
        "\n",
        "# analysis = AnalysisShearCatalogue(\n",
        "#     data=data,\n",
        "#     noise_map=noise_map,\n",
        "#     grid=grid\n",
        "# )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you are used to using **PyAutoLens**, you'll know that when we run the fit below lots of information about the\n",
        "model fit is output to hard-disk (e.g. the best-fit model, error estimates, the model info).\n",
        "\n",
        "By writing our own `Analysis` class, this is output for free without us having to do anything - pretty cool, huh?\n",
        "\n",
        "Below, we'll show you how to customize the `Analysis` class even more, to output additional information to hard-disk\n",
        "such as visualization and results which you can load elsewhere via the **PyAutoLens** database functionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "If you're familiar with **PyAutoLens**'s API, you'll know that the `Result` object returned by the non-linear search\n",
        "contains lots of information about the fit. \n",
        "\n",
        "This includes parameter estimates and errors, details of the non-linear search, etc. \n",
        "\n",
        "By writing our own `Analysis` class we get all of this information for free, without having to change our code!\n",
        "Therefore you should be good to inspect and interpret the results as normal.\n",
        "\n",
        "The results `info` attribute shows the result in a readable format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result contains the maximum log likelihood instance, which we can use to inspect the result or make plots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = result.instance\n",
        "\n",
        "print(f\"Lens Centre = {instance.galaxies.lens.mass.centre}\")\n",
        "print(f\"Lens Einstein Radius = {instance.galaxies.lens.mass.einstein_radius}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It also contains information on the posterior as estimated by the non-linear search (in this example `Nautilus`). \n",
        "\n",
        "Below, we make a corner plot of the \"Probability Density Function\" of every parameter in the model-fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plotter = aplt.NestPlotter(samples=result.samples)\n",
        "plotter.corner_anesthetic()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checkout `autolens_workspace/*/imaging/results` for a full description of analysing results in **PyAutoLens**.\n",
        "\n",
        "__To Do List__\n",
        "\n",
        "The following `Analysis` cookbook from the **PyAutoFit** readthedocs should help you get started customizing your\n",
        "own `Analysis` class: \n",
        "\n",
        "https://pyautofit.readthedocs.io/en/latest/cookbooks/analysis.html\n",
        "\n",
        "I will extend this guide to include the following in the next few days:\n",
        "\n",
        " - How to output your own custom visualization.\n",
        " - How to extend the `Result` class to include additional information about the model-fit specific to weak lensing \n",
        "   (e.g. the maximum likelihood shear map).\n",
        " - Add methods which output model-specific results to hard-disk in the files folder (e.g. as .json files) to aid in \n",
        " the interpretation of results.\n",
        " - How to output results to hard-disk in a format that can be loaded into the **PyAutoLens** database.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}