{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modeling: Expectation Propagation\n",
        "=================================\n",
        "\n",
        "In the `hierarchical` example, we fitted graphical models to a dataset comprising 3 images of strong lenses, which had a\n",
        "hierarchical parameter, the power-law `slope`. This provides the basis of composing and fitting complex graphical\n",
        "models to large datasets.\n",
        "\n",
        "The challenge is that we will soon hit a ceiling scaling these graphical models up to extremely large datasets.\n",
        "One would soon find that the parameter space is too complex to sample, and computational limits would ultimately\n",
        "cap how many datasets one could feasible fit.\n",
        "\n",
        "This example introduces expectation propagation (EP), the solution to this problem, which inspects a factor graph\n",
        "and partitions the model-fit into many simpler fits of sub-components of the graph to individual datasets. This\n",
        "overcomes the challenge of model complexity, and mitigates computational restrictions that may occur if one tries to\n",
        "fit every dataset simultaneously.\n",
        "\n",
        "__Sample Simulation__\n",
        "\n",
        "The dataset fitted in this example script is simulated imaging data of a sample of 3 galaxies.\n",
        "\n",
        "This data is not automatically provided with the autogalaxy workspace, and must be first simulated by running the\n",
        "script `autolens_workspace/scripts/advanced/graphical/simulator/samples/simple__no_lens_light.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from autoconf import jax_wrapper  # Sets JAX environment before other imports\n",
        "\n",
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import autolens as al\n",
        "import autofit as af\n",
        "\n",
        "import numpy as np\n",
        "from pathlib import Path"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "The following steps repeat all the initial steps performed in tutorial 2 and 3:\n",
        "\n",
        "This data is not automatically provided with the autogalaxy workspace, and must be first simulated by running the \n",
        "script `autolens_workspace/scripts/advanced/graphical/simulator/samples/simple__no_lens_light.py`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_label = \"samples\"\n",
        "dataset_type = \"imaging\"\n",
        "dataset_sample_name = \"mass_power_law\"\n",
        "\n",
        "dataset_path = Path(\"dataset\", dataset_type, dataset_label, dataset_sample_name)\n",
        "\n",
        "total_datasets = 3\n",
        "\n",
        "dataset_list = []\n",
        "\n",
        "for dataset_index in range(total_datasets):\n",
        "    dataset_sample_path = Path(dataset_path, f\"dataset_{dataset_index}\")\n",
        "\n",
        "    dataset_list.append(\n",
        "        al.Imaging.from_fits(\n",
        "            data_path=Path(dataset_sample_path, \"data.fits\"),\n",
        "            psf_path=Path(dataset_sample_path, \"psf.fits\"),\n",
        "            noise_map_path=Path(dataset_sample_path, \"noise_map.fits\"),\n",
        "            pixel_scales=0.1,\n",
        "        )\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Mask__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "masked_dataset_list = []\n",
        "\n",
        "for dataset in dataset_list:\n",
        "    mask_radius = 3.0\n",
        "\n",
        "    mask = al.Mask2D.circular(\n",
        "        shape_native=dataset.shape_native,\n",
        "        pixel_scales=dataset.pixel_scales,\n",
        "        radius=mask_radius,\n",
        "    )\n",
        "\n",
        "    dataset = dataset.apply_mask(mask=mask)\n",
        "\n",
        "    over_sample_size = al.util.over_sample.over_sample_size_via_radial_bins_from(\n",
        "        grid=dataset.grid,\n",
        "        sub_size_list=[4, 2, 1],\n",
        "        radial_list=[0.3, 0.6],\n",
        "        centre_list=[(0.0, 0.0)],\n",
        "    )\n",
        "\n",
        "    dataset = dataset.apply_over_sampling(over_sample_size_lp=over_sample_size)\n",
        "\n",
        "    masked_dataset_list.append(dataset)\n",
        "\n",
        "dataset_list = masked_dataset_list"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Individual Factors__\n",
        "\n",
        "We first set up a model for each lens, with an `PowerLawSph` mass and `ExponentialSph` bulge, which we will use to \n",
        "fit the hierarchical model.\n",
        "\n",
        "Note that the `PowerLawSph` mass model has a `slope` parameter, which we will assume is drawn from a shared parent\n",
        "Gaussian distribution, albeit building this into the model is done later in this script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_list = []\n",
        "\n",
        "for dataset_index in range(total_datasets):\n",
        "\n",
        "    lens = af.Model(al.Galaxy, redshift=0.5, mass=al.mp.PowerLawSph)\n",
        "    lens.mass.centre = (0.0, 0.0)\n",
        "\n",
        "    source = af.Model(al.Galaxy, redshift=1.0, bulge=al.lp_linear.ExponentialCoreSph)\n",
        "\n",
        "    model = af.Collection(galaxies=af.Collection(lens=lens, source=source))\n",
        "\n",
        "    model_list.append(model)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "For each dataset we now create a corresponding `AnalysisImaging` class, as we are used to doing for `Imaging` data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_list = []\n",
        "\n",
        "for dataset in dataset_list:\n",
        "    analysis = al.AnalysisImaging(dataset=dataset)\n",
        "\n",
        "    analysis_list.append(analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "We now compose the hierarchical model that we fit, using the individual model components created above.\n",
        "\n",
        "This uses the same API as the `hierarchical` example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hierarchical_factor = af.HierarchicalFactor(\n",
        "    af.GaussianPrior,\n",
        "    mean=af.TruncatedGaussianPrior(\n",
        "        mean=2.0, sigma=1.0, lower_limit=0.0, upper_limit=100.0\n",
        "    ),\n",
        "    sigma=af.TruncatedGaussianPrior(\n",
        "        mean=0.5, sigma=0.5, lower_limit=0.0, upper_limit=100.0\n",
        "    ),\n",
        "    use_jax=False,\n",
        ")\n",
        "\n",
        "for model in model_list:\n",
        "    hierarchical_factor.add_drawn_variable(model.galaxies.lens.mass.slope)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Paths__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "path_prefix = Path(\"imaging\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis Factors__\n",
        "\n",
        "Now we have our `Analysis` classes and graphical model, we can compose our `AnalysisFactor`'s.\n",
        "\n",
        "However, unlike the previous tutorials, each `AnalysisFactor` is now assigned its own `search`. This is because the EP \n",
        "framework performs a model-fit to each node on the factor graph (e.g. each `AnalysisFactor`). Therefore, each node \n",
        "requires its own non-linear search, and in this tutorial we use `dynesty`. For complex graphs consisting of many \n",
        "nodes, one could easily use different searches for different nodes on the factor graph.\n",
        "\n",
        "Each `AnalysisFactor` is also given a `name`, corresponding to the name of the dataset it fits. These names are used\n",
        "to name the folders containing the results in the output directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "paths = af.DirectoryPaths(\n",
        "    path_prefix=path_prefix,\n",
        "    name=\"expectation_propagation\",\n",
        ")\n",
        "\n",
        "search = af.Nautilus(paths=paths, n_live=100)\n",
        "\n",
        "analysis_factor_list = []\n",
        "\n",
        "dataset_index = 0\n",
        "\n",
        "for model, analysis in zip(model_list, analysis_list):\n",
        "\n",
        "    dataset_name = f\"dataset_{dataset_index}\"\n",
        "    dataset_index += 1\n",
        "\n",
        "    analysis_factor = af.AnalysisFactor(\n",
        "        prior_model=model, analysis=analysis, optimiser=search, name=dataset_name\n",
        "    )\n",
        "\n",
        "    analysis_factor_list.append(analysis_factor)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Factor Graph__\n",
        "\n",
        "We combine our `AnalysisFactors` into one, to compose the factor graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "factor_graph = af.FactorGraphModel(\n",
        "    *analysis_factor_list, hierarchical_factor, use_jax=False\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The factor graph model `info` attribute shows the model which we fit via expectaton propagation (note that we do\n",
        "not use `global_prior_model` below when performing the fit)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(factor_graph.global_prior_model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Expectation Propagation__\n",
        "\n",
        "In the previous tutorials, we used the `global_prior_model` of the `factor_graph` to fit the global model. In this \n",
        "tutorial, we instead fit the `factor_graph` using the EP framework, which fits the graphical model composed in this \n",
        "tutorial as follows:\n",
        "\n",
        "1) Go to the first node on the factor graph (e.g. `analysis_factor_list[0]`) and fit its model to its dataset. This is \n",
        "simply a fit of the `Gaussian` model to the first 1D Gaussian dataset, the model-fit we are used to performing by now.\n",
        "\n",
        "2) Once the model-fit is complete, inspect the model for parameters that are shared with other nodes on the factor\n",
        "graph. In this example, the `centre` of the `Gaussian` fitted to the first dataset is global, and therefore connects\n",
        "to the other nodes on the factor graph (the `AnalysisFactor`'s) of the second and first `Gaussian` datasets.\n",
        "\n",
        "3) The EP framework now creates a 'message' that is to be passed to the connecting nodes on the factor graph. This\n",
        "message informs them of the results of the model-fit, so they can update their priors on the `Gaussian`'s centre \n",
        "accordingly and, more importantly, update their posterior inference and therefore estimate of the global centre.\n",
        "\n",
        "For example, the model fitted to the first Gaussian dataset includes the global centre. Therefore, after the model is \n",
        "fitted, the EP framework creates a 'message' informing the factor graph about its inference on that Gaussians's centre,\n",
        "thereby updating our overall inference on this shared parameter. This is termed 'message passing'.\n",
        "\n",
        "__Cyclic Fitting__\n",
        "\n",
        "After every `AnalysisFactor` has been fitted (e.g. after each fit to each of the 5 datasets in this example), we have a \n",
        "new estimate of the shared parameter `centre`. This updates our priors on the shared parameter `centre`, which needs \n",
        "to be reflected in each model-fit we perform on each `AnalysisFactor`. \n",
        "\n",
        "The EP framework therefore performs a second iteration of model-fits. It again cycles through each `AnalysisFactor` \n",
        "and refits the model, using updated priors on shared parameters like the `centre`. At the end of each fit, we again \n",
        "create messages that update our knowledge about other parameters on the graph.\n",
        "\n",
        "This process is repeated multiple times, until a convergence criteria is met whereby continued cycles are expected to\n",
        "produce the same estimate of the shared parameter `centre`. \n",
        "\n",
        "When we fit the factor graph a `name` is passed, which determines the folder all results of the factor graph are\n",
        "stored in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "laplace = af.LaplaceOptimiser()\n",
        "\n",
        "factor_graph_result = factor_graph.optimise(\n",
        "    optimiser=laplace, paths=paths, ep_history=af.EPHistory(kl_tol=0.05), max_steps=5\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "An `info` attribute for the result of a factor graph fitted via EP does not exist yet, its on the to do list!\n",
        "\n",
        "The result can be seen in the `graph.result` file output to hard-disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "##print(factor_graph_result.info)##"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Output__\n",
        "\n",
        "The results of the factor graph, using the EP framework and message passing, are contained in the folder \n",
        "`output/howtofit/chapter_graphical_models/tutorial_5_expectation_propagation`. \n",
        "\n",
        "The following folders and files are worth of note:\n",
        "\n",
        " - `graph.info`: this provides an overall summary of the graphical model that is fitted, including every parameter, \n",
        " how parameters are shared across `AnalysisFactor`'s and the priors associated to each individual parameter.\n",
        "\n",
        " - The 3 folders titled `gaussian_x1_#__low_snr` correspond to the three `AnalysisFactor`'s and therefore signify \n",
        " repeated non-linear searches that are performed to fit each dataset.\n",
        "\n",
        " - Inside each of these folders are `optimization_#` folders, corresponding to each model-fit performed over cycles of\n",
        " the EP fit. A careful inspection of the `model.info` files inside each folder reveals how the priors are updated\n",
        " over each cycle, whereas the `model.results` file should indicate the improved estimate of model parameters over each\n",
        " cycle.\n",
        "\n",
        "__Results__\n",
        "\n",
        "The `MeanField` object represent the posterior of the entire factor graph and is used to infer estimates of the \n",
        "values and error of each parameter in the graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mean_field = factor_graph_result.updated_ep_mean_field.mean_field\n",
        "print(mean_field)\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The object has a `variables` property which lists every variable in the factor graph, which is essentially all of the \n",
        "free parameters on the graph.\n",
        "\n",
        "This includes the parameters specific to each data (E.g. each node on the graph) as well as the shared centre."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(mean_field.variables)\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The variables above use the priors on each parameter as their key. \n",
        "\n",
        "Therefore to estimate mean-field quantities of the shared centre, we can simply use the `centre_shared_prior` defined\n",
        "above.\n",
        "\n",
        "Each parameter estimate is given by the mean of its value in the `MeanField`. Below, we use the `centred_shared_prior` \n",
        "as a key to the `MeanField.mean` dictionary to print the estimated value of the shared centre."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# prior = hierarchical_factor.drawn_variables[0]\n",
        "#\n",
        "print(f\"Centre Mean Parameter Estimate = {mean_field.mean[prior]}\")\n",
        "# print()\n",
        "#\n",
        "# \"\"\"\n",
        "# If we want the parameter estimate of another parameter in the model, we can use the `model_list` that we composed\n",
        "# above to pass a parameter prior to the mean field dictionary.\n",
        "# \"\"\"\n",
        "# print(\n",
        "#     f\"Einstein Radius Dataset 0 Mean = {mean_field.mean[model_list[0].galaxies.lens.mass.einstein_radius]}\"\n",
        "# )\n",
        "#\n",
        "# \"\"\"\n",
        "# The mean-field mean dictionary contains the estimate value of every parameter.\n",
        "# \"\"\"\n",
        "print(f\"All Parameter Estimates = {mean_field.mean}\")\n",
        "# print()\n",
        "#\n",
        "# \"\"\"\n",
        "# The mean-field also contains a `variance` dictionary, which has the same keys as the `mean` dictionary above.\n",
        "#\n",
        "# This is the easier way to estimate the error on every parameter, for example that of the shared centre.\n",
        "# \"\"\"\n",
        "print(f\"Centre Variance = {mean_field.variance[prior]}\")\n",
        "# print()\n",
        "#\n",
        "# \"\"\"\n",
        "# The standard deviation (or error at one sigma confidence interval) is given by the square root of the variance.\n",
        "# \"\"\"\n",
        "print(f\"Centre 1 Sigma = {np.sqrt(mean_field.variance[prior])}\")\n",
        "# print()\n",
        "#\n",
        "# \"\"\"\n",
        "# The mean field object also contains a dictionary of the s.d./variance**0.5.\n",
        "# \"\"\"\n",
        "print(f\"Centre SD/sqrt(variance) = {mean_field.scale[prior]}\")\n",
        "# print()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}