{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results: CSV\n",
        "============\n",
        "\n",
        "This example is a results workflow example, which means it provides tool to set up an effective workflow inspecting\n",
        "and interpreting the large libraries of modeling results.\n",
        "\n",
        "In this tutorial, we use the aggregator to load the results of model-fits and output them in a single .csv file.\n",
        "\n",
        "This enables the results of many model-fits to be concisely summarised and inspected in a single table, which\n",
        "can also be easily passed on to other collaborators.\n",
        "\n",
        "__CSV, Png and Fits__\n",
        "\n",
        "Workflow functionality closely mirrors the `png_make.py` and `fits_make.py`  examples, which load results of\n",
        "model-fits and output th em as .png files and .fits files to quickly summarise results.\n",
        "\n",
        "The same initial fit creating results in a folder called `results_folder_csv_png_fits` is therefore used.\n",
        "\n",
        "__Interferometer__\n",
        "\n",
        "This script can easily be adapted to analyse the results of charge injection imaging model-fits.\n",
        "\n",
        "The only entries that needs changing are:\n",
        "\n",
        " - `ImagingAgg` -> `InterferometerAgg`.\n",
        " - `FitImagingAgg` -> `FitInterferometerAgg`.\n",
        " - `ImagingPlotter` -> `InterferometerPlotter`.\n",
        " - `FitImagingPlotter` -> `FitInterferometerPlotter`.\n",
        "\n",
        "Quantities specific to an interfometer, for example its uv-wavelengths real space mask, are accessed using the same API\n",
        "(e.g. `values(\"dataset.uv_wavelengths\")` and `.values{\"dataset.real_space_mask\")).\n",
        "\n",
        "__Database File__\n",
        "\n",
        "The aggregator can also load results from a `.sqlite` database file.\n",
        "\n",
        "This is beneficial when loading results for large numbers of model-fits (e.g. more than hundreds)\n",
        "because it is optimized for fast querying of results.\n",
        "\n",
        "See the package `results/database` for a full description of how to set up the database and the benefits it provides,\n",
        "especially if loading results from hard-disk is slow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from autoconf import jax_wrapper  # Sets JAX environment before other imports\n",
        "\n",
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import autofit as af\n",
        "import autolens as al"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fit__\n",
        "\n",
        "The code below performs a model-fit using nautilus. \n",
        "\n",
        "You should be familiar with modeling already, if not read the `modeling/start_here.py` script before reading this one!\n",
        "\n",
        "__Unique Tag__\n",
        "\n",
        "One thing to note is that the `unique_tag` of the search is given the name of the dataset with an index for the\n",
        "fit of 0 and 1. \n",
        "\n",
        "This `unique_tag` names the fit in a descriptive and human-readable way, which we will exploit to make our .csv files\n",
        "more descriptive and easier to interpret."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for i in range(2):\n",
        "    dataset_name = \"simple__no_lens_light\"\n",
        "    dataset_path = Path(\"dataset\") / \"imaging\" / dataset_name\n",
        "\n",
        "    dataset = al.Imaging.from_fits(\n",
        "        data_path=dataset_path / \"data.fits\",\n",
        "        psf_path=dataset_path / \"psf.fits\",\n",
        "        noise_map_path=dataset_path / \"noise_map.fits\",\n",
        "        pixel_scales=0.1,\n",
        "    )\n",
        "\n",
        "    mask_radius = 3.0\n",
        "\n",
        "    mask = al.Mask2D.circular(\n",
        "        shape_native=dataset.shape_native,\n",
        "        pixel_scales=dataset.pixel_scales,\n",
        "        radius=mask_radius,\n",
        "    )\n",
        "\n",
        "    dataset = dataset.apply_mask(mask=mask)\n",
        "\n",
        "    bulge = al.model_util.mge_model_from(\n",
        "        mask_radius=mask_radius,\n",
        "        total_gaussians=20,\n",
        "        gaussian_per_basis=1,\n",
        "        centre_prior_is_uniform=False,\n",
        "    )\n",
        "\n",
        "    model = af.Collection(\n",
        "        galaxies=af.Collection(\n",
        "            lens=af.Model(\n",
        "                al.Galaxy,\n",
        "                redshift=0.5,\n",
        "                mass=al.mp.Isothermal,\n",
        "                shear=al.mp.ExternalShear,\n",
        "            ),\n",
        "            source=af.Model(al.Galaxy, redshift=1.0, bulge=bulge, disk=None),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    search = af.Nautilus(\n",
        "        path_prefix=Path(\"results_folder_csv_png_fits\"),\n",
        "        name=\"results\",\n",
        "        unique_tag=f\"simple__no_lens_light_{i}\",\n",
        "        n_live=100,\n",
        "        n_batch=50,  # GPU batching and VRAM use explained in `modeling` examples.\n",
        "        iterations_per_quick_update=10000,\n",
        "    )\n",
        "\n",
        "    class AnalysisLatent(al.AnalysisImaging):\n",
        "\n",
        "        LATENT_KEYS = [\n",
        "            \"galaxies.lens.shear.magnitude\",\n",
        "            \"galaxies.lens.shear.angle\",\n",
        "        ]\n",
        "\n",
        "        def compute_latent_variables(self, parameters, model):\n",
        "            \"\"\"\n",
        "            A latent variable is not a model parameter but can be derived from the model. Its value and errors may be\n",
        "            of interest and aid in the interpretation of a model-fit.\n",
        "\n",
        "            This code implements a simple example of a latent variable, the magn\n",
        "\n",
        "            By overwriting this method we can manually specify latent variables that are calculated and output to\n",
        "            a `latent.csv` file, which mirrors the `samples.csv` file.\n",
        "\n",
        "            In the example below, the `latent.csv` file will contain at least two columns with the shear magnitude and\n",
        "            angle sampled by the non-linear search.\n",
        "\n",
        "            This function is called for every non-linear search sample, where the `instance` passed in corresponds to\n",
        "            each sample.\n",
        "\n",
        "            You can add your own custom latent variables here, if you have particular quantities that you\n",
        "            would like to output to the `latent.csv` file.\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            parameters : array-like\n",
        "                The parameter vector of the model sample. This will typically come from the non-linear search.\n",
        "                Inside this method it is mapped back to a model instance via `model.instance_from_vector`.\n",
        "            model : Model\n",
        "                The model object defining how the parameter vector is mapped to an instance. Passed explicitly\n",
        "                so that this function can be used inside JAX transforms (`vmap`, `jit`) with `functools.partial`.\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            A dictionary mapping every latent variable name to its value.\n",
        "\n",
        "            \"\"\"\n",
        "            instance = model.instance_from_vector(vector=parameters)\n",
        "\n",
        "            if hasattr(instance.galaxies.lens, \"shear\"):\n",
        "                magnitude, angle = al.convert.shear_magnitude_and_angle_from(\n",
        "                    gamma_1=instance.galaxies.lens.shear.gamma_1,\n",
        "                    gamma_2=instance.galaxies.lens.shear.gamma_2,\n",
        "                )\n",
        "\n",
        "            return (magnitude, angle)\n",
        "\n",
        "    analysis = AnalysisLatent(dataset=dataset)\n",
        "\n",
        "    result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Workflow Paths__\n",
        "\n",
        "The workflow examples are designed to take large libraries of results and distill them down to the key information\n",
        "required for your science, which are therefore placed in a single path for easy access.\n",
        "\n",
        "The `workflow_path` specifies where these files are output, in this case the .csv files which summarise the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "workflow_path = Path(\"output\") / \"results_folder_csv_png_fits\" / \"workflow_make_example\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Aggregator__\n",
        "\n",
        "Set up the aggregator as shown in `start_here.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from autofit.aggregator.aggregator import Aggregator\n",
        "\n",
        "agg = Aggregator.from_directory(\n",
        "    directory=Path(\"output\") / \"results_folder_csv_png_fits\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract the `AggregateCSV` object, which has specific functions for outputting results in a CSV format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg_csv = af.AggregateCSV(aggregator=agg)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Paths__\n",
        "\n",
        "The paths are the tuples which define how model parameters are accessed from the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = [model for model in agg.values(\"model\")][0]\n",
        "print(model.paths)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Adding CSV Columns__\n",
        "\n",
        "We first make a simple .csv which contains two columns, corresponding to the inferred median PDF values for\n",
        "the y centre of the mass of the lens galaxy and its einstein radius.\n",
        "\n",
        "To do this, we use the `add_variable` method, which adds a column to the .csv file we write at the end. Every time\n",
        "we call `add_variable` we add a new column to the .csv file.\n",
        "\n",
        "Note the API for the `centre`, which is a tuple parameter and therefore needs for `centre_0` to be specified.\n",
        "\n",
        "The `results_folder_csv_png_fits` contained two model-fits to two different datasets, meaning that each `add_variable` \n",
        "call will add three rows, corresponding to the three model-fits.\n",
        "\n",
        "This adds the median PDF value of the parameter to the .csv file, we show how to add other values later in this script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg_csv.add_variable(argument=\"galaxies.lens.mass.centre.centre_0\")\n",
        "agg_csv.add_variable(argument=\"galaxies.lens.mass.einstein_radius\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Saving the CSV__\n",
        "\n",
        "We can now output the results of all our model-fits to the .csv file, using the `save` method.\n",
        "\n",
        "This will output in your current working directory (e.g. the `autolens_workspace/output.results_folder_csv_png_fits`) \n",
        "as a .csv file containing the median PDF values of the parameters, have a quick look now to see the format of \n",
        "the .csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg_csv.save(path=workflow_path / \"csv_simple.csv\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Customizing CSV Headers__\n",
        "\n",
        "The headers of the .csv file are by default the argument input above based on the model. \n",
        "\n",
        "However, we can customize these headers using the `name` input of the `add_variable` method, for example making them\n",
        "shorter or more readable.\n",
        "\n",
        "We recreate the `agg_csv` first, so that we begin adding columns to a new .csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg_csv = af.AggregateCSV(aggregator=agg)\n",
        "\n",
        "agg_csv.add_variable(\n",
        "    argument=\"galaxies.lens.mass.centre.centre_0\",\n",
        "    name=\"mass_centre_0\",\n",
        ")\n",
        "agg_csv.add_variable(\n",
        "    argument=\"galaxies.lens.mass.einstein_radius\",\n",
        "    name=\"mass_einstein_radius\",\n",
        ")\n",
        "\n",
        "agg_csv.save(path=workflow_path / \"csv_simple_custom_headers.csv\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Maximum Likelihood Values__\n",
        "\n",
        "We can also output the maximum likelihood values of each parameter to the .csv file, using the `use_max_log_likelihood`\n",
        "input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg_csv = af.AggregateCSV(aggregator=agg)\n",
        "\n",
        "agg_csv.add_variable(\n",
        "    argument=\"galaxies.lens.mass.einstein_radius\",\n",
        "    name=\"mass_einstein_radius_max_lh\",\n",
        "    value_types=[af.ValueType.MaxLogLikelihood],\n",
        ")\n",
        "\n",
        "agg_csv.save(path=workflow_path / \"csv_simple_max_likelihood.csv\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Errors__\n",
        "\n",
        "We can also output PDF values at a given sigma confidence of each parameter to the .csv file, using \n",
        "the `af.ValueType.ValuesAt3Sigma` input and specifying the sigma confidence.\n",
        "\n",
        "Below, we add the values at 3.0 sigma confidence to the .csv file, in order to compute the errors you would \n",
        "subtract the median value from these values. We add this after the median value, so that the overall inferred\n",
        "uncertainty of the parameter is clear.\n",
        "\n",
        "The method below adds three columns to the .csv file, corresponding to the values at the median, lower and upper sigma \n",
        "values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg_csv = af.AggregateCSV(aggregator=agg)\n",
        "\n",
        "agg_csv.add_variable(\n",
        "    argument=\"galaxies.lens.mass.einstein_radius\",\n",
        "    name=\"mass_einstein_radius\",\n",
        "    value_types=[af.ValueType.Median, af.ValueType.ValuesAt3Sigma],\n",
        ")\n",
        "\n",
        "agg_csv.save(path=workflow_path / \"csv_simple_errors.csv\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Column Label List__\n",
        "\n",
        "We can add a list of values to the .csv file, provided the list is the same length as the number of model-fits\n",
        "in the aggregator.\n",
        "\n",
        "A useful example is adding the name of every dataset to the .csv file in a column on the left, indicating \n",
        "which dataset each row corresponds to.\n",
        "\n",
        "To make this list, we use the `Aggregator` to loop over the `search` objects and extract their `unique_tag`'s, which \n",
        "when we fitted the model above used the dataset names. This API can also be used to extract the `name` or `path_prefix`\n",
        "of the search and build an informative list for the names of the subplots.\n",
        "\n",
        "We then pass the column `name` and this list to the `add_label_column` method, which will add a column to the .csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg_csv = af.AggregateCSV(aggregator=agg)\n",
        "\n",
        "unique_tag_list = [search.unique_tag for search in agg.values(\"search\")]\n",
        "\n",
        "agg_csv.add_label_column(\n",
        "    name=\"lens_name\",\n",
        "    values=unique_tag_list,\n",
        ")\n",
        "\n",
        "agg_csv.save(path=workflow_path / \"csv_simple_dataset_name.csv\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Latent Variables__\n",
        "\n",
        "Latent variables are not free model parameters but can be derived from the model, and they are described fully in\n",
        "?.\n",
        "\n",
        "This example was run with a latent variable for the shear magnitude, and below we show that this latent variable\n",
        "can be added to the .csv file using the same API as above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg_csv = af.AggregateCSV(aggregator=agg)\n",
        "\n",
        "agg_csv.add_variable(\n",
        "    argument=\"galaxies.lens.shear.magnitude\",\n",
        ")\n",
        "\n",
        "agg_csv.save(path=workflow_path / \"csv_example_latent.csv\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Computed Columns__\n",
        "\n",
        "We can also add columns to the .csv file that are computed from the non-linear search samples (e.g. the nested sampling\n",
        "samples), for example a value derived from the median PDF instance values of the model.\n",
        "\n",
        "To do this, we write a function which is input into the `add_computed_column` method, where this function takes the\n",
        "median PDF instance as input and returns the computed value.\n",
        "\n",
        "Below, we add a trivial example of a computed column, where the median PDF value that is twice lens Einstein radius\n",
        "is computed and added to the .csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg_csv = af.AggregateCSV(aggregator=agg)\n",
        "\n",
        "\n",
        "def einstein_radius_x2_from(result):\n",
        "\n",
        "    samples = result.samples\n",
        "\n",
        "    instance = samples.median_pdf()\n",
        "\n",
        "    return 2.0 * instance.galaxies.lens.mass.einstein_radius\n",
        "\n",
        "\n",
        "agg_csv.add_computed_column(\n",
        "    name=\"bulge_einstein_radius_x2_computed\",\n",
        "    compute=einstein_radius_x2_from,\n",
        ")\n",
        "\n",
        "agg_csv.save(path=workflow_path / \"csv_computed_columns.csv\")\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}