{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results: PNG Make\n",
        "=================\n",
        "\n",
        "This example is a results workflow example, which means it provides tool to set up an effective workflow inspecting\n",
        "and interpreting the large libraries of modeling results.\n",
        "\n",
        "In this tutorial, we use the aggregator to load .png files output by a model-fit, make them together to create\n",
        "new .png images and then output them all to a single folder on your hard-disk.\n",
        "\n",
        "For example, a common use case is extracting a subset of 3 or 4 images from `subplot_fit.png` which show the model-fit\n",
        "quality, put them on a single line .png subplot and output them all to a single folder on your hard-disk. If you have\n",
        "modeled 100+ datasets, you can then inspect all fits as .pngs in a single folder (or make a single. png file of all of\n",
        "them which you scroll down), which is more efficient than clicking throughout the `output` folder to inspect\n",
        "each lens result one-by-one.\n",
        "\n",
        "Different .png images can be combined together, for example the goodness-of-fit images from `subplot.png`,\n",
        "RGB images of each galaxy in the `dataset` folder and other images.\n",
        "\n",
        "This enables the results of many model-fits to be concisely visualized and inspected, which can also be easily passed\n",
        "on to other collaborators.\n",
        "\n",
        "Internally, splicing uses the Python Imaging Library (PIL) to open, edit and save .png files. This is a Python library\n",
        "that provides extensive file format support, an efficient internal representation and powerful image-processing\n",
        "capabilities.\n",
        "\n",
        "__CSV, Png and Fits__\n",
        "\n",
        "Workflow functionality closely mirrors the `png_make.py` and `fits_make.py`  examples, which load results of\n",
        "model-fits and output th em as .png files and .fits files to quickly summarise results.\n",
        "\n",
        "The same initial fit creating results in a folder called `results_folder_csv_png_fits` is therefore used.\n",
        "\n",
        "__Interferometer__\n",
        "\n",
        "This script can easily be adapted to analyse the results of charge injection imaging model-fits.\n",
        "\n",
        "The only entries that needs changing are:\n",
        "\n",
        " - `ImagingAgg` -> `InterferometerAgg`.\n",
        " - `FitImagingAgg` -> `FitInterferometerAgg`.\n",
        " - `ImagingPlotter` -> `InterferometerPlotter`.\n",
        " - `FitImagingPlotter` -> `FitInterferometerPlotter`.\n",
        "\n",
        "Quantities specific to an interfometer, for example its uv-wavelengths real space mask, are accessed using the same API\n",
        "(e.g. `values(\"dataset.uv_wavelengths\")` and `.values{\"dataset.real_space_mask\")).\n",
        "\n",
        "__Database File__\n",
        "\n",
        "The aggregator can also load results from a `.sqlite` database file.\n",
        "\n",
        "This is beneficial when loading results for large numbers of model-fits (e.g. more than hundreds)\n",
        "because it is optimized for fast querying of results.\n",
        "\n",
        "See the package `results/database` for a full description of how to set up the database and the benefits it provides,\n",
        "especially if loading results from hard-disk is slow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from autoconf import jax_wrapper  # Sets JAX environment before other imports\n",
        "\n",
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from pathlib import Path\n",
        "from pathlib import Path\n",
        "\n",
        "import autofit as af\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fit__\n",
        "\n",
        "The code below performs a model-fit using nautilus. \n",
        "\n",
        "You should be familiar with modeling already, if not read the `modeling/start_here.py` script before reading this one!\n",
        "\n",
        "__Unique Tag__\n",
        "\n",
        "One thing to note is that the `unique_tag` of the search is given the name of the dataset with an index for the\n",
        "fit of 0 and 1. \n",
        "\n",
        "This `unique_tag` names the fit in a descriptive and human-readable way, which we will exploit to make our .png files\n",
        "more descriptive and easier to interpret."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for i in range(2):\n",
        "    dataset_name = \"simple__no_lens_light\"\n",
        "    dataset_path = Path(\"dataset\") / \"imaging\" / dataset_name\n",
        "\n",
        "    dataset = al.Imaging.from_fits(\n",
        "        data_path=dataset_path / \"data.fits\",\n",
        "        psf_path=dataset_path / \"psf.fits\",\n",
        "        noise_map_path=dataset_path / \"noise_map.fits\",\n",
        "        pixel_scales=0.1,\n",
        "    )\n",
        "\n",
        "    mask_radius = 3.0\n",
        "\n",
        "    mask = al.Mask2D.circular(\n",
        "        shape_native=dataset.shape_native,\n",
        "        pixel_scales=dataset.pixel_scales,\n",
        "        radius=mask_radius,\n",
        "    )\n",
        "\n",
        "    dataset = dataset.apply_mask(mask=mask)\n",
        "\n",
        "    bulge = al.model_util.mge_model_from(\n",
        "        mask_radius=mask_radius,\n",
        "        total_gaussians=20,\n",
        "        gaussian_per_basis=1,\n",
        "        centre_prior_is_uniform=False,\n",
        "    )\n",
        "\n",
        "    model = af.Collection(\n",
        "        galaxies=af.Collection(\n",
        "            lens=af.Model(\n",
        "                al.Galaxy,\n",
        "                redshift=0.5,\n",
        "                mass=al.mp.Isothermal,\n",
        "                shear=al.mp.ExternalShear,\n",
        "            ),\n",
        "            source=af.Model(al.Galaxy, redshift=1.0, bulge=bulge, disk=None),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    search = af.Nautilus(\n",
        "        path_prefix=Path(\"results_folder_csv_png_fits\"),\n",
        "        name=\"results\",\n",
        "        unique_tag=f\"simple__no_lens_light_{i}\",\n",
        "        n_live=100,\n",
        "        iterations_per_quick_update=10000,\n",
        "    )\n",
        "\n",
        "    class AnalysisLatent(al.AnalysisImaging):\n",
        "\n",
        "        LATENT_KEYS = [\n",
        "            \"galaxies.lens.shear.magnitude\",\n",
        "            \"galaxies.lens.shear.angle\",\n",
        "        ]\n",
        "\n",
        "        def compute_latent_variables(self, parameters, model):\n",
        "            \"\"\"\n",
        "            A latent variable is not a model parameter but can be derived from the model. Its value and errors may be\n",
        "            of interest and aid in the interpretation of a model-fit.\n",
        "\n",
        "            This code implements a simple example of a latent variable, the magn\n",
        "\n",
        "            By overwriting this method we can manually specify latent variables that are calculated and output to\n",
        "            a `latent.csv` file, which mirrors the `samples.csv` file.\n",
        "\n",
        "            In the example below, the `latent.csv` file will contain at least two columns with the shear magnitude and\n",
        "            angle sampled by the non-linear search.\n",
        "\n",
        "            This function is called for every non-linear search sample, where the `instance` passed in corresponds to\n",
        "            each sample.\n",
        "\n",
        "            You can add your own custom latent variables here, if you have particular quantities that you\n",
        "            would like to output to the `latent.csv` file.\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            parameters : array-like\n",
        "                The parameter vector of the model sample. This will typically come from the non-linear search.\n",
        "                Inside this method it is mapped back to a model instance via `model.instance_from_vector`.\n",
        "            model : Model\n",
        "                The model object defining how the parameter vector is mapped to an instance. Passed explicitly\n",
        "                so that this function can be used inside JAX transforms (`vmap`, `jit`) with `functools.partial`.\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            A dictionary mapping every latent variable name to its value.\n",
        "\n",
        "            \"\"\"\n",
        "            instance = model.instance_from_vector(vector=parameters)\n",
        "\n",
        "            if hasattr(instance.galaxies.lens, \"shear\"):\n",
        "                magnitude, angle = al.convert.shear_magnitude_and_angle_from(\n",
        "                    gamma_1=instance.galaxies.lens.shear.gamma_1,\n",
        "                    gamma_2=instance.galaxies.lens.shear.gamma_2,\n",
        "                )\n",
        "\n",
        "            return (magnitude, angle)\n",
        "\n",
        "    analysis = AnalysisLatent(dataset=dataset)\n",
        "\n",
        "    result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Workflow Paths__\n",
        "\n",
        "The workflow examples are designed to take large libraries of results and distill them down to the key information\n",
        "required for your science, which are therefore placed in a single path for easy access.\n",
        "\n",
        "The `workflow_path` specifies where these files are output, in this case the .png files containing the key \n",
        "results we require."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "workflow_path = Path(\"output\") / \"results_folder_csv_png_fits\" / \"workflow_make_example\"\n",
        "folder_path = workflow_path.parent if workflow_path.suffix else workflow_path\n",
        "folder_path.mkdir(parents=True, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Aggregator__\n",
        "\n",
        "Set up the aggregator as shown in `start_here.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from autofit.aggregator.aggregator import Aggregator\n",
        "\n",
        "agg = Aggregator.from_directory(\n",
        "    directory=Path(\"output\") / \"results_folder_csv_png_fits\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract the `AggregateImages` object, which has specific functions for loading image files (e.g. .png, .pdf) and\n",
        "outputting results in an image format (e.g. .png, .pdf)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg_image = af.AggregateImages(aggregator=agg)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Extract Images__\n",
        "\n",
        "We now extract 3 images from the `subplot_fit.png` file and make them together into a single image.\n",
        "\n",
        "We will extract the `data`, `model_data` and `normalized_residual_map` images, which are images you are used to\n",
        "plotting and inspecting in the `output` folder of a model-fit.\n",
        "\n",
        "We do this by simply passing the `agg_image.extract_image` method the `al.agg` attribute for each image we want to\n",
        "extract.\n",
        "\n",
        "This runs on all results the `Aggregator` object has loaded from the `output` folder, meaning that for this example\n",
        "where two model-fits are loaded, the `image` object contains two images.\n",
        "\n",
        "The `subplot_shape` input above determines the layout of the subplots in the final image, which for the example below\n",
        "is a single row of 3 subplots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "image = agg_image.extract_image(\n",
        "    subplots=[\n",
        "        al.agg.subplot_fit.data,\n",
        "        al.agg.subplot_fit.model_data,\n",
        "        al.agg.subplot_fit.normalized_residual_map,\n",
        "    ],\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Output Single Png__\n",
        "\n",
        "The `image` object which has been extracted is a `Image` object from the Python package `PIL`, which we use\n",
        "to save the image to the hard-disk as a .png file.\n",
        "\n",
        "The .png is a single subplot of two rows, where each subplot is the data, model data and residual-map of a model-fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "image.save(workflow_path / \"png_make_single_subplot.png\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Output to Folder__\n",
        "\n",
        "An alternative way to output the image is to output them as single .png files for each model-fit in a single folder,\n",
        "which is done using the `output_to_folder` method.\n",
        "\n",
        "It can sometimes be easier and quicker to inspect the results of many model-fits when they are output to individual\n",
        "files in a folder, as using an IDE you can click load and flick through the images. This contrasts a single .png\n",
        "file you scroll through, which may be slower to load and inspect.\n",
        "\n",
        "__Naming Convention__\n",
        "\n",
        "We require a naming convention for the output files. In this example, we have two model-fits, therefore two .png\n",
        "files are going to be output.\n",
        "\n",
        "One way to name the .png files is to use the `unique_tag` of the search, which is unique to every model-fit. For\n",
        "the search above, the `unique_tag` was `simple_0` and `simple_1`, therefore this will informatively name the .png\n",
        "files the names of the datasets.\n",
        "\n",
        "We achieve this behaviour by inputting `name=\"unique_tag\"` to the `output_to_folder` method. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg_image.output_to_folder(\n",
        "    folder=workflow_path,\n",
        "    name=\"unique_tag\",\n",
        "    subplots=[\n",
        "        al.agg.subplot_fit.data,\n",
        "        al.agg.subplot_fit.model_data,\n",
        "        al.agg.subplot_fit.normalized_residual_map,\n",
        "    ],\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `name` can be any search attribute, for example the `name` of the search, the `path_prefix` of the search, etc,\n",
        "if they will give informative names to the .png files.\n",
        "\n",
        "You can also manually input a list of names, one for each fit, if you want to name the .png files something else.\n",
        "However, the list must be the same length as the number of fits in the aggregator, and you may not be certain of the\n",
        "order of fits in the aggregator and therefore will need to extract this information, for example by printing the\n",
        "`unique_tag` of each search (or another attribute containing the dataset name)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print([search.unique_tag for search in agg.values(\"search\")])\n",
        "\n",
        "agg_image.output_to_folder(\n",
        "    folder=workflow_path,\n",
        "    name=\"unique_tag\",\n",
        "    subplots=[\n",
        "        al.agg.subplot_fit.data,\n",
        "        al.agg.subplot_fit.model_data,\n",
        "        al.agg.subplot_fit.normalized_residual_map,\n",
        "    ],\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Combine Images From Subplots__\n",
        "\n",
        "We now combine images from two different subplots into a single image, which we will save to the hard-disk as a .png\n",
        "file.\n",
        "\n",
        "We will extract images from the `subplot_dataset.png` and `subplot_fit.png` images, which are images you are used to \n",
        "plotting and inspecting in the `output` folder of a model-fit.\n",
        "\n",
        "We extract the `data` and `psf_log10` from the dataset and the `model_data` and `chi_squared_map` from the fit,\n",
        "and combine them into a subplot with an overall shape of (2, 2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "image = agg_image.extract_image(\n",
        "    subplots=[\n",
        "        al.agg.subplot_dataset.data,\n",
        "        al.agg.subplot_dataset.psf_log_10,\n",
        "        al.agg.subplot_fit.model_data,\n",
        "        al.agg.subplot_fit.chi_squared_map,\n",
        "    ]\n",
        "    # subplot_shape=(2, 2),\n",
        ")\n",
        "\n",
        "image.save(workflow_path / \"png_make_multi_subplot.png\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Custom Subplots in Analysis__\n",
        "\n",
        "Describe how a user can extend the `Analysis` class to compute custom images that are output to the .png files,\n",
        "which they can then extract and make together.\n",
        "\n",
        "__Path Navigation__\n",
        "\n",
        "Example combinng `subplot_fit.png` from `source_lp[1]` and `mass_total[0]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}