{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modeling: Start Here\n",
        "====================\n",
        "\n",
        "This script is the starting point for lens modeling of CCD imaging data (E.g. Hubble Space Telescope, Euclid) with\n",
        "**PyAutoLens** and it provides an overview of the lens modeling API.\n",
        "\n",
        "After reading this script, the `features`, `customize` and `searches` folders provide example for performing lens\n",
        "modeling in different ways and customizing the analysis.\n",
        "\n",
        "__Model__\n",
        "\n",
        "This script fits an `Imaging` dataset of a 'galaxy-scale' strong lens with a model where:\n",
        "\n",
        " - The lens galaxy's light is a linear parametric `Sersic` bulge.\n",
        " - The lens galaxy's total mass distribution is an `Isothermal` and `ExternalShear`.\n",
        " - The source galaxy's light is a linear parametric `SersicCore`.\n",
        "\n",
        "This lens model is simple and computationally fast to fit, and therefore acts as a good starting point for new\n",
        "users.\n",
        "\n",
        "__Plotters__\n",
        "\n",
        "To produce images of the data `Plotter` objects are used, which are high-level wrappers of matplotlib\n",
        "code which produce high quality visualization of strong lenses.\n",
        "\n",
        "The `PLotter` API is described in the script `autolens_workspace/*/plot/start_here.py`.\n",
        "\n",
        "__Simulation__\n",
        "\n",
        "This script fits a simulated `Imaging` dataset of a strong lens, which is produced in the\n",
        "script `autolens_workspace/*/simulators/imaging/start_here.py`\n",
        "\n",
        "__Data Preparation__\n",
        "\n",
        "The `Imaging` dataset fitted in this example confirms to a number of standard that make it suitable to be fitted in\n",
        "**PyAutoLens**.\n",
        "\n",
        "If you are intending to fit your own strong lens data, you will need to ensure it conforms to these standards, which are\n",
        "described in the script `autolens_workspace/*/data_preparation/imaging/start_here.ipynb`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "import autofit as af\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "Load the strong lens dataset `simple` via .fits files, which is a data format used by astronomers to store images.\n",
        "\n",
        "The `pixel_scales` define the arc-second to pixel conversion factor of the image, which for the dataset we are using \n",
        "is 0.1\" / pixel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"simple\"\n",
        "dataset_path = Path(\"dataset\") / \"imaging\" / dataset_name\n",
        "\n",
        "dataset = al.Imaging.from_fits(\n",
        "    data_path=dataset_path / \"data.fits\",\n",
        "    psf_path=dataset_path / \"psf.fits\",\n",
        "    noise_map_path=dataset_path / \"noise_map.fits\",\n",
        "    pixel_scales=0.1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use an `ImagingPlotter` the plot the data, including: \n",
        "\n",
        " - `data`: The image of the strong lens.\n",
        " - `noise_map`: The noise-map of the image, which quantifies the noise in every pixel as their RMS values.\n",
        " - `psf`: The point spread function of the image, which describes the blurring of the image by the telescope optics.\n",
        " - `signal_to_noise_map`: Quantifies the signal-to-noise in every pixel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Mask__\n",
        "\n",
        "The model-fit requires a 2D mask defining the regions of the image we fit the lens model to the data. \n",
        "\n",
        "We create a 3.0 arcsecond circular mask and apply it to the `Imaging` object that the lens model fits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mask_radius = 3.0\n",
        "\n",
        "mask = al.Mask2D.circular(\n",
        "    shape_native=dataset.shape_native,\n",
        "    pixel_scales=dataset.pixel_scales,\n",
        "    radius=mask_radius,\n",
        ")\n",
        "\n",
        "dataset = dataset.apply_mask(mask=mask)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we plot the masked data, the mask removes the exterior regions of the image where there is no emission from the \n",
        "lens and lensed source galaxies.\n",
        "\n",
        "The mask used to fit the data can be customized, as described in \n",
        "the script `autolens_workspace/*/modeling/imaging/customize/custom_mask.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Over Sampling__\n",
        "\n",
        "Over sampling is a numerical technique where the images of light profiles and galaxies are evaluated \n",
        "on a higher resolution grid than the image data to ensure the calculation is accurate. \n",
        "\n",
        "For lensing calculations, the high magnification regions of a lensed source galaxy require especially high levels of \n",
        "over sampling to ensure the lensed images are evaluated accurately.\n",
        "\n",
        "For a new user, the details of over-sampling are not important, therefore just be aware that calculations either:\n",
        "\n",
        " (i) use adaptive over sampling for the foregorund lens's light, which ensures high accuracy across. \n",
        " (ii) use cored light profiles for the background source galaxy, where the core ensures low levels of over-sampling \n",
        " produce numerically accurate but fast to compute results.\n",
        "\n",
        "\n",
        "Once you are more experienced, you should read up on over-sampling in more detail via \n",
        "the `autolens_workspace/*/guides/over_sampling.ipynb` notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "over_sample_size = al.util.over_sample.over_sample_size_via_radial_bins_from(\n",
        "    grid=dataset.grid,\n",
        "    sub_size_list=[4, 2, 1],\n",
        "    radial_list=[0.3, 0.6],\n",
        "    centre_list=[(0.0, 0.0)],\n",
        ")\n",
        "\n",
        "dataset = dataset.apply_over_sampling(over_sample_size_lp=over_sample_size)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The imaging subplot updates the bottom two panels to reflect the update to over sampling, which now uses a higher\n",
        "values in the centre.\n",
        "\n",
        "Whilst you may not yet understand the details of over-sampling, you can at least track it visually in the plots\n",
        "and later learnt more about it in the `over_sampling.ipynb` guide."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "In this example we compose a lens model where:\n",
        "\n",
        " - The lens galaxy's light is a linear parametric `Sersic` bulge [6 parameters].\n",
        " \n",
        " - The lens galaxy's total mass distribution is an `Isothermal` and `ExternalShear` [7 parameters].\n",
        " \n",
        " - The source galaxy's light is a linear parametric `SersicCore` [6 parameters].\n",
        "\n",
        "The number of free parameters and therefore the dimensionality of non-linear parameter space is N=21.\n",
        "\n",
        "__Model Composition__\n",
        "\n",
        "The API below for composing a lens model uses the `Model` and `Collection` objects, which are imported from \n",
        "**PyAutoLens**'s parent project **PyAutoFit** \n",
        "\n",
        "The API is fairly self explanatory and is straight forward to extend, for example adding more light profiles\n",
        "to the lens and source or using a different mass profile.\n",
        "\n",
        "__Model Cookbook__\n",
        "\n",
        "A full description of model composition is provided by the model cookbook: \n",
        "\n",
        "https://pyautolens.readthedocs.io/en/latest/general/model_cookbook.html\n",
        "\n",
        "__Coordinates__\n",
        "\n",
        "The model fitting default settings assume that the lens galaxy centre is near the coordinates (0.0\", 0.0\"). \n",
        "\n",
        "If for your dataset the lens is not centred at (0.0\", 0.0\"), we recommend that you either: \n",
        "\n",
        " - Reduce your data so that the centre is (`autolens_workspace/*/data_preparation`). \n",
        " - Manually override the lens model priors (`autolens_workspace/*/modeling/imaging/customize/priors.py`).\n",
        "\n",
        "__Over Sampling__\n",
        "\n",
        "As discussed above, a cored Sersic is used to ensure over-sampling is not required for the source galaxy's light.\n",
        "\n",
        "The lens galaxy is adaptively over sampled to a high degree, therefore a normal Sersic light profile is used.\n",
        "\n",
        "The over sampling guide fully explains how these choices, but new users should not worry for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Lens:\n",
        "\n",
        "bulge = al.model_util.mge_model_from(\n",
        "    mask_radius=mask_radius, total_gaussians=20, centre_prior_is_uniform=True\n",
        ")\n",
        "\n",
        "mass = af.Model(al.mp.Isothermal)\n",
        "\n",
        "shear = af.Model(al.mp.ExternalShear)\n",
        "\n",
        "lens = af.Model(al.Galaxy, redshift=0.5, bulge=bulge, mass=mass, shear=shear)\n",
        "\n",
        "# Source:\n",
        "\n",
        "bulge = al.model_util.mge_model_from(\n",
        "    mask_radius=mask_radius,\n",
        "    total_gaussians=20,\n",
        "    gaussian_per_basis=1,\n",
        "    centre_prior_is_uniform=False,\n",
        ")\n",
        "\n",
        "source = af.Model(al.Galaxy, redshift=1.0, bulge=bulge)\n",
        "\n",
        "# Overall Lens Model:\n",
        "\n",
        "model = af.Collection(galaxies=af.Collection(lens=lens, source=source))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `info` attribute shows the model in a readable format.\n",
        "\n",
        "The `info` below may not display optimally on your computer screen, for example the whitespace between parameter\n",
        "names on the left and parameter priors on the right may lead them to appear across multiple lines. This is a\n",
        "common issue in Jupyter notebooks.\n",
        "\n",
        "The`info_whitespace_length` parameter in the file `config/general.yaml` in the [output] section can be changed to \n",
        "increase or decrease the amount of whitespace (The Jupyter notebook kernel will need to be reset for this change to \n",
        "appear in a notebook)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Improved Lens Model__\n",
        "\n",
        "The previous model used S\u00e9rsic light profiles for the lens and source galaxies. This makes the model API concise, \n",
        "readable, and easy to follow.\n",
        "\n",
        "However, single S\u00e9rsic profiles perform poorly for most strong lenses. Symmetric profiles (e.g. elliptical S\u00e9rsics) \n",
        "typically leave significant residuals because they cannot capture the irregular and asymmetric morphology of real \n",
        "galaxies (e.g. isophotal twists, radially varying ellipticity).\n",
        "\n",
        "This example therefore uses a lens model that combines two features, described in detail elsewhere (but a brief \n",
        "overview is provided below):\n",
        "\n",
        "- **Linear light profiles**  (see ``autolens_workspace/*/modeling/features/linear_light_profiles.py``)\n",
        "- **Multi-Gaussian Expansion (MGE) light profiles**  (see ``autolens_workspace/*/modeling/features/multi_gaussian_expansion.py``)\n",
        "\n",
        "These features avoid wasted effort trying to fit S\u00e9rsic profiles to complex data, which is likely to fail unless the \n",
        "lens is extremely simple. This does mean the model composition is more complex and as a user its a steeper learning\n",
        "curve to understand the API, but its worth it for the improved accuracy and speed of lens modeling.\n",
        "\n",
        "__Multi-Gaussian Expansion (MGE)__\n",
        "\n",
        "A Multi-Gaussian Expansion (MGE) decomposes the lens and source light into ~50\u2013100 Gaussians with varying ellipticities \n",
        "and sizes. An MGE captures irregular features far more effectively than S\u00e9rsic profiles, leading to more accurate lens m\n",
        "odels.\n",
        "\n",
        "Remarkably, modeling with MGEs is also significantly faster than using S\u00e9rsics: they remain efficient in JAX (on CPU \n",
        "or GPU), require fewer non-linear parameters despite their flexibility, and yield simpler parameter spaces that\n",
        "sample in far fewer iterations. \n",
        "\n",
        "__Linear Light Profiles__\n",
        "\n",
        "The MGE model below uses a **linear light profile** for the bulge and disk via the ``lp_linear`` API, instead of the \n",
        "standard ``lp`` light profiles used above.\n",
        "\n",
        "A linear light profile solves for the *intensity* of each component via a linear inversion, rather than treating it as \n",
        "a free parameter. This reduces the dimensionality of the non-linear parameter space: a model with ~80 Gaussians\n",
        "does not introduce ~80 additional free parameters.\n",
        "\n",
        "Linear light profiles therefore improve speed and accuracy, and they are used by default in all modeling example.\n",
        "\n",
        "__Concise API__\n",
        "\n",
        "The MGE model composition API is quite long and technical, so we simply load the MGE models for the lens and source \n",
        "below via a utility function `mge_model_from` which hides the API to make the code in this introduction example ready \n",
        "to read. We then use the PyAutoLens Model API to compose the over lens model.\n",
        "\n",
        "The full MGE composition API is given in the `multi_gaussian_expansion.py` example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Lens:\n",
        "\n",
        "bulge = al.model_util.mge_model_from(\n",
        "    mask_radius=mask_radius,\n",
        "    total_gaussians=20,\n",
        "    gaussian_per_basis=2,\n",
        "    centre_prior_is_uniform=True,\n",
        ")\n",
        "\n",
        "mass = af.Model(al.mp.Isothermal)\n",
        "\n",
        "shear = af.Model(al.mp.ExternalShear)\n",
        "\n",
        "lens = af.Model(al.Galaxy, redshift=0.5, bulge=bulge, mass=mass, shear=shear)\n",
        "\n",
        "# Source:\n",
        "\n",
        "bulge = al.model_util.mge_model_from(\n",
        "    mask_radius=mask_radius, total_gaussians=20, centre_prior_is_uniform=False\n",
        ")\n",
        "\n",
        "source = af.Model(al.Galaxy, redshift=1.0, bulge=bulge)\n",
        "\n",
        "# Overall Lens Model:\n",
        "\n",
        "model = af.Collection(galaxies=af.Collection(lens=lens, source=source))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Printing the model info confirms the model has Gaussians for both the lens and source galaxies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "The lens model is fitted to the data using a non-linear search. \n",
        "\n",
        "All examples in the autolens workspace use the nested sampling algorithm \n",
        "Nautilus (https://nautilus-sampler.readthedocs.io/en/latest/), which extensive testing has revealed gives the most \n",
        "accurate and efficient modeling results.\n",
        "\n",
        "Nautilus has one main setting that trades-off accuracy and computational run-time, the number of `live_points`. \n",
        "A higher number of live points gives a more accurate result, but increases the run-time. A lower value give \n",
        "less reliable lens modeling (e.g. the fit may infer a local maxima), but is faster. \n",
        "\n",
        "The suitable value depends on the model complexity whereby models with more parameters require more live points. \n",
        "The default value of 200 is sufficient for the vast majority of common lens models. Lower values often given reliable\n",
        "results though, and speed up the run-times. In this example, given the model is quite simple (N=21 parameters), we \n",
        "reduce the number of live points to 100 to speed up the run-time.\n",
        "\n",
        "__Customization__\n",
        "\n",
        "The folders `autolens_workspace/*/modeling/imaging/searches` gives an overview of alternative non-linear searches,\n",
        "other than Nautilus, that can be used to fit lens models. They also provide details on how to customize the\n",
        "model-fit, for example the priors.\n",
        "\n",
        "The `name` and `path_prefix` below specify the path where results ae stored in the output folder:  \n",
        "\n",
        " `/autolens_workspace/output/modeling/imaging/simple/start_here/unique_identifier`.\n",
        "\n",
        "__Unique Identifier__\n",
        "\n",
        "In the path above, the `unique_identifier` appears as a collection of characters, where this identifier is generated \n",
        "based on the model, search and dataset that are used in the fit.\n",
        " \n",
        "An identical combination of model and search generates the same identifier, meaning that rerunning the script will use \n",
        "the existing results to resume the model-fit. In contrast, if you change the model or search, a new unique identifier \n",
        "will be generated, ensuring that the model-fit results are output into a separate folder.\n",
        "\n",
        "We additionally want the unique identifier to be specific to the dataset fitted, so that if we fit different datasets\n",
        "with the same model and search results are output to a different folder. We achieve this below by passing \n",
        "the `dataset_name` to the search's `unique_tag`.\n",
        "\n",
        "__Parallel Script__\n",
        "\n",
        "Depending on the operating system (e.g. Linux, Mac, Windows), Python version, if you are running a Jupyter notebook \n",
        "and other factors, this script may not run a successful parallel fit (e.g. running the script \n",
        "with `number_of_cores` > 1 will produce an error). It is also common for Jupyter notebooks to not run in parallel \n",
        "correctly, requiring a Python script to be run, often from a command line terminal.\n",
        "\n",
        "To fix these issues, the Python script needs to be adapted to use an `if __name__ == \"__main__\":` API, as this allows\n",
        "the Python `multiprocessing` module to allocate threads and jobs correctly. An adaptation of this example script \n",
        "is provided at `autolens_workspace/scripts/modeling/imaging/customize/parallel.py`, which will hopefully run \n",
        "successfully in parallel on your computer!\n",
        "\n",
        "Therefore if paralellization for this script doesn't work, check out the `parallel.py` example. You will need to update\n",
        "all scripts you run to use the this format and API. \n",
        "\n",
        "__Iterations Per Update__\n",
        "\n",
        "Every N iterations, the non-linear search outputs the maximum likelihood model and its best fit image to the \n",
        "Notebook visualizer and to hard-disk.\n",
        "\n",
        "This process takes around ~10 seconds, so we don't want it to happen too often so as to slow down the overall\n",
        "fit, but we also want it to happen frequently enough that we can track the progress.\n",
        "\n",
        "On GPU, a value of ~2500 will see this output happens every minute, a good balance. On CPU it'll be a little\n",
        "longer, but still a good balance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.Nautilus(\n",
        "    path_prefix=Path(\"imaging\") / \"modeling\",\n",
        "    name=\"start_here\",\n",
        "    unique_tag=dataset_name,\n",
        "    n_live=100,\n",
        "    n_batch=50,\n",
        "    iterations_per_quick_update=100000,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "We next create an `AnalysisImaging` object, which can be given many inputs customizing how the lens model is \n",
        "fitted to the data (in this example they are omitted for simplicity).\n",
        "\n",
        "Internally, this object defines the `log_likelihood_function` used by the non-linear search to fit the model to \n",
        "the `Imaging` dataset. \n",
        "\n",
        "It is not vital that you as a user understand the details of how the `log_likelihood_function` fits a lens model to \n",
        "data, but interested readers can find a step-by-step guide of the likelihood \n",
        "function at ``autolens_workspace/*/imaging/log_likelihood_function`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = al.AnalysisImaging(dataset=dataset)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Run Times__\n",
        "\n",
        "Lens modeling can be a computationally expensive process. When fitting complex models to high resolution datasets \n",
        "run times can be of order hours, days, weeks or even months.\n",
        "\n",
        "Run times are dictated by two factors:\n",
        "\n",
        " - The log likelihood evaluation time: the time it takes for a single `instance` of the lens model to be fitted to \n",
        "   the dataset such that a log likelihood is returned.\n",
        " \n",
        " - The number of iterations (e.g. log likelihood evaluations) performed by the non-linear search: more complex lens\n",
        "   models require more iterations to converge to a solution.\n",
        "   \n",
        "For this analysis, the log likelihood evaluation time is < 0.01 seconds on CPU, < 0.001 seconds on GPU, which is \n",
        "extremely fast for lens modeling. \n",
        "\n",
        "To estimate the expected overall run time of the model-fit we multiply the log likelihood evaluation time by an \n",
        "estimate of the number of iterations the non-linear search will perform, which is around 10000 to 30000 for this model.\n",
        "\n",
        "Tests thus far reveal CPU run times are around 30 minutes, and GPU run times around 10 minutes.\n",
        "\n",
        "__Model-Fit__\n",
        "\n",
        "We can now begin the model-fit by passing the model and analysis object to the search, which performs the \n",
        "Nautilus non-linear search in order to find which models fit the data with the highest likelihood."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Output Folder__\n",
        "\n",
        "Now this is running you should checkout the `autolens_workspace/output` folder. This is where the results of the \n",
        "search are written to hard-disk (in the `start_here` folder), where all outputs are human readable (e.g. as .json,\n",
        ".csv or text files).\n",
        "\n",
        "As the fit progresses, results are written to the `output` folder on the fly using the highest likelihood model found\n",
        "by the non-linear search so far. This means you can inspect the results of the model-fit as it runs, without having to\n",
        "wait for the non-linear search to terminate.\n",
        " \n",
        "The `output` folder includes:\n",
        "\n",
        " - `model.info`: Summarizes the lens model, its parameters and their priors discussed in the next tutorial.\n",
        " \n",
        " - `model.results`: Summarizes the highest likelihood lens model inferred so far including errors.\n",
        " \n",
        " - `images`: Visualization of the highest likelihood model-fit to the dataset, (e.g. a fit subplot showing the lens \n",
        " and source galaxies, model data and residuals).\n",
        " \n",
        " - `files`: A folder containing .fits files of the dataset, the model as a human-readable .json file, \n",
        " a `.csv` table of every non-linear search sample and other files containing information about the model-fit.\n",
        " \n",
        " - search.summary: A file providing summary statistics on the performance of the non-linear search.\n",
        " \n",
        " - `search_internal`: Internal files of the non-linear search (in this case Nautilus) used for resuming the fit and\n",
        "  visualizing the search.\n",
        "\n",
        "__Result__\n",
        "\n",
        "The search returns a result object, which whose `info` attribute shows the result in a readable format.\n",
        "\n",
        "[Above, we discussed that the `info_whitespace_length` parameter in the config files could b changed to make \n",
        "the `model.info` attribute display optimally on your computer. This attribute also controls the whitespace of the\n",
        "`result.info` attribute.]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Result` object also contains:\n",
        "\n",
        " - The model corresponding to the maximum log likelihood solution in parameter space.\n",
        " - The corresponding maximum log likelihood `Tracer` and `FitImaging` objects.\n",
        " \n",
        "Checkout `autolens_workspace/*/results` for a full description of analysing results in **PyAutoLens**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.max_log_likelihood_instance)\n",
        "\n",
        "tracer_plotter = aplt.TracerPlotter(\n",
        "    tracer=result.max_log_likelihood_tracer, grid=result.grids.lp\n",
        ")\n",
        "tracer_plotter.subplot_tracer()\n",
        "\n",
        "fit_plotter = aplt.FitImagingPlotter(fit=result.max_log_likelihood_fit)\n",
        "fit_plotter.subplot_fit()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It also contains information on the posterior as estimated by the non-linear search (in this example `Nautilus`). \n",
        "\n",
        "Below, we make a corner plot of the \"Probability Density Function\" of every parameter in the model-fit.\n",
        "\n",
        "The plot is labeled with short hand parameter names (e.g. `sersic_index` is mapped to the short hand \n",
        "parameter `n`). These mappings ate specified in the `config/notation.yaml` file and can be customized by users.\n",
        "\n",
        "The superscripts of labels correspond to the name each component was given in the model (e.g. for the `Isothermal`\n",
        "mass its name `mass` defined when making the `Model` above is used)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plotter = aplt.NestPlotter(samples=result.samples)\n",
        "plotter.corner_anesthetic()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This script gives a concise overview of the basic modeling API, fitting one the simplest lens models possible.\n",
        "\n",
        "Lets now consider what features you should read about to improve your lens modeling, especially if you are aiming\n",
        "to fit more complex models to your data.\n",
        "\n",
        "__Features__\n",
        "\n",
        "The examples in the `autolens_workspace/*/modeling/features` package illustrate other lens modeling features. \n",
        "\n",
        "We recommend you checkout the following features, because they make lens modeling in general more reliable and \n",
        "efficient (you will therefore benefit from using these features irrespective of the quality of your data and \n",
        "scientific topic of study).\n",
        "\n",
        "We recommend you now checkout the following features:\n",
        "\n",
        "- ``linear_light_profiles.py``: The model light profiles use linear algebra to solve for their intensity, reducing model complexity.\n",
        "- ``multi_gaussian_expansion.py``: The lens (or source) light is modeled as ~25-100 Gaussian basis functions \n",
        "- ``pixelization.py``: The source is reconstructed using an adaptive Rectangular or Voronoi mesh.\n",
        "- ``no_lens_light.py``: The foreground lens's light is not present in the data and thus omitted from the model.\n",
        "\n",
        "The folders `autolens_workspace/*/modeling/imaging/searches` and `autolens_workspace/*/modeling/imaging/customize`\n",
        "provide guides on how to customize many other aspects of the model-fit. Check them out to see if anything\n",
        "sounds useful, but for most users you can get by without using these forms of customization!\n",
        "  \n",
        "__Data Preparation__\n",
        "\n",
        "If you are looking to fit your own CCD imaging data of a strong lens, checkout  \n",
        "the `autolens_workspace/*/data_preparation/imaging/start_here.ipynb` script for an overview of how data should be \n",
        "prepared before being modeled.\n",
        "\n",
        "__HowToLens__\n",
        "\n",
        "This `start_here.py` script, and the features examples above, do not explain many details of how lens modeling is \n",
        "performed, for example:\n",
        "\n",
        " - How does PyAutoLens perform ray-tracing and lensing calculations in order to fit a lens model?\n",
        " - How is a lens model fitted to data? What quantifies the goodness of fit (e.g. how is a log likelihood computed?).\n",
        " - How does Nautilus find the highest likelihood lens models? What exactly is a \"non-linear search\"?\n",
        "\n",
        "You do not need to be able to answer these questions in order to fit lens models with PyAutoLens and do science.\n",
        "However, having a deeper understanding of how it all works is both interesting and will benefit you as a scientist\n",
        "\n",
        "This deeper insight is offered by the **HowToLens** Jupyter notebook lectures, found \n",
        "at `autolens_workspace/*/howtolens`. \n",
        "\n",
        "I recommend that you check them out if you are interested in more details!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}