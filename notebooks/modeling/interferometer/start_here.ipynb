{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modeling: Start Here\n",
        "====================\n",
        "\n",
        "This script is the starting point for lens modeling of interferometer datasets (e.g. ALMA, VLBI) with\n",
        "**PyAutoLens** and it provides an overview of the lens modeling API.\n",
        "\n",
        "After reading this script, the `features`, `customize` and `searches` folders provide example for performing lens\n",
        "modeling in different ways and customizing the analysis.\n",
        "\n",
        "__Model__\n",
        "\n",
        "This script fits `Interferometer` dataset of a 'galaxy-scale' strong lens with a model where:\n",
        "\n",
        " - The lens galaxy's light is omitted (and is not present in the simulated data).\n",
        " - The lens galaxy's total mass distribution is an `Isothermal` and `ExternalShear`.\n",
        " - The source galaxy's light is a linear parametric `SersicCore`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import autofit as af\n",
        "import autolens as al\n",
        "import autolens.plot as aplt\n",
        "import numpy as np"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Mask__\n",
        "\n",
        "We define the \u2018real_space_mask\u2019 which defines the grid the image the strong lens is evaluated using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "real_space_mask = al.Mask2D.circular(\n",
        "    shape_native=(800, 800), pixel_scales=0.05, radius=4.0\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "Load and plot the strong lens `Interferometer` dataset `simple` from .fits files, which we will fit \n",
        "with the lens model.\n",
        "\n",
        "This includes the method used to Fourier transform the real-space image of the strong lens to the uv-plane and compare \n",
        "directly to the visiblities. We use a non-uniform fast Fourier transform, which is the most efficient method for \n",
        "interferometer datasets containing ~1-10 million visibilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"simple\"\n",
        "dataset_path = path.join(\"dataset\", \"interferometer\", dataset_name)\n",
        "\n",
        "dataset = al.Interferometer.from_fits(\n",
        "    data_path=path.join(dataset_path, \"data.fits\"),\n",
        "    noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "    uv_wavelengths_path=path.join(dataset_path, \"uv_wavelengths.fits\"),\n",
        "    real_space_mask=real_space_mask,\n",
        "    transformer_class=al.TransformerDFT,\n",
        ")\n",
        "\n",
        "dataset_plotter = aplt.InterferometerPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()\n",
        "dataset_plotter.subplot_dirty_images()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Over Sampling__\n",
        "\n",
        "If you are familiar with using imaging data, you may have seen that a numerical technique called over sampling is used, \n",
        "which evaluates light profiles on a higher resolution grid than the image data to ensure the calculation is accurate.\n",
        "\n",
        "Interferometer does not observe galaxies in a way where over sampling is necessary, therefore all interferometer\n",
        "calculations are performed without over sampling.\n",
        "\n",
        "__Model__\n",
        "\n",
        "We compose our lens model using `Model` objects, which represent the galaxies we fit to our data. In this \n",
        "example our lens model is:\n",
        "\n",
        " - The lens galaxy's total mass distribution is an `Isothermal` with `ExternalShear` [7 parameters].\n",
        " - An `Sersic` `LightProfile` for the source galaxy's light [7 parameters].\n",
        "\n",
        "The number of free parameters and therefore the dimensionality of non-linear parameter space is N=14.\n",
        "\n",
        "__Linear Light Profiles__\n",
        "\n",
        "The model below uses a `linear light profile` for the bulge and disk, via the API `lp_linear`. This is a specific type \n",
        "of light profile that solves for the `intensity` of each profile that best fits the data via a linear inversion. \n",
        "This means it is not a free parameter, reducing the dimensionality of non-linear parameter space. \n",
        "\n",
        "Linear light profiles significantly improve the speed, accuracy and reliability of modeling and they are used\n",
        "by default in every modeling example. A full description of linear light profiles is provided in the\n",
        "`autolens_workspace/*/modeling/imaging/features/linear_light_profiles.py` example.\n",
        "\n",
        "A standard light profile can be used if you change the `lp_linear` to `lp`, but it is not recommended.\n",
        "\n",
        "__Coordinates__\n",
        "\n",
        "The model fitting default settings assume that the lens galaxy centre is near the coordinates (0.0\", 0.0\"). \n",
        "\n",
        "If for your dataset the  lens is not centred at (0.0\", 0.0\"), we recommend that you either: \n",
        "\n",
        " - Reduce your data so that the centre is (`autolens_workspace/*/data_preparation`). \n",
        " - Manually override the lens model priors (`autolens_workspace/*/modeling/imaging/customize/priors.py`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Lens:\n",
        "\n",
        "mass = af.Model(al.mp.Isothermal)\n",
        "shear = af.Model(al.mp.ExternalShear)\n",
        "\n",
        "lens = af.Model(al.Galaxy, redshift=0.5, mass=mass, shear=shear)\n",
        "\n",
        "# Source:\n",
        "\n",
        "source = af.Model(al.Galaxy, redshift=1.0, bulge=al.lp_linear.SersicCore)\n",
        "\n",
        "# Overall Lens Model:\n",
        "\n",
        "model = af.Collection(galaxies=af.Collection(lens=lens, source=source))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `info` attribute shows the model in a readable format.\n",
        "\n",
        "[The `info` below may not display optimally on your computer screen, for example the whitespace between parameter\n",
        "names on the left and parameter priors on the right may lead them to appear across multiple lines. This is a\n",
        "common issue in Jupyter notebooks.\n",
        "\n",
        "The`info_whitespace_length` parameter in the file `config/general.yaml` in the [output] section can be changed to \n",
        "increase or decrease the amount of whitespace (The Jupyter notebook kernel will need to be reset for this change to \n",
        "appear in a notebook).]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "The lens model is fitted to the data using the nested sampling algorithm Nautilus (see `start.here.py` for a \n",
        "full description).\n",
        "\n",
        "The folders: \n",
        "\n",
        " - `autolens_workspace/*/modeling/imaging/searches`.\n",
        " - `autolens_workspace/*/modeling/imaging/customize`\n",
        "  \n",
        "Give overviews of the non-linear searches **PyAutoLens** supports and more details on how to customize the\n",
        "model-fit, including the priors on the model.\n",
        "\n",
        "The `name` and `path_prefix` below specify the path where results are stored in the output folder:  \n",
        "\n",
        " `/autolens_workspace/output/imaging/simple/mass[sie]_source[bulge]/unique_identifier`.\n",
        "\n",
        "__Unique Identifier__\n",
        "\n",
        "In the path above, the `unique_identifier` appears as a collection of characters, where this identifier is generated \n",
        "based on the model, search and dataset that are used in the fit.\n",
        " \n",
        "An identical combination of model, search and dataset generates the same identifier, meaning that rerunning the\n",
        "script will use the existing results to resume the model-fit. In contrast, if you change the model, search or dataset,\n",
        "a new unique identifier will be generated, ensuring that the model-fit results are output into a separate folder.\n",
        "\n",
        "__Number Of Cores__\n",
        "\n",
        "We include an input `number_of_cores`, which when above 1 means that Nautilus uses parallel processing to sample multiple \n",
        "lens models at once on your CPU. When `number_of_cores=2` the search will run roughly two times as\n",
        "fast, for `number_of_cores=3` three times as fast, and so on. The downside is more cores on your CPU will be in-use\n",
        "which may hurt the general performance of your computer.\n",
        "\n",
        "You should experiment to figure out the highest value which does not give a noticeable loss in performance of your \n",
        "computer. If you know that your processor is a quad-core processor you should be able to use `number_of_cores=4`. \n",
        "\n",
        "Above `number_of_cores=4` the speed-up from parallelization diminishes greatly. We therefore recommend you do not\n",
        "use a value above this.\n",
        "\n",
        "For users on a Windows Operating system, using `number_of_cores>1` may lead to an error, in which case it should be \n",
        "reduced back to 1 to fix it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.Nautilus(\n",
        "    path_prefix=path.join(\"interferometer\"),\n",
        "    name=\"start_here\",\n",
        "    unique_tag=dataset_name,\n",
        "    n_live=100,\n",
        "    number_of_cores=1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "The `AnalysisInterferometer` object defines the `log_likelihood_function` used by the non-linear search to fit the \n",
        "model to the `Interferometer`dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = al.AnalysisInterferometer(dataset=dataset)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Run Times__\n",
        "\n",
        "Lens modeling can be a computationally expensive process. When fitting complex models to high resolution datasets \n",
        "run times can be of order hours, days, weeks or even months.\n",
        "\n",
        "Run times are dictated by two factors:\n",
        "\n",
        " - The log likelihood evaluation time: the time it takes for a single `instance` of the lens model to be fitted to \n",
        "   the dataset such that a log likelihood is returned.\n",
        "\n",
        " - The number of iterations (e.g. log likelihood evaluations) performed by the non-linear search: more complex lens\n",
        "   models require more iterations to converge to a solution.\n",
        "\n",
        "The log likelihood evaluation time can be estimated before a fit using the `profile_log_likelihood_function` method,\n",
        "which returns two dictionaries containing the run-times and information about the fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "run_time_dict, info_dict = analysis.profile_log_likelihood_function(\n",
        "    instance=model.random_instance()\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The overall log likelihood evaluation time is given by the `fit_time` key.\n",
        "\n",
        "For this example, it should be around ~0.25 seconds, which is extremely fast for interferometer lens modeling. \n",
        "More advanced lens modeling features (e.g. shapelets, multi Gaussian expansions, pixelizations) have slower log \n",
        "likelihood evaluation times (1-3 seconds), and you should be wary of this when using these features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Log Likelihood Evaluation Time (second) = {run_time_dict['fit_time']}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To estimate the expected overall run time of the model-fit we multiply the log likelihood evaluation time by an \n",
        "estimate of the number of iterations the non-linear search will perform. \n",
        "\n",
        "Estimating this is tricky, as it depends on the lens model complexity (e.g. number of parameters)\n",
        "and the properties of the dataset and model being fitted.\n",
        "\n",
        "For this example, we conservatively estimate that the non-linear search will perform ~10000 iterations per free \n",
        "parameter in the model. This is an upper limit, with models typically converging in far fewer iterations.\n",
        "\n",
        "If you perform the fit over multiple CPUs, you can divide the run time by the number of cores to get an estimate of\n",
        "the time it will take to fit the model. Parallelization with Nautilus scales well, it speeds up the model-fit by the \n",
        "`number_of_cores` for N < 8 CPUs and roughly `0.5*number_of_cores` for N > 8 CPUs. This scaling continues \n",
        "for N> 50 CPUs, meaning that with super computing facilities you can always achieve fast run times!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    \"Estimated Run Time Upper Limit (seconds) = \",\n",
        "    (run_time_dict[\"fit_time\"] * model.total_free_parameters * 10000)\n",
        "    / search.number_of_cores,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model-Fit__\n",
        "\n",
        "We begin the model-fit by passing the model and analysis object to the non-linear search (checkout the output folder\n",
        "for on-the-fly visualization and results)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Output Folder__\n",
        "\n",
        "Now this is running you should checkout the `autolens_workspace/output` folder. This is where the results of the \n",
        "search are written to hard-disk (in the `start_here` folder), where all outputs are human readable (e.g. as .json,\n",
        ".csv or text files).\n",
        "\n",
        "As the fit progresses, results are written to the `output` folder on the fly using the highest likelihood model found\n",
        "by the non-linear search so far. This means you can inspect the results of the model-fit as it runs, without having to\n",
        "wait for the non-linear search to terminate.\n",
        " \n",
        "The `output` folder includes:\n",
        "\n",
        " - `model.info`: Summarizes the lens model, its parameters and their priors discussed in the next tutorial.\n",
        " \n",
        " - `model.results`: Summarizes the highest likelihood lens model inferred so far including errors.\n",
        " \n",
        " - `images`: Visualization of the highest likelihood model-fit to the dataset, (e.g. a fit subplot showing the lens \n",
        " and source galaxies, model data and residuals).\n",
        " \n",
        " - `files`: A folder containing .fits files of the dataset, the model as a human-readable .json file, \n",
        " a `.csv` table of every non-linear search sample and other files containing information about the model-fit.\n",
        " \n",
        " - search.summary: A file providing summary statistics on the performance of the non-linear search.\n",
        " \n",
        " - `search_internal`: Internal files of the non-linear search (in this case Nautilus) used for resuming the fit and\n",
        "  visualizing the search.\n",
        "\n",
        "__Result__\n",
        "\n",
        "The search returns a result object, which whose `info` attribute shows the result in a readable format.\n",
        "\n",
        "[Above, we discussed that the `info_whitespace_length` parameter in the config files could b changed to make \n",
        "the `model.info` attribute display optimally on your computer. This attribute also controls the whitespace of the\n",
        "`result.info` attribute.]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We plot the maximum likelihood fit, tracer images and posteriors inferred via Nautilus.\n",
        "\n",
        "Checkout `autolens_workspace/*/imaging/results` for a full description of analysing results in **PyAutoLens**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.max_log_likelihood_instance)\n",
        "\n",
        "tracer_plotter = aplt.TracerPlotter(\n",
        "    tracer=result.max_log_likelihood_tracer,\n",
        "    grid=real_space_mask.derive_grid.unmasked,\n",
        ")\n",
        "tracer_plotter.subplot_tracer()\n",
        "\n",
        "fit_plotter = aplt.FitInterferometerPlotter(fit=result.max_log_likelihood_fit)\n",
        "fit_plotter.subplot_fit()\n",
        "fit_plotter.subplot_fit_dirty_images()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result contains the full posterior information of our non-linear search, including all parameter samples, \n",
        "log likelihood values and tools to compute the errors on the lens model. \n",
        "\n",
        "There are built in visualization tools for plotting this.\n",
        "\n",
        "The plot is labeled with short hand parameter names (e.g. `sersic_index` is mapped to the short hand \n",
        "parameter `n`). These mappings ate specified in the `config/notation.yaml` file and can be customized by users.\n",
        "\n",
        "The superscripts of labels correspond to the name each component was given in the model (e.g. for the `Isothermal`\n",
        "mass its name `mass` defined when making the `Model` above is used)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plotter = aplt.NestPlotter(samples=result.samples)\n",
        "plotter.corner_anesthetic()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This script gives a concise overview of the PyAutoLens modeling API, fitting one the simplest lens models possible.\n",
        "So, what next? \n",
        "\n",
        "__Features__\n",
        "\n",
        "The examples in the `autolens_workspace/*/interferometer/modeling/features` package illustrate other lens modeling \n",
        "features. \n",
        "\n",
        "We recommend you checkout the following two features, because they make lens modeling of interferometer datasets \n",
        "in general more reliable and  efficient (you will therefore benefit from using these features irrespective of the \n",
        "quality of your data and scientific topic of study).\n",
        "\n",
        "We recommend you now checkout the following two features for interferometer modeling:\n",
        "\n",
        "- ``linear_light_profiles.py``: The model light profiles use linear algebra to solve for their intensity, reducing model complexity.\n",
        "- ``pixelization.py``: The source is reconstructed using an adaptive Delaunay or Voronoi mesh.\n",
        "\n",
        "The folders `autolens_workspace/*/modeling/imaging/searches` and `autolens_workspace/*/modeling/imaging/customize`\n",
        "provide guides on how to customize many other aspects of the model-fit. Check them out to see if anything\n",
        "sounds useful, but for most users you can get by without using these forms of customization!\n",
        "  \n",
        "__Data Preparation__\n",
        "\n",
        "If you are looking to fit your own interferometer data of a strong lens, checkout  \n",
        "the `autolens_workspace/*/interferometer/data_preparation/start_here.ipynb` script for an overview of how data should be \n",
        "prepared before being modeled.\n",
        "\n",
        "__HowToLens__\n",
        "\n",
        "This `start_here.py` script, and the features examples above, do not explain many details of how lens modeling is \n",
        "performed, for example:\n",
        "\n",
        " - How does PyAutoLens perform ray-tracing and lensing calculations in order to fit a lens model?\n",
        " - How is a lens model fitted to data? What quantifies the goodness of fit (e.g. how is a log likelihood computed?).\n",
        " - How does Nautilus find the highest likelihood lens models? What exactly is a \"non-linear search\"?\n",
        "\n",
        "You do not need to be able to answer these questions in order to fit lens models with PyAutoLens and do science.\n",
        "However, having a deeper understanding of how it all works is both interesting and will benefit you as a scientist\n",
        "\n",
        "This deeper insight is offered by the **HowToLens** Jupyter notebook lectures, found \n",
        "at `autolens_workspace/*/howtolens`. \n",
        "\n",
        "I recommend that you check them out if you are interested in more details!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}