{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modeling: Group Start Here Extended\n",
        "===================================\n",
        "\n",
        "This script models an example strong lens on the 'group' scale, where there is a single primary lens galaxy\n",
        "and two smaller galaxies nearby, whose mass contributes significantly to the ray-tracing and is therefore included in\n",
        "the strong lens model.\n",
        "\n",
        "There are two methods for modeling group-scale strong lenses:\n",
        "\n",
        "- `point`: The source is modeled as a point source, where the positions of its multiple images are fitted.\n",
        "- `extended`: The source is modeled as an extended source, where the full emission is fitted, using imaging or interferometry data.\n",
        "\n",
        "This example demonstrates the `extended` method. For the `point` source modeling, refer to the `start_here_point.ipynb`\n",
        "example.\n",
        "\n",
        "Consider which method is appropriate for your dataset and the scientific goals of your analysis.\n",
        "\n",
        "After completing this example, explore other relevant modeling packages for your\n",
        "approach (e.g., `modeling/point_source` for point sources, `modeling/imaging` or `modeling/interferometer` for extended sources).\n",
        "\n",
        "You may also want to explore the `features` package to extend the model for specific requirements (e.g., using a\n",
        "pixelized source). Details on available features are provided at the end of this example.\n",
        "\n",
        "__Scaling Relations__\n",
        "\n",
        "This example models the mass of each galaxy individually, which means the number of dimensions of the model increases\n",
        "as we model group scale lenses with more galaxies. This can lead to a model that is slow to fit and poorly constrained.\n",
        "\n",
        "A common approach to overcome this is to put many of the galaxies a scaling relation, where the mass of the galaxies\n",
        "are related to their light via a observationally motivated scaling relation. This means that as more galaxies are\n",
        "included in the lens model, the dimensionality of the model does not increase.\n",
        "\n",
        "Lens modeling using scaling relations is fully support and described in the `features/scaling_relation.ipynb` example,\n",
        "you will likely want to read this example as soon as you have finished this one.\n",
        "\n",
        "__Example__\n",
        "\n",
        "This script fits an `Imaging` dataset of a 'group-scale' strong lens where\n",
        "\n",
        " - There is a main lens galaxy whose lens galaxy's light is a linear parametric `Sersic` bulge.\n",
        " - There is a main lens galaxy whose total mass distribution is an `Isothermal` and `ExternalShear`.\n",
        " - There are two extra lens galaxies whose light models are `SersicSph` profiles and total mass distributions\n",
        "   are `IsothermalSph` models.\n",
        " - The source galaxy's light is a linear parametric `SersicCore`.\n",
        "\n",
        " __Plotters__\n",
        "\n",
        "To produce images of the data `Plotter` objects are used, which are high-level wrappers of matplotlib\n",
        "code which produce high quality visualization of strong lenses.\n",
        "\n",
        "The `PLotter` API is described in the script `autolens_workspace/*/plot/start_here.py`.\n",
        "\n",
        "__Simulation__\n",
        "\n",
        "This script fits a simulated `Imaging` dataset of a strong lens, which is produced in the\n",
        "script `autolens_workspace/*/imaging/simulators/start_here.py`\n",
        "\n",
        "__Data Preparation__\n",
        "\n",
        "The `Imaging` dataset fitted in this example confirms to a number of standard that make it suitable to be fitted in\n",
        "**PyAutoLens**.\n",
        "\n",
        "If you are intending to fit your own strong lens data, you will need to ensure it conforms to these standards, which are\n",
        "described in the script `autolens_workspace/*/data_preparation/imaging/start_here.ipynb`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import autofit as af\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "Load the strong lens group dataset `simple`, which is the dataset we will use to perform lens modeling.\n",
        "\n",
        "This is loaded via .fits files, which is a data format used by astronomers to store images.\n",
        "\n",
        "The `pixel_scales` define the arc-second to pixel conversion factor of the image, which for the dataset we are using \n",
        "is 0.1\" / pixel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"simple\"\n",
        "dataset_path = path.join(\"dataset\", \"group\", dataset_name)\n",
        "\n",
        "dataset = al.Imaging.from_fits(\n",
        "    data_path=path.join(dataset_path, \"data.fits\"),\n",
        "    psf_path=path.join(dataset_path, \"psf.fits\"),\n",
        "    noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "    pixel_scales=0.1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use an `ImagingPlotter` the plot the data, including: \n",
        "\n",
        " - `data`: The image of the strong lens.\n",
        " - `noise_map`: The noise-map of the image, which quantifies the noise in every pixel as their RMS values.\n",
        " - `psf`: The point spread function of the image, which describes the blurring of the image by the telescope optics.\n",
        " - `signal_to_noise_map`: Quantifies the signal-to-noise in every pixel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Mask__\n",
        "\n",
        "The model-fit requires a `Mask2D` defining the regions of the image we fit the lens model to the data. \n",
        "\n",
        "Below, we create a 3.0 arcsecond circular mask and apply it to the `Imaging` object that the lens model fits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mask = al.Mask2D.circular(\n",
        "    shape_native=dataset.shape_native, pixel_scales=dataset.pixel_scales, radius=7.5\n",
        ")\n",
        "\n",
        "dataset = dataset.apply_mask(mask=mask)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we plot the masked data, the mask removes the exterior regions of the image where there is no emission from the \n",
        "lens and lensed source galaxies.\n",
        "\n",
        "The mask used to fit the data can be customized, as described in \n",
        "the script `autolens_workspace/*/modeling/imaging/customize/custom_mask.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Over Sampling__\n",
        "\n",
        "Over sampling is a numerical technique where the images of light profiles and galaxies are evaluated \n",
        "on a higher resolution grid than the image data to ensure the calculation is accurate. \n",
        "\n",
        "For lensing calculations, the high magnification regions of a lensed source galaxy require especially high levels of \n",
        "over sampling to ensure the lensed images are evaluated accurately.\n",
        "\n",
        "For a new user, the details of over-sampling are not important, therefore just be aware that calculations either:\n",
        "\n",
        " (i) use adaptive over sampling for the foregorund lens's light, which ensures high accuracy across. \n",
        " (ii) use cored light profiles for the background source galaxy, where the core ensures low levels of over-sampling \n",
        " produce numerically accurate but fast to compute results.\n",
        "\n",
        "This is why throughout the workspace the cored Sersic profile is used, instead of the regular Sersic profile which\n",
        "you may be more familiar with from the literature. Fitting a regular Sersic profile is possible, but you should\n",
        "read up on over-sampling to ensure the results are accurate.\n",
        "\n",
        "Once you are more experienced, you should read up on over-sampling in more detail via \n",
        "the `autolens_workspace/*/guides/over_sampling.ipynb` notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "over_sample_size = al.util.over_sample.over_sample_size_via_radial_bins_from(\n",
        "    grid=dataset.grid,\n",
        "    sub_size_list=[4, 4, 1],\n",
        "    radial_list=[0.3, 0.6],\n",
        "    centre_list=[(0.0, 0.0)],\n",
        ")\n",
        "\n",
        "dataset = dataset.apply_over_sampling(over_sample_size_lp=over_sample_size)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The imaging subplot updates the bottom two panels to reflect the update to over sampling, which now uses a higher\n",
        "values in the centre.\n",
        "\n",
        "Whilst you may not yet understand the details of over-sampling, you can at least track it visually in the plots\n",
        "and later learnt more about it in the `over_sampling.ipynb` guide."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Main Galaxies and Extra Galaxies__\n",
        "\n",
        "For a group-scale lens, we designate there to be two types of lens galaxies in the system:\n",
        "\n",
        " - `main_galaxies`: The main lens galaxies which likely make up the majority of light and mass in the lens system.\n",
        " These are modeled individually with a unique name for each, with their light and mass distributions modeled using \n",
        " parametric models.\n",
        " \n",
        " - `extra_galaxies`: The extra galaxies which are nearby the lens system and contribute to the lensing of the source\n",
        "  galaxy. These are modeled with a more restrictive model, for example with their centres fixed to the observed\n",
        "  centre of light and their mass distributions modeled using a scaling relation. These are grouped into a single \n",
        "  `extra_galaxies` collection.\n",
        "  \n",
        "In this simple example group scale lens, there is one main lens galaxy and two extra galaxies. \n",
        "\n",
        "__Centres__\n",
        "\n",
        "If the centres of the extra galaxies are treated as free parameters, one can run into the problem of having too many \n",
        "parameters and a model which is not fitted accurately.\n",
        "\n",
        "For group-scale lenses we therefore manually specify the centres of the extra galaxies, which are fixed to the observed\n",
        "centres of light of the galaxies. `centre_1` and `centre_2` are the observed centres of the extra galaxies.\n",
        "\n",
        "In a real analysis, one must determine the centres of the galaxies before modeling them, which can be done as follows:\n",
        "\n",
        " - Use the GUI tool in the `data_preparation/point_source/gui/extra_galaxies_centres.py` script to determine the centres\n",
        "   of the extra galaxies. \n",
        "\n",
        " - Use image processing software like Source Extractor (https://sextractor.readthedocs.io/en/latest/).\n",
        "\n",
        " - Fit every galaxy individually with a parametric light profile (e.g. an `Sersic`).\n",
        "\n",
        "__Redshifts__\n",
        "\n",
        "In this example all line of sight galaxies are at the same redshift as the lens galaxy, meaning multi-plane lensing\n",
        "is not used.\n",
        "\n",
        "If you have redshift information on the line of sight galaxies and some of their redshifts are different to the lens\n",
        "galaxy, you can easily extend this example below to perform multi-plane lensing.\n",
        "\n",
        "You would simply define a `redshift_list` and use this to set up the extra `Galaxy` redshifts.\n",
        "\n",
        "__Model__\n",
        "\n",
        "We compose a lens model where:\n",
        "\n",
        "  - The main lens galaxy's light is a linear parametric `Sersic` bulge [6 parameters].\n",
        "\n",
        " - The main lens galaxy's total mass distribution is an `Isothermal` and `ExternalShear` [7 parameters].\n",
        " \n",
        " - There are two extra lens galaxies with linear `SersicSph` light and `IsothermalSph` total mass distributions, with \n",
        "   centres fixed to the observed centres of light [8 parameters].\n",
        " \n",
        " - The source galaxy's light is a point `SersicCore` [6 parameters].\n",
        "\n",
        "The number of free parameters and therefore the dimensionality of non-linear parameter space is N=27.\n",
        "\n",
        "__Linear Light Profiles__\n",
        "\n",
        "The model below uses a `linear light profile` for the bulge and disk, via the API `lp_linear`. This is a specific type \n",
        "of light profile that solves for the `intensity` of each profile that best fits the data via a linear inversion. \n",
        "This means it is not a free parameter, reducing the dimensionality of non-linear parameter space. \n",
        "\n",
        "Linear light profiles significantly improve the speed, accuracy and reliability of modeling and they are used\n",
        "by default in every modeling example. A full description of linear light profiles is provided in the\n",
        "`autolens_workspace/*/modeling/features/linear_light_profiles.py` example.\n",
        "\n",
        "A standard light profile can be used if you change the `lp_linear` to `lp`, but it is not recommended.\n",
        "\n",
        "__Model Composition__\n",
        "\n",
        "The API below for composing a lens model uses the `Model` and `Collection` objects, which are imported from \n",
        "**PyAutoLens**'s parent project **PyAutoFit** \n",
        "\n",
        "The API is fairly self explanatory and is straight forward to extend, for example adding more light profiles\n",
        "to the lens and source or using a different mass profile.\n",
        "\n",
        "__Model Cookbook__\n",
        "\n",
        "A full description of model composition is provided by the model cookbook: \n",
        "\n",
        "https://pyautolens.readthedocs.io/en/latest/general/model_cookbook.html\n",
        "\n",
        "__Coordinates__\n",
        "\n",
        "The model fitting default settings assume that the lens galaxy centre is near the coordinates (0.0\", 0.0\"). \n",
        "\n",
        "If for your dataset the lens is not centred at (0.0\", 0.0\"), we recommend that you either: \n",
        "\n",
        " - Reduce your data so that the centre is (`autolens_workspace/*/data_preparation`). \n",
        " - Manually override the lens model priors (`autolens_workspace/*/modeling/imaging/customize/priors.py`).\n",
        "\n",
        "__Complexity__\n",
        "\n",
        "A 27 parameter model is a high degree of complexity, and means that group-scale moodeling may be slow, converge poorly\n",
        "and be difficult to interpret.\n",
        "\n",
        "This example is illustrative, but to succeed with group scale modeling you will likely need to used advanced \n",
        "autolens functionality to overcome this high dimensionality. These features and tools are described at the end\n",
        "of this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Main Lens:\n",
        "bulge = af.Model(al.lp_linear.Sersic)\n",
        "\n",
        "mass = af.Model(al.mp.Isothermal)\n",
        "\n",
        "shear = af.Model(al.mp.ExternalShear)\n",
        "\n",
        "lens = af.Model(al.Galaxy, redshift=0.5, bulge=bulge, mass=mass, shear=shear)\n",
        "\n",
        "# Extra Galaxies\n",
        "\n",
        "extra_galaxies_centres = [(3.5, 2.5), (-4.4, -5.0)]\n",
        "\n",
        "extra_galaxies_list = []\n",
        "\n",
        "for extra_galaxy_centre in extra_galaxies_centres:\n",
        "\n",
        "    # Extra Galaxy Light\n",
        "\n",
        "    bulge = af.Model(al.lp_linear.SersicSph)\n",
        "\n",
        "    bulge.centre = extra_galaxy_centre\n",
        "\n",
        "    # Extra Galaxy Mass\n",
        "\n",
        "    mass = af.Model(al.mp.IsothermalSph)\n",
        "\n",
        "    mass.centre = extra_galaxy_centre\n",
        "    mass.einstein_radius = af.UniformPrior(lower_limit=0.0, upper_limit=0.5)\n",
        "\n",
        "    # Extra Galaxy\n",
        "\n",
        "    extra_galaxy = af.Model(al.Galaxy, redshift=0.5, bulge=bulge, mass=mass)\n",
        "\n",
        "    extra_galaxies_list.append(extra_galaxy)\n",
        "\n",
        "extra_galaxies = af.Collection(extra_galaxies_list)\n",
        "\n",
        "# Source:\n",
        "\n",
        "source = af.Model(al.Galaxy, redshift=1.0, bulge=al.lp_linear.SersicCore)\n",
        "\n",
        "# Overall Lens Model:\n",
        "\n",
        "model = af.Collection(\n",
        "    galaxies=af.Collection(lens=lens, source=source), extra_galaxies=extra_galaxies\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `info` attribute shows the model in a readable format.\n",
        "\n",
        "This shows the group scale model, with separate entries for the main lens galaxy, the source galaxy and the \n",
        "extra galaxies.\n",
        "\n",
        "The `info` below may not display optimally on your computer screen, for example the whitespace between parameter\n",
        "names on the left and parameter priors on the right may lead them to appear across multiple lines. This is a\n",
        "common issue in Jupyter notebooks.\n",
        "\n",
        "The`info_whitespace_length` parameter in the file `config/general.yaml` in the [output] section can be changed to \n",
        "increase or decrease the amount of whitespace (The Jupyter notebook kernel will need to be reset for this change to \n",
        "appear in a notebook)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "The lens model is fitted to the data using the nested sampling algorithm Nautilus (see `start.here.py` for a \n",
        "full description).\n",
        "\n",
        "The folders: \n",
        "\n",
        " - `autolens_workspace/*/modeling/point_source/searches`.\n",
        " - `autolens_workspace/*/modeling/point_source/customize`\n",
        "  \n",
        "Give overviews of the non-linear searches **PyAutoLens** supports and more details on how to customize the\n",
        "model-fit, including the priors on the model.\n",
        "\n",
        "The `name` and `path_prefix` below specify the path where results ae stored in the output folder:  \n",
        "\n",
        " `/autolens_workspace/output/group/simple/mass[sie]_source[point]/unique_identifier`.\n",
        "\n",
        "__Unique Identifier__\n",
        "\n",
        "In the path above, the `unique_identifier` appears as a collection of characters, where this identifier is generated \n",
        "based on the model, search and dataset that are used in the fit.\n",
        "\n",
        "An identical combination of model, search and dataset generates the same identifier, meaning that rerunning the\n",
        "script will use the existing results to resume the model-fit. In contrast, if you change the model, search or dataset,\n",
        "a new unique identifier will be generated, ensuring that the model-fit results are output into a separate folder. \n",
        "\n",
        "__Number Of Cores__\n",
        "\n",
        "We include an input `number_of_cores`, which when above 1 means that Nautilus uses parallel processing to sample multiple \n",
        "lens models at once on your CPU. When `number_of_cores=2` the search will run roughly two times as\n",
        "fast, for `number_of_cores=3` three times as fast, and so on. The downside is more cores on your CPU will be in-use\n",
        "which may hurt the general performance of your computer.\n",
        "\n",
        "You should experiment to figure out the highest value which does not give a noticeable loss in performance of your \n",
        "computer. If you know that your processor is a quad-core processor you should be able to use `number_of_cores=4`. \n",
        "\n",
        "Above `number_of_cores=4` the speed-up from parallelization diminishes greatly. We therefore recommend you do not\n",
        "use a value above this.\n",
        "\n",
        "For users on a Windows Operating system, using `number_of_cores>1` may lead to an error, in which case it should be \n",
        "reduced back to 1 to fix it.\n",
        "\n",
        "__Parallel Script__\n",
        "\n",
        "Depending on the operating system (e.g. Linux, Mac, Windows), Python version, if you are running a Jupyter notebook \n",
        "and other factors, this script may not run a successful parallel fit (e.g. running the script \n",
        "with `number_of_cores` > 1 will produce an error). It is also common for Jupyter notebooks to not run in parallel \n",
        "correctly, requiring a Python script to be run, often from a command line terminal.\n",
        "\n",
        "To fix these issues, the Python script needs to be adapted to use an `if __name__ == \"__main__\":` API, as this allows\n",
        "the Python `multiprocessing` module to allocate threads and jobs correctly. An adaptation of this example script \n",
        "is provided at `autolens_workspace/scripts/modeling/point_source/customize/parallel.py`, which will hopefully run \n",
        "successfully in parallel on your computer!\n",
        "\n",
        "Therefore if paralellization for this script doesn't work, check out the `parallel.py` example. You will need to update\n",
        "all scripts you run to use the this format and API. \n",
        "\n",
        "__Iterations Per Update__\n",
        "\n",
        "Every N iterations, the non-linear search outputs the current results to the folder `autolens_workspace/output`,\n",
        "which includes producing visualization. \n",
        "\n",
        "Depending on how long it takes for the model to be fitted to the data (see discussion about run times below), \n",
        "this can take up a large fraction of the run-time of the non-linear search.\n",
        "\n",
        "For this fit, the fit is very fast, thus we set a high value of `iterations_per_update=10000` to ensure these updates\n",
        "so not slow down the overall speed of the model-fit.\n",
        "\n",
        "**If the iteration per update is too low, the model-fit may be significantly slowed down by the time it takes to\n",
        "output results and visualization frequently to hard-disk. If your fit is consistent displaying a log saying that it\n",
        "is outputting results, try increasing this value to ensure the model-fit runs efficiently.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.Nautilus(\n",
        "    path_prefix=path.join(\"group\", \"modeling\"),\n",
        "    name=\"start_here_extended\",\n",
        "    unique_tag=dataset_name,\n",
        "    n_live=300,  # Increased to higher value than many examples to account for the high dimensionality of the model.\n",
        "    number_of_cores=4,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "We next create an `AnalysisImaging` object, which can be given many inputs customizing how the lens model is \n",
        "fitted to the data (in this example they are omitted for simplicity).\n",
        "\n",
        "Internally, this object defines the `log_likelihood_function` used by the non-linear search to fit the model to \n",
        "the `Imaging` dataset. \n",
        "\n",
        "It is not vital that you as a user understand the details of how the `log_likelihood_function` fits a lens model to \n",
        "data, but interested readers can find a step-by-step guide of the likelihood \n",
        "function at ``autolens_workspace/*/imaging/log_likelihood_function`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = al.AnalysisImaging(dataset=dataset)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Run Times__\n",
        "\n",
        "Lens modeling can be a computationally expensive process. When fitting complex models to high resolution datasets \n",
        "run times can be of order hours, days, weeks or even months.\n",
        "\n",
        "Run times are dictated by two factors:\n",
        "\n",
        " - The log likelihood evaluation time: the time it takes for a single `instance` of the lens model to be fitted to \n",
        "   the dataset such that a log likelihood is returned.\n",
        "\n",
        " - The number of iterations (e.g. log likelihood evaluations) performed by the non-linear search: more complex lens\n",
        "   models require more iterations to converge to a solution.\n",
        "\n",
        "The log likelihood evaluation time can be estimated before a fit using the `profile_log_likelihood_function` method,\n",
        "which returns two dictionaries containing the run-times and information about the fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "run_time_dict, info_dict = analysis.profile_log_likelihood_function(\n",
        "    instance=model.random_instance()\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The overall log likelihood evaluation time is given by the `fit_time` key.\n",
        "\n",
        "For this example, it is ~0.001 seconds, which is extremely fast for lens modeling. The source-plane chi-squared\n",
        "is possibly the fastest way to fit a lens model to a dataset, and therefore whilst it has limitations it is a good\n",
        "way to get a rough estimate of the lens model parameters quickly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Log Likelihood Evaluation Time (second) = {run_time_dict['fit_time']}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To estimate the expected overall run time of the model-fit we multiply the log likelihood evaluation time by an \n",
        "estimate of the number of iterations the non-linear search will perform. \n",
        "\n",
        "Estimating this is tricky, as it depends on the lens model complexity (e.g. number of parameters)\n",
        "and the properties of the dataset and model being fitted.\n",
        "\n",
        "For this example, we conservatively estimate that the non-linear search will perform ~10000 iterations per free \n",
        "parameter in the model. This is an upper limit, with models typically converging in far fewer iterations.\n",
        "\n",
        "If you perform the fit over multiple CPUs, you can divide the run time by the number of cores to get an estimate of\n",
        "the time it will take to fit the model. Parallelization with Nautilus scales well, it speeds up the model-fit by the \n",
        "`number_of_cores` for N < 8 CPUs and roughly `0.5*number_of_cores` for N > 8 CPUs. This scaling continues \n",
        "for N> 50 CPUs, meaning that with super computing facilities you can always achieve fast run times!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    \"Estimated Run Time Upper Limit (seconds) = \",\n",
        "    (run_time_dict[\"fit_time\"] * model.total_free_parameters * 10000)\n",
        "    / search.number_of_cores,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model-Fit__\n",
        "\n",
        "We begin the model-fit by passing the model and analysis object to the non-linear search (checkout the output folder\n",
        "for on-the-fly visualization and results)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Output Folder__\n",
        "\n",
        "Now this is running you should checkout the `autolens_workspace/output` folder. This is where the results of the \n",
        "search are written to hard-disk (in the `start_here` folder), where all outputs are human readable (e.g. as .json,\n",
        ".csv or text files).\n",
        "\n",
        "As the fit progresses, results are written to the `output` folder on the fly using the highest likelihood model found\n",
        "by the non-linear search so far. This means you can inspect the results of the model-fit as it runs, without having to\n",
        "wait for the non-linear search to terminate.\n",
        "\n",
        "The `output` folder includes:\n",
        "\n",
        " - `model.info`: Summarizes the lens model, its parameters and their priors discussed in the next tutorial.\n",
        "\n",
        " - `model.results`: Summarizes the highest likelihood lens model inferred so far including errors.\n",
        "\n",
        " - `images`: Visualization of the highest likelihood model-fit to the dataset, (e.g. a fit subplot showing the lens \n",
        " and source galaxies, model data and residuals).\n",
        "\n",
        " - `files`: A folder containing .fits files of the dataset, the model as a human-readable .json file, \n",
        " a `.csv` table of every non-linear search sample and other files containing information about the model-fit.\n",
        "\n",
        " - search.summary: A file providing summary statistics on the performance of the non-linear search.\n",
        "\n",
        " - `search_internal`: Internal files of the non-linear search (in this case Nautilus) used for resuming the fit and\n",
        "  visualizing the search.\n",
        "\n",
        "__Result__\n",
        "\n",
        "The search returns a result object, which whose `info` attribute shows the result in a readable format.\n",
        "\n",
        "[Above, we discussed that the `info_whitespace_length` parameter in the config files could b changed to make \n",
        "the `model.info` attribute display optimally on your computer. This attribute also controls the whitespace of the\n",
        "`result.info` attribute.]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Result` object also contains:\n",
        "\n",
        " - The model corresponding to the maximum log likelihood solution in parameter space.\n",
        " - The corresponding maximum log likelihood `Tracer` and `FitImaging` objects.\n",
        "\n",
        "Checkout `autolens_workspace/*/results` for a full description of analysing results in **PyAutoLens**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.max_log_likelihood_instance)\n",
        "\n",
        "tracer_plotter = aplt.TracerPlotter(\n",
        "    tracer=result.max_log_likelihood_tracer, grid=result.grids.lp\n",
        ")\n",
        "tracer_plotter.subplot_tracer()\n",
        "\n",
        "fit_plotter = aplt.FitImagingPlotter(fit=result.max_log_likelihood_fit)\n",
        "fit_plotter.subplot_fit()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It also contains information on the posterior as estimated by the non-linear search (in this example `Nautilus`). \n",
        "\n",
        "Below, we make a corner plot of the \"Probability Density Function\" of every parameter in the model-fit.\n",
        "\n",
        "The plot is labeled with short hand parameter names (e.g. `sersic_index` is mapped to the short hand \n",
        "parameter `n`). These mappings ate specified in the `config/notation.yaml` file and can be customized by users.\n",
        "\n",
        "The superscripts of labels correspond to the name each component was given in the model (e.g. for the `Isothermal`\n",
        "mass its name `mass` defined when making the `Model` above is used)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plotter = aplt.NestPlotter(samples=result.samples)\n",
        "plotter.corner_anesthetic()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This script gives a concise overview of the basic modeling API, fitting one the simplest lens models possible.\n",
        "\n",
        "Lets now consider what features you should read about to improve your lens modeling, especially if you are aiming\n",
        "to fit more complex models to your data.\n",
        "\n",
        "This is especially important for group scale modeling, in order to reduce the complexity of the model.\n",
        "\n",
        "__Features__\n",
        "\n",
        "The examples in the `autolens_workspace/*/modeling/features` package illustrate other lens modeling features. \n",
        "\n",
        "The examples in the `autolens_workspace/*/modeling/features` package illustrate other lens modeling features. \n",
        "\n",
        "We recommend you checkout the following features, because they make lens modeling in general more reliable and \n",
        "efficient (you will therefore benefit from using these features irrespective of the quality of your data and \n",
        "scientific topic of study).\n",
        "\n",
        "We recommend you now checkout the following features:\n",
        "\n",
        "- ``scaling_relation.ipynb``: This feature allows you to model the light and mass of the extra galaxies using a scaling relation.\n",
        "- ``linear_light_profiles.py``: The model light profiles use linear algebra to solve for their intensity, reducing model complexity.\n",
        "- ``multi_gaussian_expansion.py``: The lens (or source) light is modeled as ~25-100 Gaussian basis functions \n",
        "- ``pixelization.py``: The source is reconstructed using an adaptive Delaunay or Voronoi mesh.\n",
        "- ``no_lens_light.py``: The foreground lens's light is not present in the data and thus omitted from the model.\n",
        "\n",
        "For group scale modeling, the multi Gaussian expansion is particularly important, as this can dramatically reduce the\n",
        "dimensionality of the model and improve the accuracy of the fit for both the lens and source galaxies.\n",
        "\n",
        "It is also recommended you read through the `imaging` package, to get a complete picture of how point-source \n",
        "modeling works.\n",
        "\n",
        "__Data Preparation__\n",
        "\n",
        "If you are looking to fit your own CCD imaging data of a strong lens, checkout  \n",
        "the `autolens_workspace/*/data_preparation/imaging/start_here.ipynb` script for an overview of how data should be \n",
        "prepared before being modeled.\n",
        "\n",
        "__HowToLens__\n",
        "\n",
        "This `start_here.py` script, and the features examples above, do not explain many details of how lens modeling is \n",
        "performed, for example:\n",
        "\n",
        " - How does PyAutoLens perform ray-tracing and lensing calculations in order to fit a lens model?\n",
        " - How is a lens model fitted to data? What quantifies the goodness of fit (e.g. how is a log likelihood computed?).\n",
        " - How does Nautilus find the highest likelihood lens models? What exactly is a \"non-linear search\"?\n",
        "\n",
        "You do not need to be able to answer these questions in order to fit lens models with PyAutoLens and do science.\n",
        "However, having a deeper understanding of how it all works is both interesting and will benefit you as a scientist\n",
        "\n",
        "This deeper insight is offered by the **HowToLens** Jupyter notebook lectures, found \n",
        "at `autolens_workspace/*/howtolens`. \n",
        "\n",
        "I recommend that you check them out if you are interested in more details!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}