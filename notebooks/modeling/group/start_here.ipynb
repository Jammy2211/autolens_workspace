{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modeling: Lens x3 + Source x1\n",
        "=============================\n",
        "\n",
        "This script fits a `PointDataset` dataset of a 'group-scale' strong lens where:\n",
        "\n",
        " - There are three lens galaxies whose light models are `SersicSph` profiles and total mass distributions\n",
        " are `IsothermalSph` models.\n",
        " - The source `Galaxy` is modeled as a point source `Point`.\n",
        "\n",
        "The point-source dataset used in this example consists of the positions of the lensed source's multiple images and\n",
        "their fluxes, both of which are used in the fit.\n",
        "\n",
        "__Strong Lens Scale__\n",
        "\n",
        "This script models an example strong lens on the 'group' scale, where there is a single primary lens galaxy\n",
        "and two smaller galaxies nearby, whose mass contributes significantly to the ray-tracing and is therefore included in\n",
        "the strong lens model.\n",
        "\n",
        "In this example we model the source as a point-source, as fitting the full `Imaging` data and extended emission in the\n",
        "lensed source's arcs is challenging due to the high complexity of the lens model.\n",
        "\n",
        "The `group/chaining` package includes an example script showing how **PyAutoLens** can model this dataset's full\n",
        "extended emission, however this requires familiarity's advanced feature called 'search chaining'\n",
        "which is covered in chapter 3 of **HowToLens**. This package also shows how to do this using a pixelized source\n",
        "reconstruction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import autofit as af\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "Load the strong lens dataset `group`, which is the dataset we will use to perform lens modeling.\n",
        "\n",
        "We begin by loading an image of the dataset. Although we perform point-source modeling and will not use this data in \n",
        "the model-fit, it is useful to load it for visualization. By passing this dataset to the model-fit at the\n",
        "end of the script it will be used when visualizing the results. However, the use of an image in this way is entirely\n",
        "optional, and if it were not included in the model-fit visualization would simple be performed using grids without\n",
        "the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"lens_x3__source_x1\"\n",
        "dataset_path = path.join(\"dataset\", \"group\", dataset_name)\n",
        "\n",
        "data = al.Array2D.from_fits(\n",
        "    file_path=path.join(dataset_path, \"data.fits\"), pixel_scales=0.1\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now load the point source dataset we will fit using point source modeling. \n",
        "\n",
        "We load this data as a `PointDataset`, which contains the positions and fluxes of every point source. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset = al.from_json(\n",
        "    file_path=path.join(dataset_path, \"point_dataset.json\"),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can print this dictionary to see the dataset's `name`, `positions` and `fluxes` and noise-map values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Point Dataset Info:\")\n",
        "print(dataset.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can plot our positions dataset over the observed image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "visuals = aplt.Visuals2D(positions=dataset.positions)\n",
        "\n",
        "array_plotter = aplt.Array2DPlotter(array=data, visuals_2d=visuals)\n",
        "array_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also just plot the positions, omitting the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grid_plotter = aplt.Grid2DPlotter(grid=dataset.positions)\n",
        "grid_plotter.figure_2d()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Point Solver__\n",
        "\n",
        "For point-source modeling we require a `PointSolver`, which determines the multiple-images of the mass model for a \n",
        "point source at location (y,x) in the source plane. \n",
        "\n",
        "It does this by ray tracing triangles from the image-plane to the source-plane and calculating if the \n",
        "source-plane (y,x) centre is inside the triangle. The method gradually ray-traces smaller and smaller triangles so \n",
        "that the multiple images can be determine with sub-pixel precision.\n",
        "\n",
        "The `PointSolver` requires a starting grid of (y,x) coordinates in the image-plane which defines the first set\n",
        "of triangles that are ray-traced to the source-plane. It also requires that a `pixel_scale_precision` is input, \n",
        "which is the resolution up to which the multiple images are computed. The lower the `pixel_scale_precision`, the\n",
        "longer the calculation, with the value of 0.001 below balancing efficiency with precision.\n",
        "\n",
        "Strong lens mass models have a multiple image called the \"central image\". However, the image is nearly always \n",
        "significantly demagnified, meaning that it is not observed and cannot constrain the lens model. As this image is a\n",
        "valid multiple image, the `PointSolver` will locate it irrespective of whether its so demagnified it is not observed.\n",
        "To ensure this does not occur, we set a `magnification_threshold=0.1`, which discards this image because its\n",
        "magnification will be well below this threshold.\n",
        "\n",
        "If your dataset contains a central image that is observed you should reduce to include it in\n",
        "the analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grid = al.Grid2D.uniform(\n",
        "    shape_native=(200, 200),\n",
        "    pixel_scales=0.2,  # <- The pixel-scale describes the conversion from pixel units to arc-seconds.\n",
        ")\n",
        "\n",
        "solver = al.PointSolver.for_grid(\n",
        "    grid=grid, pixel_scale_precision=0.001, magnification_threshold=0.1\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "We compose a lens model where:\n",
        "\n",
        " - There are three lens galaxy's with `IsothermalSph` total mass distributions, with the prior on the centre of each \n",
        " profile informed by its observed centre of light [9 parameters].\n",
        " - The source galaxy's light is a point `PointFlux` [3 parameters].\n",
        "\n",
        "The number of free parameters and therefore the dimensionality of non-linear parameter space is N=12.\n",
        "\n",
        "__Model JSON File_\n",
        "\n",
        "For group modeling, there can be many lens and source galaxies. Manually writing the model in a Python script, in the\n",
        "way we do for galaxy-scale lenses, is therefore not ideal.\n",
        "\n",
        "We therefore write the model for this system in a separate Python file and output it to a .json file, which we created \n",
        "via the script `group/model_maker/lens_x3__source_x1.py` and can be found in the \n",
        "file `group/models`. \n",
        "\n",
        "This file is used to load the model below and it can be easily altered to compose a group model suited to your lens \n",
        "dataset!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_path = path.join(\"dataset\", \"group\", dataset_name)\n",
        "\n",
        "lenses_file = path.join(model_path, \"lenses.json\")\n",
        "lenses = af.Collection.from_json(file=lenses_file)\n",
        "\n",
        "sources_file = path.join(model_path, \"sources.json\")\n",
        "sources = af.Collection.from_json(file=sources_file)\n",
        "\n",
        "galaxies = lenses + sources\n",
        "\n",
        "model = af.Collection(galaxies=galaxies)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `info` attribute shows the model in a readable format.\n",
        "\n",
        "The source does not use the ``Point`` class discussed in the previous overview example, but instead uses\n",
        "a ``Point`` object.\n",
        "\n",
        "This object changes the behaviour of how the positions in the point dataset are fitted. For a normal ``Point`` object,\n",
        "the positions are fitted in the image-plane, by mapping the source-plane back to the image-plane via the lens model\n",
        "and iteratively searching for the best-fit solution.\n",
        "\n",
        "The ``Point`` object instead fits the positions directly in the source-plane, by mapping the image-plane\n",
        "positions to the source just one. This is a much faster way to fit the positions,and for group scale lenses it\n",
        "typically sufficient to infer an accurate lens model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Name Pairing__\n",
        "\n",
        "Every point-source dataset in the `PointDataset` has a name, which in this example was `point_0`. This `name` pairs \n",
        "the dataset to the `Point` in the model below. Because the name of the dataset is `point_0`, the \n",
        "only `Point` object that is used to fit it must have the name `point_0`.\n",
        "\n",
        "If there is no point-source in the model that has the same name as a `PointDataset`, that data is not used in\n",
        "the model-fit. If a point-source is included in the model whose name has no corresponding entry in \n",
        "the `PointDataset` **PyAutoLens** will raise an error.\n",
        "\n",
        "In this example, where there is just one source, name pairing appears unnecessary. However, point-source datasets may\n",
        "have many source galaxies in them, and name pairing is necessary to ensure every point source in the lens model is \n",
        "fitted to its particular lensed images in the `PointDataset`!\n",
        "\n",
        "The model fitting default settings assume that the lens galaxy centre is near the coordinates (0.0\", 0.0\"). \n",
        "\n",
        "If for your dataset the  lens is not centred at (0.0\", 0.0\"), we recommend that you either: \n",
        "\n",
        " - Reduce your data so that the centre is (`autolens_workspace/*/data_preparation`). \n",
        " - Manually override the lens model priors (`autolens_workspace/*/modeling/imaging/customize/priors.py`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "The lens model is fitted to the data using the nested sampling algorithm Nautilus (see `start.here.py` for a \n",
        "full description).\n",
        "\n",
        "The folders: \n",
        "\n",
        " - `autolens_workspace/*/modeling/imaging/searches`.\n",
        " - `autolens_workspace/*/modeling/imaging/customize`\n",
        "  \n",
        "Give overviews of the non-linear searches **PyAutoLens** supports and more details on how to customize the\n",
        "model-fit, including the priors on the model.\n",
        "\n",
        "The `name` and `path_prefix` below specify the path where results ae stored in the output folder:  \n",
        "\n",
        " `/autolens_workspace/output/group/lens_x3__source_x1/mass[sie]_source[point]/unique_identifier`.\n",
        "\n",
        "__Unique Identifier__\n",
        "\n",
        "In the path above, the `unique_identifier` appears as a collection of characters, where this identifier is generated \n",
        "based on the model, search and dataset that are used in the fit.\n",
        "\n",
        "An identical combination of model, search and dataset generates the same identifier, meaning that rerunning the\n",
        "script will use the existing results to resume the model-fit. In contrast, if you change the model, search or dataset,\n",
        "a new unique identifier will be generated, ensuring that the model-fit results are output into a separate folder. \n",
        "\n",
        "__Number Of Cores__\n",
        "\n",
        "We include an input `number_of_cores`, which when above 1 means that Nautilus uses parallel processing to sample multiple \n",
        "lens models at once on your CPU. When `number_of_cores=2` the search will run roughly two times as\n",
        "fast, for `number_of_cores=3` three times as fast, and so on. The downside is more cores on your CPU will be in-use\n",
        "which may hurt the general performance of your computer.\n",
        "\n",
        "You should experiment to figure out the highest value which does not give a noticeable loss in performance of your \n",
        "computer. If you know that your processor is a quad-core processor you should be able to use `number_of_cores=4`. \n",
        "\n",
        "Above `number_of_cores=4` the speed-up from parallelization diminishes greatly. We therefore recommend you do not\n",
        "use a value above this.\n",
        "\n",
        "For users on a Windows Operating system, using `number_of_cores>1` may lead to an error, in which case it should be \n",
        "reduced back to 1 to fix it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.Nautilus(\n",
        "    path_prefix=path.join(\"group\", \"modeling\"),\n",
        "    name=\"start_here\",\n",
        "    unique_tag=dataset_name,\n",
        "    n_live=100,\n",
        "    number_of_cores=1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "The `AnalysisPoint` object defines the `log_likelihood_function` used by the non-linear search to fit the model \n",
        "to the `PointDataset`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = al.AnalysisPoint(dataset=dataset, solver=solver)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Run Times__\n",
        "\n",
        "Lens modeling can be a computationally expensive process. When fitting complex models to high resolution datasets \n",
        "run times can be of order hours, days, weeks or even months.\n",
        "\n",
        "Run times are dictated by two factors:\n",
        "\n",
        " - The log likelihood evaluation time: the time it takes for a single `instance` of the lens model to be fitted to \n",
        "   the dataset such that a log likelihood is returned.\n",
        "\n",
        " - The number of iterations (e.g. log likelihood evaluations) performed by the non-linear search: more complex lens\n",
        "   models require more iterations to converge to a solution.\n",
        "\n",
        "The log likelihood evaluation time can be estimated before a fit using the `profile_log_likelihood_function` method,\n",
        "which returns two dictionaries containing the run-times and information about the fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "run_time_dict, info_dict = analysis.profile_log_likelihood_function(\n",
        "    instance=model.random_instance()\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The overall log likelihood evaluation time is given by the `fit_time` key.\n",
        "\n",
        "For this example, it is ~0.001 seconds, which is extremely fast for lens modeling. The source-plane chi-squared\n",
        "is possibly the fastest way to fit a lens model to a dataset, and therefore whilst it has limitations it is a good\n",
        "way to get a rough estimate of the lens model parameters quickly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Log Likelihood Evaluation Time (second) = {run_time_dict['fit_time']}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To estimate the expected overall run time of the model-fit we multiply the log likelihood evaluation time by an \n",
        "estimate of the number of iterations the non-linear search will perform. \n",
        "\n",
        "Estimating this is tricky, as it depends on the lens model complexity (e.g. number of parameters)\n",
        "and the properties of the dataset and model being fitted.\n",
        "\n",
        "For this example, we conservatively estimate that the non-linear search will perform ~10000 iterations per free \n",
        "parameter in the model. This is an upper limit, with models typically converging in far fewer iterations.\n",
        "\n",
        "If you perform the fit over multiple CPUs, you can divide the run time by the number of cores to get an estimate of\n",
        "the time it will take to fit the model. Parallelization with Nautilus scales well, it speeds up the model-fit by the \n",
        "`number_of_cores` for N < 8 CPUs and roughly `0.5*number_of_cores` for N > 8 CPUs. This scaling continues \n",
        "for N> 50 CPUs, meaning that with super computing facilities you can always achieve fast run times!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    \"Estimated Run Time Upper Limit (seconds) = \",\n",
        "    (run_time_dict[\"fit_time\"] * model.total_free_parameters * 10000)\n",
        "    / search.number_of_cores,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model-Fit__\n",
        "\n",
        "We begin the model-fit by passing the model and analysis object to the non-linear search (checkout the output folder\n",
        "for on-the-fly visualization and results)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "The search returns a result object, which includes: \n",
        "\n",
        " - The lens model corresponding to the maximum log likelihood solution in parameter space.\n",
        " - The corresponding maximum log likelihood `Tracer` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.max_log_likelihood_instance)\n",
        "\n",
        "tracer_plotter = aplt.TracerPlotter(\n",
        "    tracer=result.max_log_likelihood_tracer, grid=result.grid\n",
        ")\n",
        "tracer_plotter.subplot_tracer()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result contains the full posterior information of our non-linear search, including all parameter samples, \n",
        "log likelihood values and tools to compute the errors on the lens model. \n",
        "\n",
        "There are built in visualization tools for plotting this.\n",
        "\n",
        "The plot is labeled with short hand parameter names (e.g. `sersic_index` is mapped to the short hand \n",
        "parameter `n`). These mappings ate specified in the `config/notation.yaml` file and can be customized by users.\n",
        "\n",
        "The superscripts of labels correspond to the name each component was given in the model (e.g. for the `Isothermal`\n",
        "mass its name `mass` defined when making the `Model` above is used)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plotter = aplt.NestPlotter(samples=result.samples)\n",
        "plotter.corner_anesthetic()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checkout `autolens_workspace/*/modeling/results.py` for a full description of the result object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}