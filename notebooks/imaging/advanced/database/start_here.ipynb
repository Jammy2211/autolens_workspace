{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Database: Introduction\n",
        "======================\n",
        "\n",
        "The default behaviour of **PyAutoLens** is for model-fitting results to be output to hard-disc in folders, which are\n",
        "straight forward to navigate and manually check. For small model-fitting tasks this is sufficient, however many users \n",
        "have a need to perform many model fits to large samples of lenses, making manual inspection of results time consuming.\n",
        "\n",
        "PyAutoLens's database feature outputs all model-fitting results as a\n",
        "sqlite3 (https://docs.python.org/3/library/sqlite3.html) relational database, such that all results\n",
        "can be efficiently loaded into a Jupyter notebook or Python script for inspection, analysis and interpretation. This\n",
        "database supports advanced querying, so that specific model-fits (e.g., which fit a certain model or dataset) can be\n",
        "loaded.\n",
        "\n",
        "This script fits a sample of three simulated strong lenses using the same non-linear search. The results will be used\n",
        "to illustrate the database in the database tutorials that follow.\n",
        "\n",
        "The search fits each lens with:\n",
        " \n",
        " - An `Isothermal` `MassProfile` for the lens galaxy's mass.\n",
        " - An `Sersic` `LightProfile` for the source galaxy's light."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import json\n",
        "from os import path\n",
        "import autofit as af\n",
        "import autolens as al"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "For each dataset we load it from hard-disc, set up its `Analysis` class and fit it with a non-linear search. \n",
        "\n",
        "\n",
        "We want each results to be stored in the database with an entry specific to the dataset. We'll use the `Dataset`'s name \n",
        "string to do this, so lets create a list of the 3 dataset names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_names = [\n",
        "    \"simple\",\n",
        "    \"lens_sersic\",\n",
        "    \"mass_power_law\",\n",
        "]\n",
        "\n",
        "pixel_scales = 0.1"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "___Session__\n",
        "\n",
        "To output results directly to the database, we start a session, which includes the name of the database `.sqlite` file\n",
        "where results are stored."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "session = af.db.open_database(\"database.sqlite\")\n",
        "\n",
        "for dataset_name in dataset_names:\n",
        "    \"\"\"\n",
        "    __Paths__\n",
        "\n",
        "    Set up the config and output paths.\n",
        "    \"\"\"\n",
        "    dataset_path = path.join(\"dataset\", \"imaging\", dataset_name)\n",
        "\n",
        "    \"\"\"\n",
        "    __Dataset__\n",
        "    \n",
        "    Using the dataset path, load the data (image, noise-map, PSF) as an `Imaging` object from .fits files.\n",
        "    \n",
        "    This `Imaging` object will be available via the aggregator. Note also that we give the dataset a `name` via the\n",
        "    command `name=dataset_name`. we'll use this name in the aggregator tutorials.\n",
        "    \"\"\"\n",
        "    dataset = al.Imaging.from_fits(\n",
        "        data_path=path.join(dataset_path, \"data.fits\"),\n",
        "        psf_path=path.join(dataset_path, \"psf.fits\"),\n",
        "        noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "        pixel_scales=pixel_scales,\n",
        "    )\n",
        "\n",
        "    \"\"\"\n",
        "    __Mask__\n",
        "    \n",
        "    The `Mask2D` we fit this data-set with, which will be available via the aggregator.\n",
        "\n",
        "    The `SettingsImaging` (which customize the fit of the search`s fit), will also be available to the aggregator! \n",
        "    \"\"\"\n",
        "    mask = al.Mask2D.circular(\n",
        "        shape_native=dataset.shape_native, pixel_scales=dataset.pixel_scales, radius=3.0\n",
        "    )\n",
        "\n",
        "    settings_dataset = al.SettingsImaging(grid_class=al.Grid2D, sub_size=1)\n",
        "\n",
        "    dataset = dataset.apply_mask(mask=mask)\n",
        "    dataset = dataset.apply_settings(settings=settings_dataset)\n",
        "\n",
        "    \"\"\"\n",
        "    __Info__\n",
        "\n",
        "    Information about our model-fit that isn't part of the model-fit can be made accessible to the database, by \n",
        "    passing an `info` dictionary. \n",
        "\n",
        "    Below we load this info dictionary from an `info.json` file stored in each dataset's folder. This dictionary\n",
        "    contains the (hypothetical) lens redshift, source redshift and lens velocity dispersion of every lens in our sample.\n",
        "    \"\"\"\n",
        "    with open(path.join(dataset_path, \"info.json\")) as json_file:\n",
        "        info = json.load(json_file)\n",
        "\n",
        "    \"\"\"\n",
        "    __Pickle Files__\n",
        "\n",
        "    We can pass strings specifying the path and filename of .pickle files stored on our hard-drive to the `search.fit()`\n",
        "    method, which will make them accessible to the aggregator to aid interpretation of results. Our simulated strong\n",
        "    lens datasets have a `true_tracer.pickle` file which we pass in below, which we use in the `Aggregator` tutorials \n",
        "    to check if the model-fit recovers its true input parameters.\n",
        "    \"\"\"\n",
        "    pickle_files = [path.join(dataset_path, \"true_tracer.pickle\")]\n",
        "\n",
        "    \"\"\"\n",
        "    __Model__\n",
        "    \n",
        "    Set up the model as per usual, and will see in tutorial 3 why we have included `disk=None`.\n",
        "    \"\"\"\n",
        "    model = af.Collection(\n",
        "        galaxies=af.Collection(\n",
        "            lens=af.Model(al.Galaxy, redshift=0.5, mass=al.mp.Isothermal),\n",
        "            source=af.Model(al.Galaxy, redshift=1.0, bulge=al.lp.Sersic, disk=None),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    \"\"\"\n",
        "    In all examples so far, results were written to the `autofit_workspace/output` folder with a path and folder \n",
        "    named after a unique identifier, which was derived from the non-linear search and model. This unique identifier\n",
        "    plays a vital role in the database: it is used to ensure every entry in the database is unique. \n",
        "\n",
        "    In this example, results are written directly to the `database.sqlite` file after the model-fit is complete and \n",
        "    only stored in the output folder during the model-fit. This can be important for performing large model-fitting \n",
        "    tasks on high performance computing facilities where there may be limits on the number of files allowed, or there\n",
        "    are too many results to make navigating the output folder manually feasible.\n",
        "\n",
        "    The `unique_tag` below uses the `dataset_name` to alter the unique identifier, which as we have seen is also \n",
        "    generated depending on the search settings and model. In this example, all three model fits use an identical \n",
        "    search and model, so this `unique_tag` is key for ensuring 3 separate sets of results for each model-fit are \n",
        "    stored in the output folder and written to the .sqlite database. \n",
        "    \"\"\"\n",
        "    search = af.DynestyStatic(\n",
        "        path_prefix=path.join(\"database\"),\n",
        "        name=\"database_example\",\n",
        "        unique_tag=dataset_name,  # This makes the unique identifier use the dataset name\n",
        "        session=session,  # This instructs the search to write to the .sqlite database.\n",
        "        nlive=50,\n",
        "    )\n",
        "\n",
        "    analysis = al.AnalysisImaging(dataset=dataset)\n",
        "\n",
        "    search.fit(analysis=analysis, model=model, info=info, pickle_files=pickle_files)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you inspect the `autolens_workspace/output/database` folder during the model-fit, you'll see that the results\n",
        "are only stored there during the model fit, and they are written to the database and removed once complete. \n",
        "\n",
        "__Loading Results__\n",
        "\n",
        "After fitting a large suite of data, we can use the aggregator to load the database's results. We can then\n",
        "manipulate, interpret and visualize them using a Python script or Jupyter notebook.\n",
        "\n",
        "The results are not contained in the `output` folder after each search completes. Instead, they are\n",
        "contained in the `database.sqlite` file, which we can load using the `Aggregator`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "database_file = \"database.sqlite\"\n",
        "agg = af.Aggregator.from_database(filename=database_file)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Generators__\n",
        "\n",
        "Before using the aggregator to inspect results, let me quickly cover Python generators. A generator is an object that \n",
        "iterates over a function when it is called. The aggregator creates all of the objects that it loads from the database \n",
        "as generators (as opposed to a list, or dictionary, or other Python type).\n",
        "\n",
        "Why? Because lists and dictionaries store every entry in memory simultaneously. If you fit many datasets, this will use \n",
        "a lot of memory and crash your laptop! On the other hand, a generator only stores the object in memory when it is used; \n",
        "Python is then free to overwrite it afterwards. Thus, your laptop won't crash!\n",
        "\n",
        "There are two things to bare in mind with generators:\n",
        "\n",
        " 1) A generator has no length and to determine how many entries it contains you first must turn it into a list.\n",
        "\n",
        " 2) Once we use a generator, we cannot use it again and need to remake it. For this reason, we typically avoid \n",
        " storing the generator as a variable and instead use the aggregator to create them on use.\n",
        "\n",
        "We can now create a `samples` generator of every fit. The `results` example scripts show how  \n",
        "the `Samples` class acts as an interface to the results of the non-linear search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples_gen = agg.values(\"samples\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we print this the length of this generator converted to a list of outputs we see 3 different `SamplesDynesty`\n",
        "instances. \n",
        "\n",
        "These correspond to each fit of each search to each of our 3 images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"NestedSampler Samples: \\n\")\n",
        "print(samples_gen)\n",
        "print()\n",
        "print(\"Total Samples Objects = \", len(agg), \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Therefore, by loading the `Samples` via the database we can now access the results of the fit to each dataset.\n",
        "\n",
        "For example, we can plot the maximum likelihood model for each of the 3 model-fits performed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ml_vector = [\n",
        "    samps.max_log_likelihood(as_instance=False) for samps in agg.values(\"samples\")\n",
        "]\n",
        "\n",
        "print(\"Max Log Likelihood Model Parameter Lists: \\n\")\n",
        "print(ml_vector, \"\\n\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Building a Database File From an Output Folder__\n",
        "\n",
        "The fits above directly wrote the results to the .sqlite file, which we loaded above. However, you may have results\n",
        "already written to hard-disk in an output folder, which you wish to build your .sqlite file from.\n",
        "\n",
        "This can be done via the following code, which is commented out below to avoid us deleting the existing .sqlite file.\n",
        "\n",
        "Below, the `database_name` corresponds to the name of your output folder and is also the name of the `.sqlite` file\n",
        "that is created.\n",
        "\n",
        "If you are fitting a relatively small number of datasets (e.g. 10-100) having all results written\n",
        "to hard-disk (e.g. for quick visual inspection) but using the database for sample-wide analysis may be benefitial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# database_name = \"database\"\n",
        "\n",
        "# agg = af.Aggregator.from_database(\n",
        "#    filename=f\"{database_name}.sqlite\", completed_only=False\n",
        "# )\n",
        "\n",
        "# agg.add_directory(directory=path.join(\"output\", database_name)))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Wrap Up__\n",
        "\n",
        "This example illustrates how to use the database.\n",
        "\n",
        "The `database/examples` folder contains examples illustrating the following:\n",
        "\n",
        "- ``samples.py``: Loads the non-linear search results from the SQLite3 database and inspect the \n",
        "   samples (e.g. parameter estimates, posterior).\n",
        "   \n",
        "- ``queries.py``: Query the database to get certain  modeling results (e.g. all lens models where `\n",
        "   einstein_radius > 1.0`).\n",
        "\n",
        "- ``models.py``: Inspect the models in the database (e.g. visualize their deflection angles).\n",
        "\n",
        "- ``data_fitting.py``: Inspect the data-fitting results in the database (e.g. visualize the residuals)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}