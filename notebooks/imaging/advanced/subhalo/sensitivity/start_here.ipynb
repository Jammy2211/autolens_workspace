{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Feature: Sensitivity Mapping\n",
        "============================\n",
        "\n",
        "Bayesian model comparison allows us to take a dataset, fit it with multiple models and use the Bayesian evidence to\n",
        "quantify which model objectively gives the best-fit following the principles of Occam's Razor.\n",
        "\n",
        "However, a complex model may not be favoured by model comparison not because it is the 'wrong' model, but simply\n",
        "because the dataset being fitted is not of a sufficient quality for the more complex model to be favoured. Sensitivity\n",
        "mapping allows us to address what quality of data would be needed for the more complex model to be favoured or\n",
        "alternatively for what sets of model parameter values it would be favoured for data of a given quality.\n",
        "\n",
        "In order to do this, sensitivity mapping involves us writing a function that uses the model(s) to simulate a dataset.\n",
        "We then use this function to simulate many datasets, for many different models, and fit each dataset using the same\n",
        "model-fitting procedure we used to perform Bayesian model comparison. This allows us to infer how much of a Bayesian\n",
        "evidence increase we should expect for datasets of varying quality and / or models with different parameters.\n",
        "\n",
        "For strong lensing, this process is crucial for dark matter substructure detection, as discussed in the following paper:\n",
        "\n",
        "https://arxiv.org/abs/0903.4752\n",
        "\n",
        "In substructure detection, we scan a strong lens dark matter subhalos by fitting a lens models which include a subhalo.\n",
        "This tells us whether we successfully did detect a subhalo, but it does not tell us where a subhalo has to be located\n",
        "(in relation to the source light) to be detectable, nor does to what masses of subhalo we could actually have made a\n",
        "detection.\n",
        "\n",
        "To answer these questions, we must perform sensitivity mapping, where we simulate many thousands of strong lens images,\n",
        "each of which include a dark matter subhalo at a given (y,x) coordinate at a given mass. We then fit each dataset twice,\n",
        "once with a lens model which does not include a subhalo and once with a lens model that does. If the Bayesian evidence\n",
        "of the model which includes a subhalo is higher than that which does not, then it means a subhalo was detectable!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import autofit as af\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset + Masking__ \n",
        "\n",
        "Load, plot and mask the `Imaging` data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"dark_matter_subhalo\"\n",
        "dataset_path = path.join(\"dataset\", \"imaging\", dataset_name)\n",
        "\n",
        "dataset = al.Imaging.from_fits(\n",
        "    data_path=path.join(dataset_path, \"data.fits\"),\n",
        "    noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "    psf_path=path.join(dataset_path, \"psf.fits\"),\n",
        "    pixel_scales=0.05,\n",
        ")\n",
        "\n",
        "mask = al.Mask2D.circular(\n",
        "    shape_native=dataset.shape_native, pixel_scales=dataset.pixel_scales, radius=3.0\n",
        ")\n",
        "\n",
        "dataset = dataset.apply_mask(mask=mask)\n",
        "\n",
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model + Search + Analysis + Model-Fit (Base Search)__\n",
        "\n",
        "We are going to perform sensitivity mapping to determine where a subhalo is detectable. This will require us to simulate \n",
        "many realizations of our dataset with a lens model, called the `simulation_instance`. To get this model, we therefore \n",
        "fit the data before performing sensitivity mapping so that we can set the `simulation_instance` as the maximum \n",
        "likelihood model.\n",
        "\n",
        "We perform this fit using the lens model we will use to perform sensitivity mapping, which we call the `base_model`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "base_model = af.Collection(\n",
        "    galaxies=af.Collection(\n",
        "        lens=af.Model(al.Galaxy, redshift=0.5, mass=al.mp.Isothermal),\n",
        "        source=af.Model(al.Galaxy, redshift=1.0, bulge=al.lp.Sersic),\n",
        "    )\n",
        ")\n",
        "\n",
        "search_base = af.Nautilus(\n",
        "    path_prefix=path.join(\"imaging\", \"misc\"),\n",
        "    name=\"sensitivity_mapping_base\",\n",
        "    unique_tag=dataset_name,\n",
        "    n_live=100,\n",
        ")\n",
        "\n",
        "analysis = al.AnalysisImaging(dataset=dataset)\n",
        "\n",
        "result = search_base.fit(model=base_model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Sensitivty Mapping__\n",
        "\n",
        "We now define the `base_model` that we use to perform sensitivity mapping. This is the lens model that is fitted to \n",
        "every simulated strong lens without a subhalo, giving us the Bayesian evidence which we compare to the model which \n",
        "includes one!). \n",
        "\n",
        "For this model, we can use the `base_model` above, however we will use the result of fitting this model to the dataset\n",
        "before sensitivity mapping. This ensures the priors associated with each parameter are initialized so as to speed up\n",
        "each non-linear search performed during sensitivity mapping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "base_model = result.model"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now define the `perturbation_model`, which is the model component whose parameters we iterate over to perform \n",
        "sensitivity mapping. In this case, this model is a `NFWMCRLudlowSph` model and we will iterate over its\n",
        "`centre` and `mass_at_200`. We set it up as a `Model` so it has an associated redshift and can be directly\n",
        "passed to the tracer in the simulate function below.\n",
        "\n",
        "Many instances of the `perturbation_model` are created and used to simulate the many strong lens datasets that we fit. \n",
        "However, it is only included in half of the model-fits; corresponding to the lens models which include a dark matter \n",
        "subhalo and whose Bayesian evidence we compare to the simpler model-fits consisting of just the `base_model` to \n",
        "determine if the subhalo was detectable.\n",
        "\n",
        "By fitting both models to every simulated lens, we therefore infer the Bayesian evidence of every model to every \n",
        "dataset. Sensitivity mapping therefore maps out for what values of `centre` and `mass_at_200` in the dark matter \n",
        "subhalo the model-fit including a subhalo provide higher values of Bayesian evidence than the simpler model-fit (and\n",
        "therefore when it is detectable!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "perturbation_model = af.Model(al.Galaxy, redshift=0.5, mass=al.mp.NFWMCRLudlowSph)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sensitivity mapping is typically performed over a large range of parameters. However, to make this demonstration quick\n",
        "and clear we are going to fix the `centre` of the subhalo to a value near the Einstein ring of (1.6, 0.0). We will \n",
        "iterate over just two `mass_at_200` values corresponding to subhalos of mass 1e6 and 1e13, of which only the latter\n",
        "will be shown to be detectable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "perturbation_model.mass.centre.centre_0 = 1.6\n",
        "perturbation_model.mass.centre.centre_1 = 0.0\n",
        "perturbation_model.mass.redshift_object = 0.5\n",
        "perturbation_model.mass.redshift_source = 1.0\n",
        "perturbation_model.mass.mass_at_200 = af.LogUniformPrior(\n",
        "    lower_limit=1e6, upper_limit=1e13\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to perform sensitivity mapping to determine where a subhalo is detectable. This will require us to \n",
        "simulate many realizations of our dataset with a lens model, called the `simulation_instance`. This model uses the\n",
        "result of the fit above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "simulation_instance = result.instance"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now write the `simulate_function`, which takes the `simulation_instance` of our model (defined above) and uses it to \n",
        "simulate a dataset which is subsequently fitted.\n",
        "\n",
        "Note that when this dataset is simulated, the quantity `instance.perturbation` is used in the `simulate_function`.\n",
        "This is an instance of the `NFWMCRLudlowSph`, and it is different every time the `simulate_function` is called\n",
        "based on the value of sensitivity being computed. \n",
        "\n",
        "In this example, this `instance.perturbation` corresponds to two different subhalos with values of `mass_at_200` of \n",
        "1e6 MSun and 1e13 MSun."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "def simulate_function(instance, simulate_path):\n",
        "    \"\"\"\n",
        "    Set up the `Tracer` which is used to simulate the strong lens imaging, which may include the subhalo in\n",
        "    addition to the lens and source galaxy.\n",
        "    \"\"\"\n",
        "    tracer = al.Tracer.from_galaxies(\n",
        "        galaxies=[\n",
        "            instance.galaxies.lens,\n",
        "            instance.perturbation,\n",
        "            instance.galaxies.source,\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    \"\"\"\n",
        "    Set up the grid, PSF and simulator settings used to simulate imaging of the strong lens. These should be tuned to\n",
        "    match the S/N and noise properties of the observed data you are performing sensitivity mapping on.\n",
        "    \"\"\"\n",
        "    grid = al.Grid2DIterate.uniform(\n",
        "        shape_native=mask.shape_native,\n",
        "        pixel_scales=mask.pixel_scales,\n",
        "        fractional_accuracy=0.9999,\n",
        "        sub_steps=[2, 4, 8, 16, 24],\n",
        "    )\n",
        "\n",
        "    simulator = al.SimulatorImaging(\n",
        "        exposure_time=300.0,\n",
        "        psf=dataset.psf,\n",
        "        background_sky_level=0.1,\n",
        "        add_poisson_noise=True,\n",
        "    )\n",
        "\n",
        "    simulated_dataset = simulator.via_tracer_from(tracer=tracer, grid=grid)\n",
        "\n",
        "    \"\"\"\n",
        "    The data generated by the simulate function is that which is fitted, so we should apply the mask for the analysis \n",
        "    here before we return the simulated data.\n",
        "    \"\"\"\n",
        "    return simulated_dataset.apply_mask(mask=mask)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each model-fit performed by sensitivity mapping creates a new instance of an `Analysis` class, which contains the\n",
        "data simulated by the `simulate_function` for that model.\n",
        "\n",
        "This requires us to write a wrapper around the PyAutoLens `Analysis` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class AnalysisImagingSensitivity(al.AnalysisImaging):\n",
        "    def __init__(self, dataset):\n",
        "        super().__init__(dataset=dataset)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We next specify the search used to perform each model fit by the sensitivity mapper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.Nautilus(\n",
        "    path_prefix=path.join(\"imaging\", \"misc\"),\n",
        "    name=\"sensitivity_mapping\",\n",
        "    unique_tag=dataset_name,\n",
        "    n_live=100,\n",
        "    force_x1_cpu=True,  # ensures parallelizing over grid search works.\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now combine all of the objects created above and perform sensitivity mapping. The inputs to the `Sensitivity`\n",
        "object below are:\n",
        "\n",
        "- `simulation_instance`: This is an instance of the model used to simulate every dataset that is fitted. In this example \n",
        "it is a lens model that does not include a subhalo, which was inferred by fitting the dataset we perform sensitivity \n",
        "mapping on.\n",
        "\n",
        "- `base_model`: This is the lens model that is fitted to every simulated dataset, which does not include a subhalo. In \n",
        "this example is composed of an `Isothermal` lens and `Sersic` source.\n",
        "\n",
        "- `perturbation_model`: This is the extra model component that alongside the `base_model` is fitted to every simulated \n",
        "dataset. In this example it is a `NFWMCRLudlowSph` dark matter subhalo.\n",
        "\n",
        "- `simulate_function`: This is the function that uses the `simulation_instance` and many instances of the `perturbation_model` \n",
        "to simulate many datasets that are fitted with the `base_model` and `base_model` + `perturbation_model`.\n",
        "\n",
        "- `analysis_class`: The wrapper `Analysis` class that passes each simulated dataset to the `Analysis` class that fits \n",
        "the data.\n",
        "\n",
        "- `number_of_steps`: The number of steps over which the parameters in the `perturbation_model` are iterated. In this \n",
        "example, `mass_at_200` has a `LogUniformPrior` with lower limit 1e6 and upper limit 1e13, therefore \n",
        "the `number_of_steps` of 2 will simulate and fit just 2 datasets where the `mass_at_200` is between 1e6 and 1e13.\n",
        "\n",
        "- `number_of_cores`: The number of cores over which the sensitivity mapping is performed, enabling parallel processing\n",
        "if set above 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from autofit.non_linear.grid import sensitivity as s\n",
        "\n",
        "sensitivity = s.Sensitivity(\n",
        "    search=search,\n",
        "    simulation_instance=simulation_instance,\n",
        "    base_model=base_model,\n",
        "    perturbation_model=perturbation_model,\n",
        "    simulate_function=simulate_function,\n",
        "    analysis_class=AnalysisImagingSensitivity,\n",
        "    number_of_cores=2,\n",
        "    number_of_steps=2,\n",
        ")\n",
        "\n",
        "sensitivity_result = sensitivity.run()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should now look at the results of the sensitivity mapping in the folder `output/features/sensitivity_mapping`. \n",
        "\n",
        "You will note the following 4 model-fits have been performed:\n",
        "\n",
        " - The `base_model` is fitted to a simulated dataset where a subhalo with `mass_at_200=1e6` is included.\n",
        "\n",
        " - The `base_model` + `perturbation_model` is fitted to a simulated dataset where a subhalo with `mass_at_200=1e6` \n",
        " is included.\n",
        "\n",
        " - The `base_model` is fitted to a simulated dataset where a subhalo with `mass_at_200=1e13` is included.\n",
        "\n",
        " - The `base_model` + `perturbation_model` is fitted to a simulated dataset where a subhalo with `mass_at_200=1e13` is \n",
        " included.\n",
        "\n",
        "The fit produces a `sensitivity_result`. \n",
        "\n",
        "We are still developing the `SensitivityResult` class to provide a data structure that better streamlines the analysis\n",
        "of results. If you intend to use sensitivity mapping, the best way to interpret the resutls is currently via\n",
        "**PyAutoFit**'s database and `Aggregator` tools. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(sensitivity_result.results[0].result.samples.log_evidence)\n",
        "print(sensitivity_result.results[1].result.samples.log_evidence)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}