{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sensitivity Mapping: Start Here\n",
        "===============================\n",
        "\n",
        "Bayesian model comparison allows us to take a dataset, fit it with multiple models and use the Bayesian evidence to\n",
        "quantify which model objectively gives the best-fit following the principles of Occam's Razor.\n",
        "\n",
        "However, a complex model may not be favoured by model comparison not because it is the 'wrong' model, but simply\n",
        "because the dataset being fitted is not of a sufficient quality for the more complex model to be favoured. Sensitivity\n",
        "mapping addresses what quality of data would be needed for the more complex model to be favoured.\n",
        "\n",
        "In order to do this, sensitivity mapping involves us writing a function that uses the model(s) to simulate a dataset.\n",
        "We then use this function to simulate many datasets, for many different models, and fit each dataset to quantify\n",
        "how much the change in the model led to a measurable change in the data. This is called computing the sensitivity.\n",
        "\n",
        "How we compute the sensitivity is chosen by us, the user. In this example, we will perform multiple model-fits\n",
        "with a nested sampling search, and therefore perform Bayesian model comparison to compute the sensitivity. This allows\n",
        "us to infer how much of a Bayesian evidence increase we should expect for datasets of varying quality and / or models\n",
        "with different parameters.\n",
        "\n",
        "__Subhalo Detection Discussion__\n",
        "\n",
        "For strong lensing, this process is crucial for dark matter substructure detection, as discussed in the following paper:\n",
        "\n",
        "https://arxiv.org/abs/0903.4752\n",
        "\n",
        "In subhalo detection, our strong lens modeling informs us of whether there is a dark matter subhalo at a given (y,x)\n",
        "image-plane location of the strong lens. We determine this by fitting a lens models which includes a subhalo. However,\n",
        "we are only able to detect dark matter subhalos with (y,x) locations near the lensed source light, and when the\n",
        "subhalo is massive enough to perturb its light in an observable way.\n",
        "\n",
        "Subhalo detection analysis therefore does not tell us where we could detect subhalos and of what mass. To know this,\n",
        "we must perform sensitivity mapping.\n",
        "\n",
        "__Subhalo Sensitivity Mapping__\n",
        "\n",
        "Sensitivity mapping is a process where we simulate many thousands of strong lens images. Each simulated image includes a\n",
        "dark matter subhalo at a given (y,x) coordinate and at a given mass. We fit each simulated dataset twice,\n",
        "with a lens model which does not include a subhalo and with a lens model that does.\n",
        "\n",
        "If the Bayesian evidence of the lens model including a subhalo is higher than the model which does not, a subhalo at t\n",
        "hat (y,x) location and mass is therefore detectable.\n",
        "\n",
        "For many simulated datasets, we will find the evidence does not increase when we include a subhalo in the model-fit,\n",
        "informing us that regions of the image-plane away from the lensed source are not sensitive to subhalos.\n",
        "\n",
        "The sensitivity map is performed over a three dimensional (or higher) grid of subhalo (y,x) and mass. Thus, once\n",
        "sensitivity mapping is complete, we have a complete map of where in the image-plane subhalos of what mass are\n",
        "detectable. We can plot 2D plots of this grid to visualize where we are sensitive to dark matter subhalos.\n",
        "\n",
        "The information provided by a sensitivity map is ultimately required to turn dark matter subhalo detections into\n",
        "constraints on the dark matter subhalo mass function, which is the primary goal of subhalo detection. Thus, it is\n",
        "necessary to make statements about the nature of dark matter: cold, warm, fuzzy, or something else entirely?\n",
        "\n",
        "__SLaM Pipelines__\n",
        "\n",
        "The Source, (lens) Light and Mass (SLaM) pipelines are advanced lens modeling pipelines which automate the fitting\n",
        "of complex lens models. The SLaM pipelines are used for all DM subhalo detection analyses in **PyAutoLens**.\n",
        "\n",
        "This example script does not use a SLaM pipeline, to keep the sensitivity mapping self contained. However, it is\n",
        "anticipated that any user performing sensitivity mapping on real data will use the SLaM pipelines, which in the\n",
        "`subhalo` package have dedicated extensions for performing sensitivity mapping to both imaging and interferometer data.\n",
        "\n",
        "Therefore you should be familiar with the SLaM pipelines before performing DM subhalo sensitivity mapping on real\n",
        "data. If you are unfamiliarwith the SLaM pipelines, checkout the\n",
        "example `autolens_workspace/notebooks/imaging/advanced/chaining/slam/start_here.ipynb`.\n",
        "\n",
        "__Pixelized Source__\n",
        "\n",
        "Detecting a DM subhalo requires the lens model to be sufficiently accurate that the residuals of the source's light\n",
        "are at a level where the subhalo's perturbing lensing effects can be detected.\n",
        "\n",
        "This requires the source reconstruction to be performed using a pixelized source, as this provides a more detailed\n",
        "reconstruction of the source's light than fits using light profiles.\n",
        "\n",
        "Therefore, the corresponding sensitivity mapping should also be performed using pixelized sources. This example\n",
        "sticks to light profile sources, to provide faster run times illustrative purposes.\n",
        "\n",
        "The example `subhalo/sensitivity/examples/source_pixelized.ipynb` extends the SLaM pipelines with pixelized sources\n",
        "and therefore shows how to perform sensitivity mapping using pixelized sources.\n",
        "\n",
        "Note that the simulation procedure for a pixelized source is different to the one shown here. In this example, the\n",
        "light profile source parameters are used to simulate each sensitivity mapping dataset. When pixelized sources are\n",
        "used, the source reconstruction on the mesh is used, such that the simulations capture the irregular morphologies\n",
        "of real source galaxies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import os\n",
        "from os import path\n",
        "import autofit as af\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset + Masking__ \n",
        "\n",
        "Load, plot and mask the `Imaging` data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"dark_matter_subhalo\"\n",
        "dataset_path = path.join(\"dataset\", \"imaging\", dataset_name)\n",
        "\n",
        "dataset = al.Imaging.from_fits(\n",
        "    data_path=path.join(dataset_path, \"data.fits\"),\n",
        "    noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "    psf_path=path.join(dataset_path, \"psf.fits\"),\n",
        "    pixel_scales=0.05,\n",
        ")\n",
        "\n",
        "mask = al.Mask2D.circular(\n",
        "    shape_native=dataset.shape_native, pixel_scales=dataset.pixel_scales, radius=3.0\n",
        ")\n",
        "\n",
        "dataset = dataset.apply_mask(mask=mask)\n",
        "\n",
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model + Search + Analysis + Model-Fit (Base Search)__\n",
        "\n",
        "We are performing sensitivity mapping to determine where a subhalo is detectable. This will require us to simulate \n",
        "many realizations of our dataset with a lens model, called the `simulation_instance`. To get this model, we therefore \n",
        "fit the data before performing sensitivity mapping so that we can set the `simulation_instance` as the maximum \n",
        "likelihood model.\n",
        "\n",
        "We perform this fit using the lens model we will use to perform sensitivity mapping, which we call the `base_model`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "base_model = af.Collection(\n",
        "    galaxies=af.Collection(\n",
        "        lens=af.Model(al.Galaxy, redshift=0.5, mass=al.mp.Isothermal),\n",
        "        source=af.Model(al.Galaxy, redshift=1.0, bulge=al.lp.SersicCore),\n",
        "    ),\n",
        ")\n",
        "\n",
        "search_base = af.Nautilus(\n",
        "    path_prefix=path.join(\"imaging\", \"advanced\", \"subhalo\", \"sensitivity\"),\n",
        "    name=\"sensitivity_mapping_base\",\n",
        "    unique_tag=dataset_name,\n",
        "    n_live=100,\n",
        ")\n",
        "\n",
        "analysis = al.AnalysisImaging(dataset=dataset)\n",
        "\n",
        "result = search_base.fit(model=base_model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Base Model__\n",
        "\n",
        "We now define the `base_model` that we use to perform sensitivity mapping. This is the lens model that is fitted to \n",
        "every simulated strong lens without a subhalo, giving us the Bayesian evidence which we compare to the model which \n",
        "includes one!). \n",
        "\n",
        "For this model, we can use the `base_model` above, however we will use the result of fitting this model to the dataset\n",
        "before sensitivity mapping. This ensures the priors associated with each parameter are initialized so as to speed up\n",
        "each non-linear search performed during sensitivity mapping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "base_model = result.model"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Perturb Model__\n",
        "\n",
        "We now define the `perturb_model`, which is the model component whose parameters we iterate over to perform \n",
        "sensitivity mapping. In this case, this model is a `NFWMCRLudlowSph` model and we will iterate over its\n",
        "`centre` and `mass_at_200`. We set it up as a `Model` so it has an associated redshift and can be directly\n",
        "passed to the tracer in the simulate function below.\n",
        "\n",
        "Many instances of the `perturb_model` are created and used to simulate the many strong lens datasets that we fit. \n",
        "However, it is only included in half of the model-fits; corresponding to the lens models which include a dark matter \n",
        "subhalo and whose Bayesian evidence we compare to the simpler model-fits consisting of just the `base_model` to \n",
        "determine if the subhalo was detectable.\n",
        "\n",
        "By fitting both models to every simulated lens, we therefore infer the Bayesian evidence of every model to every \n",
        "dataset. Sensitivity mapping therefore maps out for what values of `centre` and `mass_at_200` in the dark matter \n",
        "subhalo the model-fit including a subhalo provide higher values of Bayesian evidence than the simpler model-fit (and\n",
        "therefore when it is detectable!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "perturb_model = af.Model(al.Galaxy, redshift=0.5, mass=al.mp.NFWMCRLudlowSph)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Mapping Grid__\n",
        "\n",
        "Sensitivity mapping is typically performed over a large range of parameters. However, to make this demonstration quick\n",
        "and clear we are going to fix the `centre` of the subhalo to a value near the Einstein ring of (1.6, 0.0). We will \n",
        "iterate over just two `mass_at_200` values corresponding to subhalos of mass 1e6 and 1e13, of which only the latter\n",
        "will be shown to be detectable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grid_dimension_arcsec = 3.0\n",
        "\n",
        "perturb_model.mass.mass_at_200 = 1e10\n",
        "perturb_model.mass.centre.centre_0 = af.UniformPrior(\n",
        "    lower_limit=-grid_dimension_arcsec, upper_limit=grid_dimension_arcsec\n",
        ")\n",
        "perturb_model.mass.centre.centre_1 = af.UniformPrior(\n",
        "    lower_limit=-grid_dimension_arcsec, upper_limit=grid_dimension_arcsec\n",
        ")\n",
        "perturb_model.mass.redshift_object = 0.5\n",
        "perturb_model.mass.redshift_source = 1.0"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Perturb Model Prior Func__\n",
        "\n",
        "The default priors on the `perturb_model` are `UniformPrior`'s bounded around each sensitivity grid cell.\n",
        "\n",
        "For example, the first simulated dark matter subhalo is at location (1.5, -1.5) and its priors are:  \n",
        "\n",
        "- y is `UniformPrior(lower_limit=0.0, upper_limit=3.0)`.\n",
        "- x is `UniformPrior(lower_limit=-3.0, upper_limit=0.0)`.\n",
        "\n",
        "The `mass_at_200` is fixed to a value of 1e10 in the `perturb_model` above, which is the fixed value used by\n",
        "the model fit.\n",
        "\n",
        "By passing a `perturb_model_prior_func` to the sensitivity mapper, we can manually overwrite the priors on \n",
        "the `perturb_model` which are used instead for the fit.\n",
        "\n",
        "Below, we update the priors as follows:\n",
        "\n",
        "- The y and x priors are trimmed to much narrower bounded priors, confined to regions 0.05\" each side of the \n",
        "  true DM subhalo.\n",
        "\n",
        "- The `mass_at_200` is made a free parameter with `LogUniformPrior(lower_limit=1e6, 1e12)`. This is a large range, \n",
        "  but ensures there are solutions where the DM subhalo can go to lower masses.    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "def perturb_model_prior_func(perturb_instance, perturb_model):\n",
        "    b = 0.05\n",
        "\n",
        "    perturb_model.mass.centre.centre_0 = af.UniformPrior(\n",
        "        lower_limit=perturb_instance.mass.centre[0] - b,\n",
        "        upper_limit=perturb_instance.mass.centre[0] + b,\n",
        "    )\n",
        "\n",
        "    perturb_model.mass.centre.centre_1 = af.UniformPrior(\n",
        "        lower_limit=perturb_instance.mass.centre[1] - b,\n",
        "        upper_limit=perturb_instance.mass.centre[1] + b,\n",
        "    )\n",
        "\n",
        "    perturb_model.mass.mass_at_200 = af.LogUniformPrior(\n",
        "        lower_limit=1e6, upper_limit=1e12\n",
        "    )\n",
        "\n",
        "    return perturb_model\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Simulation Instance__\n",
        "\n",
        "We are performing sensitivity mapping to determine where a subhalo is detectable. This will require us to \n",
        "simulate many realizations of our dataset with a lens model, called the `simulation_instance`. This model uses the\n",
        "result of the fit above.\n",
        "\n",
        "The code below ensures that the lens light, mass and source parameters of the strong lens are used when simulating\n",
        "each dataset with a dark matter subhalo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "simulation_instance = result.instance"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Simulate Function Class__\n",
        "\n",
        "We now write the `simulate_cls`, which takes the `simulation_instance` of our model (defined above) and uses it to \n",
        "simulate a strong lens dataset, which include a dark matter subhalo, which is subsequently fitted.\n",
        "\n",
        "Additional attributes required to simulate the data (mask, PSF) can be passed to the `__init__` method, and the \n",
        "simulation is  performed in the `__call__` method.\n",
        "\n",
        "When this dataset is simulated, the quantity `instance.perturb` is used in `__call__`. This is an instance \n",
        "of the `NFWMCRLudlowSph`, and it is different every time the `simulate_cls` is called based on the value of sensitivity \n",
        "being computed. \n",
        "\n",
        "In this example, this `instance.perturb` corresponds to two different subhalos with values of `mass_at_200` of \n",
        "1e6 MSun and 1e13 MSun."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class SimulateImaging:\n",
        "    def __init__(self, mask, psf):\n",
        "        \"\"\"\n",
        "        Class used to simulate the strong lens imaging used for sensitivity mapping.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        mask\n",
        "            The mask applied to the real image data, which is applied to every simulated imaging.\n",
        "        psf\n",
        "           The PSF of the real image data, which is applied to every simulated imaging and used for each fit.\n",
        "        \"\"\"\n",
        "        self.mask = mask\n",
        "        self.psf = psf\n",
        "\n",
        "    def __call__(self, instance: af.ModelInstance, simulate_path: str):\n",
        "        \"\"\"\n",
        "        The `simulate_function` called by the `Sensitivity` class which simulates each strong lens image fitted\n",
        "        by the sensitivity mapper.\n",
        "\n",
        "        The simulation procedure is as follows:\n",
        "\n",
        "        1) Use the input galaxies of the sensitivity `instance` to set up a tracer, which generates the image-plane\n",
        "           image of the strong lens system.\n",
        "\n",
        "        2) Simulate this image using the input dataset noise (Poisson) and PSF.\n",
        "\n",
        "        3) Apply the mask used in the analysis of the real image to the simulated image.\n",
        "\n",
        "        4) Output information about the simulation to hard-disk.\n",
        "\n",
        "        The `subhalo` in the sensitivity `instance` changes for every iteration of the sensitivity mapping, ensuring\n",
        "        that we map out the sensitivity of the analysis to the subhalo properties (centre, mass, etc.).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        instance\n",
        "            The sensitivity instance, which includes the galaxies whose parameters are varied to perform sensitivity.\n",
        "            The subhalo in this instance changes for every iteration of the sensitivity mapping.\n",
        "        simulate_path\n",
        "            The path where the simulated dataset is output, contained within each sub-folder of the sensitivity\n",
        "            mapping.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        A simulated image of a strong lens, which id input into the fits of the sensitivity mapper.\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        Set up the `Tracer` which is used to simulate the strong lens imaging, which may include the subhalo in\n",
        "        addition to the lens and source galaxy.\n",
        "        \"\"\"\n",
        "        tracer = al.Tracer(\n",
        "            galaxies=[\n",
        "                instance.galaxies.lens,\n",
        "                instance.perturb,\n",
        "                instance.galaxies.source,\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        Set up the grid, PSF and simulator settings used to simulate imaging of the strong lens. These should be tuned to\n",
        "        match the S/N and noise properties of the observed data you are performing sensitivity mapping on.\n",
        "        \"\"\"\n",
        "        grid = al.Grid2D.uniform(\n",
        "            shape_native=self.mask.shape_native,\n",
        "            pixel_scales=self.mask.pixel_scales,\n",
        "            over_sampling=al.OverSamplingIterate(\n",
        "                fractional_accuracy=0.9999, sub_steps=[2, 4, 8, 16]\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        simulator = al.SimulatorImaging(\n",
        "            exposure_time=300.0,\n",
        "            psf=self.psf,\n",
        "            background_sky_level=0.1,\n",
        "            add_poisson_noise=True,\n",
        "        )\n",
        "\n",
        "        dataset = simulator.via_tracer_from(tracer=tracer, grid=grid)\n",
        "\n",
        "        \"\"\"\n",
        "        The data generated by the simulate function is that which is fitted, so we should apply the mask for \n",
        "        the analysis here before we return the simulated data.\n",
        "        \"\"\"\n",
        "        dataset = dataset.apply_mask(mask=self.mask)\n",
        "\n",
        "        \"\"\"\n",
        "        Outputs info about the `Tracer` to the fit, so we know exactly how we simulated the image.\n",
        "        \"\"\"\n",
        "        self.output_info(simulate_path=simulate_path, dataset=dataset, tracer=tracer)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def output_info(self, simulate_path: str, dataset: al.Imaging, tracer: al.Imaging):\n",
        "        \"\"\"\n",
        "        Output information about the data simulated for this iteration of sensitivity mapping.\n",
        "\n",
        "        This information output is as follows:\n",
        "\n",
        "        - A subplot of the simulated imaging dataset.\n",
        "        - A subplot of the tracer used to simulate this imaging dataset.\n",
        "        - A .json file containing the tracer galaxies.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        simulate_path\n",
        "            The path where the simulated dataset is output, contained within each sub-folder of the sensitivity\n",
        "            mapping.\n",
        "        dataset\n",
        "            The simulated imaging dataset which is visualized.\n",
        "        tracer\n",
        "            The tracer used to simulate the imaging dataset, which is visualized and output to a .json file.\n",
        "        \"\"\"\n",
        "\n",
        "        mat_plot = aplt.MatPlot2D(output=aplt.Output(path=simulate_path, format=\"png\"))\n",
        "\n",
        "        dataset_plotter = aplt.ImagingPlotter(dataset=dataset, mat_plot_2d=mat_plot)\n",
        "        dataset_plotter.subplot_dataset()\n",
        "\n",
        "        tracer_plotter = aplt.TracerPlotter(\n",
        "            tracer=tracer, grid=dataset.grid, mat_plot_2d=mat_plot\n",
        "        )\n",
        "        tracer_plotter.subplot_lensed_images()\n",
        "\n",
        "        al.output_to_json(\n",
        "            obj=tracer,\n",
        "            file_path=os.path.join(simulate_path, \"tracer.json\"),\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Base Fit__\n",
        "\n",
        "We have defined a `Simulate` class that will be used to simulate every dataset simulated by the sensitivity mapper.\n",
        "Each simulated dataset will have a unique set of parameters for the `subhalo` (e.g. due to different values of\n",
        "`perturb_model`.\n",
        "\n",
        "We will fit each simulated dataset using the `base_model`, which quantifies whether not including the dark matter\n",
        "subhalo in the model changess the goodness-of-fit and therefore indicates if we are sensitive to the subhalo.\n",
        "\n",
        "We now write a `BaseFit` class, defining how the `base_model` is fitted to each simulated dataset and the \n",
        "goodness-of-fit used to quantify whether the model fits the data well. As above, the `__init__` method can be\n",
        "extended with new inputs to control how the model is fitted and the `__call__` method performs the fit.\n",
        "\n",
        "In this example, we use a full non-linear search to fit the `base_model` to the simulated data and return\n",
        "the `log_evidence` of the model fit as the goodness-of-fit. This fit could easily be something much simpler and\n",
        "more computationally efficient, for example performing a single log likelihood evaluation of the `base_model` fit\n",
        "to the simulated data.\n",
        "\n",
        "Fucntionality which adapts the mesh and regularization of a pixelized source reconstruction to the unlensed source's \n",
        "morphology require an `adapt_images`. This is an input of the __init__ constructor which is passed to the `Analysis` \n",
        "for every simulated dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class BaseFit:\n",
        "    def __init__(self, adapt_images):\n",
        "        \"\"\"\n",
        "        Class used to fit every dataset used for sensitivity mapping with the base model (the model without the\n",
        "        perturbed feature sensitivity mapping maps out).\n",
        "\n",
        "        In this example, the base model therefore does not include the dark matter subhalo, but the simulated\n",
        "        dataset includes one.\n",
        "\n",
        "        The base fit is repeated for every parameter on the sensitivity grid and compared to the perturbed fit. This\n",
        "        maps out the sensitivity of every parameter is (e.g. the sensitivity of the mass of the subhalo).\n",
        "\n",
        "        The `__init__` constructor can be extended with new inputs which can be used to control how the dataset is\n",
        "        fitted, below we include an input `analysis_cls` which is the `AnalysisImaging` class used to fit the model\n",
        "        to the dataset.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        adapt_images\n",
        "            The result of the previous search containing adapt images used to adapt certain pixelized source meshs's\n",
        "            and regularizations to the unlensed source morphology.\n",
        "        \"\"\"\n",
        "        self.adapt_images = adapt_images\n",
        "\n",
        "    def __call__(self, dataset, model, paths):\n",
        "        \"\"\"\n",
        "        The base fitting function which fits every dataset used for sensitivity mapping with the base model.\n",
        "\n",
        "        This function receives as input each simulated dataset of the sensitivity map and fits it, in order to\n",
        "        quantify how sensitive the model is to the perturbed feature.\n",
        "\n",
        "        In this example, a full non-linear search is performed to determine how well the model fits the dataset.\n",
        "        The `log_evidence` of the fit is returned which acts as the sensitivity map figure of merit.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dataset\n",
        "            The dataset which is simulated with the perturbed model and which is fitted.\n",
        "        model\n",
        "            The model instance which is fitted to the dataset, which does not include the perturbed feature.\n",
        "        paths\n",
        "            The `Paths` instance which contains the path to the folder where the results of the fit are written to.\n",
        "        \"\"\"\n",
        "\n",
        "        search = af.Nautilus(\n",
        "            paths=paths,\n",
        "            n_live=50,\n",
        "        )\n",
        "\n",
        "        analysis = al.AnalysisImaging(dataset=dataset)\n",
        "        analysis._adapt_images = self.adapt_images\n",
        "\n",
        "        return search.fit(model=model, analysis=analysis)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Perturb Fit__\n",
        "\n",
        "We now define a `PerturbFit` class, which defines how the `perturb_model` is fitted to each simulated dataset. This\n",
        "behaves analogously to the `BaseFit` class above, but now fits the `perturb_model` to the simulated data (as\n",
        "opposed to the `base_model`).\n",
        "\n",
        "Again, in this example we use a full non-linear search to fit the `perturb_model` to the simulated data and return\n",
        "the `log_evidence` of the model fit as the goodness-of-fit. This fit could easily be something much simpler and\n",
        "more computationally efficient, for example performing a single log likelihood evaluation of the `perturb_model` fit\n",
        "to the simulated data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class PerturbFit:\n",
        "    def __init__(self, adapt_images):\n",
        "        \"\"\"\n",
        "        Class used to fit every dataset used for sensitivity mapping with the perturbed model (the model with the\n",
        "        perturbed feature sensitivity mapping maps out).\n",
        "\n",
        "        In this example, the perturbed model therefore includes the dark matter subhalo, which is also in the\n",
        "        simulated dataset.\n",
        "\n",
        "        The perturbed fit is repeated for every parameter on the sensitivity grid and compared to the base fit. This\n",
        "        maps out the sensitivity of every parameter is (e.g. the sensitivity of mass of the subhalo).\n",
        "\n",
        "        The `__init__` constructor can be extended with new inputs which can be used to control how the dataset is\n",
        "        fitted, below we include an input `analysis_cls` which is the `Analysis` class used to fit the model to the\n",
        "        dataset.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        adapt_images\n",
        "            The result of the previous search containing adapt images used to adapt certain pixelized source meshs's\n",
        "            and regularizations to the unlensed source morphology.\n",
        "        \"\"\"\n",
        "        self.adapt_images = adapt_images\n",
        "\n",
        "    def __call__(self, dataset, model, paths):\n",
        "        \"\"\"\n",
        "        The perturbed fitting function which fits every dataset used for sensitivity mapping with the perturbed model.\n",
        "\n",
        "        This function receives as input each simulated dataset of the sensitivity map and fits it, in order to\n",
        "        quantify how sensitive the model is to the perturbed feature.\n",
        "\n",
        "        In this example, a full non-linear search is performed to determine how well the model fits the dataset.\n",
        "        The `log_evidence` of the fit is returned which acts as the sensitivity map figure of merit.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        dataset\n",
        "            The dataset which is simulated with the perturbed model and which is fitted.\n",
        "        model\n",
        "            The model instance which is fitted to the dataset, which includes the perturbed feature.\n",
        "        paths\n",
        "            The `Paths` instance which contains the path to the folder where the results of the fit are written to.\n",
        "        \"\"\"\n",
        "\n",
        "        search = af.Nautilus(\n",
        "            paths=paths,\n",
        "            n_live=50,\n",
        "        )\n",
        "\n",
        "        analysis = al.AnalysisImaging(dataset=dataset)\n",
        "        analysis._adapt_images = self.adapt_images\n",
        "\n",
        "        return search.fit(model=model, analysis=analysis)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now combine all of the objects created above and perform sensitivity mapping. The inputs to the `Sensitivity`\n",
        "object below are:\n",
        "\n",
        "- `simulation_instance`: This is an instance of the model used to simulate every dataset that is fitted. In this example \n",
        "it is a lens model that does not include a subhalo, which was inferred by fitting the dataset we perform sensitivity \n",
        "mapping on.\n",
        "\n",
        "- `base_model`: This is the lens model that is fitted to every simulated dataset, which does not include a subhalo. In \n",
        "this example is composed of an `Isothermal` lens and `Sersic` source.\n",
        "\n",
        "- `perturb_model`: This is the extra model component that alongside the `base_model` is fitted to every simulated \n",
        "dataset. In this example it is a `NFWMCRLudlowSph` dark matter subhalo.\n",
        "\n",
        "- `simulate_cls`: This is the function that uses the `simulation_instance` and many instances of the `perturb_model` \n",
        "to simulate many datasets that are fitted with the `base_model` and `base_model` + `perturb_model`.\n",
        "\n",
        "- `base_fit_cls`: This is the function that fits the `base_model` to every simulated dataset and returns the\n",
        "goodness-of-fit of the model to the data.\n",
        "\n",
        "- `perturb_fit_cls`: This is the function that fits the `base_model` + `perturb_model` to every simulated dataset and\n",
        "returns the goodness-of-fit of the model to the data.\n",
        "\n",
        "- `number_of_steps`: The number of steps over which the parameters in the `perturb_model` are iterated. In this \n",
        "example, each subhalo ``centre` has a `UniformPrior` with lower limit -3.0 and upper limit 3.0, therefore \n",
        "the `number_of_steps=2` will simulate and fit 4 datasets where the `centre` values \n",
        "are [(-1.5, -1.5), (-1.5, 1.5), (1.5, -1.5), (1.5, 1.5)].\n",
        "\n",
        "- `number_of_cores`: The number of cores over which the sensitivity mapping is performed, enabling parallel processing\n",
        "if set above 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "paths = af.DirectoryPaths(\n",
        "    path_prefix=path.join(\"features\"),\n",
        "    name=\"sensitivity_mapping\",\n",
        ")\n",
        "\n",
        "sensitivity = af.Sensitivity(\n",
        "    paths=paths,\n",
        "    simulation_instance=simulation_instance,\n",
        "    base_model=base_model,\n",
        "    perturb_model=perturb_model,\n",
        "    simulate_cls=SimulateImaging(mask=mask, psf=dataset.psf),\n",
        "    base_fit_cls=BaseFit(adapt_images=result.adapt_images_from()),\n",
        "    perturb_fit_cls=PerturbFit(adapt_images=result.adapt_images_from()),\n",
        "    perturb_model_prior_func=perturb_model_prior_func,\n",
        "    number_of_steps=2,\n",
        "    #    number_of_steps=(4, 2),\n",
        "    number_of_cores=2,\n",
        ")\n",
        "\n",
        "sensitivity_result = sensitivity.run()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Results__\n",
        "\n",
        "You should now look at the results of the sensitivity mapping in the folder `output/features/sensitivity_mapping`. \n",
        "\n",
        "You will note the following 4 sets of x2 model-fits have been performed:\n",
        "\n",
        " - The `base_model` is fitted to a simulated dataset where a subhalo is included at the (y,x) \n",
        "   coorindates [(-1.5, -1.5), (-1.5, 1.5), (1.5, -1.5), (1.5, 1.5)].\n",
        "\n",
        " - The `base_model` + `perturb_model` is fitted to a simulated dataset where a subhalo is included at the (y,x) \n",
        "   coorindates [(-1.5, -1.5), (-1.5, 1.5), (1.5, -1.5), (1.5, 1.5)].\n",
        "\n",
        "The fit produces a `sensitivity_result`. \n",
        "\n",
        "We can print the `log_evidence_differences` of every cell of the sensitivity map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(sensitivity_result.log_evidence_differences.native)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}