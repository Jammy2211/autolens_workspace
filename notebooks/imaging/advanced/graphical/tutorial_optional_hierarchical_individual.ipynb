{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial Optional: Hierarchical Individual\n",
        "==========================================\n",
        "\n",
        "In tutorial 4, we fit a hierarchical model using a graphical model, whereby all datasets are fitted simultaneously\n",
        "and the hierarchical parameters are fitted for simultaneously with the model parameters of each lens in each\n",
        "dataset.\n",
        "\n",
        "This script illustrates how the hierarchical parameters can be estimated using a simpler approach, which fits\n",
        "each dataset one-by-one and estimates the hierarchical parameters afterwards by fitting the inferred `slope`'s\n",
        "with a Gaussian distribution.\n",
        "\n",
        "__Sample Simulation__\n",
        "\n",
        "The dataset fitted in this example script is simulated imaging data of a sample of 3 galaxies.\n",
        "\n",
        "This data is not automatically provided with the autogalaxy workspace, and must be first simulated by running the\n",
        "script `autolens_workspace/scripts/simulators/imaging/samples/advanced/mass_power_law.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import numpy as np\n",
        "from os import path\n",
        "\n",
        "import autofit as af\n",
        "import autolens as al"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "For each lens dataset in our sample we set up the correct path and load it by iterating over a for loop. \n",
        "\n",
        "We are loading a different dataset to the previous tutorials, where the lenses only have a single bulge component\n",
        "which each have different Sersic indexes which are drawn from a parent Gaussian distribution with a mean centre value \n",
        "of 4.0 and sigma of 1.0.\n",
        "\n",
        "This data is not automatically provided with the autogalaxy workspace, and must be first simulated by running the \n",
        "script `autolens_workspace/scripts/simulators/imaging/samples/advanced/mass_power_law.py`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_label = \"samples\"\n",
        "dataset_type = \"imaging\"\n",
        "dataset_sample_name = \"mass_power_law\"\n",
        "\n",
        "dataset_path = path.join(\"dataset\", dataset_type, dataset_label, dataset_sample_name)\n",
        "\n",
        "total_datasets = 3\n",
        "\n",
        "dataset_list = []\n",
        "\n",
        "for dataset_index in range(total_datasets):\n",
        "    dataset_sample_path = path.join(dataset_path, f\"dataset_{dataset_index}\")\n",
        "\n",
        "    dataset_list.append(\n",
        "        al.Imaging.from_fits(\n",
        "            data_path=path.join(dataset_sample_path, \"data.fits\"),\n",
        "            psf_path=path.join(dataset_sample_path, \"psf.fits\"),\n",
        "            noise_map_path=path.join(dataset_sample_path, \"noise_map.fits\"),\n",
        "            pixel_scales=0.1,\n",
        "        )\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Mask__\n",
        "\n",
        "We now mask each lens in our dataset, using the imaging list we created above.\n",
        "\n",
        "We will assume a 3.0\" mask for every lens in the dataset is appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "masked_imaging_list = []\n",
        "\n",
        "for dataset in dataset_list:\n",
        "    mask = al.Mask2D.circular(\n",
        "        shape_native=dataset.shape_native, pixel_scales=dataset.pixel_scales, radius=3.0\n",
        "    )\n",
        "\n",
        "    masked_imaging_list.append(dataset.apply_mask(mask=mask))\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Paths__\n",
        "\n",
        "The path the results of all model-fits are output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "path_prefix = path.join(\"imaging\", \"hierarchical\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "For each dataset we now create a corresponding `AnalysisImaging` class, as we are used to doing for `Imaging` data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_list = []\n",
        "\n",
        "for masked_dataset in masked_imaging_list:\n",
        "    analysis = al.AnalysisImaging(dataset=masked_dataset)\n",
        "\n",
        "    analysis_list.append(analysis)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "The model we fit to each dataset, which is a `PowerLawSph` lens mass model and `ExponentialSph` source."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lens = af.Model(al.Galaxy, redshift=0.5, mass=al.mp.PowerLawSph)\n",
        "lens.mass.centre = (0.0, 0.0)\n",
        "\n",
        "source = af.Model(al.Galaxy, redshift=1.0, bulge=al.lp_linear.ExponentialCoreSph)\n",
        "\n",
        "model = af.Collection(galaxies=af.Collection(lens=lens, source=source))\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fits (one-by-one)__\n",
        "\n",
        "For every dataset we now create an `Analysis` class using it and use `Nautilus` to fit it with a lens model.\n",
        "\n",
        "The `Result` is stored in the list `results`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_list = []\n",
        "\n",
        "for dataset_index, analysis in enumerate(analysis_list):\n",
        "    dataset_name = f\"dataset_{dataset_index}\"\n",
        "\n",
        "    \"\"\"\n",
        "    Create the `Nautilus` non-linear search and use it to fit the data.\n",
        "    \"\"\"\n",
        "    Nautilus = af.Nautilus(\n",
        "        name=\"\",\n",
        "        path_prefix=path.join(\"tutorial_optional_hierarchical_individual\"),\n",
        "        unique_tag=dataset_name,\n",
        "        n_live=200,\n",
        "        f_live=1e-4,\n",
        "    )\n",
        "\n",
        "    result_list.append(Nautilus.fit(model=model, analysis=analysis))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Results__\n",
        "\n",
        "Checkout the output folder, you should see three new sets of results corresponding to our 3 datasets.\n",
        "\n",
        "The `result_list` allows us to plot the median PDF value and 3.0 confidence intervals of the `slope` estimate \n",
        "from the model-fit to each dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples_list = [result.samples for result in result_list]\n",
        "\n",
        "mp_instances = [samps.median_pdf() for samps in samples_list]\n",
        "ue3_instances = [samp.errors_at_upper_sigma(sigma=3.0) for samp in samples_list]\n",
        "le3_instances = [samp.errors_at_lower_sigma(sigma=3.0) for samp in samples_list]\n",
        "\n",
        "mp_slopes = [instance.lenses.lens.bulge.slope for instance in mp_instances]\n",
        "ue3_slopes = [instance.lenses.lens.bulge.slope for instance in ue3_instances]\n",
        "le3_slopes = [instance.lenses.lens.bulge.slope for instance in le3_instances]\n",
        "\n",
        "print(f\"Median PDF inferred slope values\")\n",
        "print(mp_slopes)\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Overall Gaussian Parent Distribution__\n",
        "\n",
        "Fit the inferred `slope`'s from the fits performed above with a Gaussian distribution, in order to \n",
        "estimate the mean and scatter of the Gaussian from which the Sersic indexes were drawn.\n",
        "\n",
        "We first extract the inferred median PDF Sersic index values and their 1 sigma errors below, which will be the inputs\n",
        "to our fit for the parent Gaussian."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ue1_instances = [samp.values_at_upper_sigma(sigma=1.0) for samp in samples_list]\n",
        "le1_instances = [samp.values_at_lower_sigma(sigma=1.0) for samp in samples_list]\n",
        "\n",
        "ue1_slopes = [instance.lenses.lens.bulge.slope for instance in ue1_instances]\n",
        "le1_slopes = [instance.lenses.lens.bulge.slope for instance in le1_instances]\n",
        "\n",
        "error_list = [ue1 - le1 for ue1, le1 in zip(ue1_slopes, le1_slopes)]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Analysis` class below fits a Gaussian distribution to the inferred `slope` values from each of the fits above,\n",
        "where the inferred error values are used as the errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class Analysis(af.Analysis):\n",
        "    def __init__(self, data: np.ndarray, errors: np.ndarray):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data = np.array(data)\n",
        "        self.errors = np.array(errors)\n",
        "\n",
        "    def log_likelihood_function(self, instance: af.ModelInstance) -> float:\n",
        "        \"\"\"\n",
        "        Fits a set of 1D data points with a 1D Gaussian distribution, in order to determine from what Gaussian\n",
        "        distribution the analysis classes `data` were drawn.\n",
        "\n",
        "        In this example, this function determines from what parent Gaussian disrtribution the inferred slope\n",
        "        of each lens were drawn.\n",
        "        \"\"\"\n",
        "        log_likelihood_term_1 = np.sum(\n",
        "            -np.divide(\n",
        "                (self.data - instance.median) ** 2,\n",
        "                2 * (instance.scatter**2 + self.errors**2),\n",
        "            )\n",
        "        )\n",
        "        log_likelihood_term_2 = -np.sum(\n",
        "            0.5 * np.log(instance.scatter**2 + self.errors**2)\n",
        "        )\n",
        "\n",
        "        return log_likelihood_term_1 + log_likelihood_term_2\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `ParentGaussian` class is the model-component which used to fit the parent Gaussian to the inferred `slope` values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "class ParentGaussian:\n",
        "    def __init__(self, median: float = 0.0, scatter: float = 0.01):\n",
        "        \"\"\"\n",
        "        A model component which represents a parent Gaussian distribution, which can be fitted to a 1D set of\n",
        "        measurments with errors in order to determine the probabilty they were drawn from this Gaussian.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        median\n",
        "            The median value of the parent Gaussian distribution.\n",
        "        scatter\n",
        "            The scatter (E.g. the sigma value) of the Gaussian.\n",
        "        \"\"\"\n",
        "\n",
        "        self.median = median\n",
        "        self.scatter = scatter\n",
        "\n",
        "    def probability_from_values(self, values: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        For a set of 1D values, determine the probability that they were random drawn from this parent Gaussian\n",
        "        based on its `median` and `scatter` attributes.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        values\n",
        "            A set of 1D values from which we will determine the probability they were drawn from the parent Gaussian.\n",
        "        \"\"\"\n",
        "        values = np.sort(np.array(values))\n",
        "        transformed_values = np.subtract(values, self.median)\n",
        "\n",
        "        return np.multiply(\n",
        "            np.divide(1, self.scatter * np.sqrt(2.0 * np.pi)),\n",
        "            np.exp(-0.5 * np.square(np.divide(transformed_values, self.scatter))),\n",
        "        )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "The `ParentGaussian` is the model component we fit in order to determine the probability the inferred Sersic indexes \n",
        "were drawn from the distribution.\n",
        "\n",
        "This will be fitted via a non-linear search and therefore is created as a model component using `af.Model()` as per \n",
        "usual in **PyAutoFit**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Model(ParentGaussian)\n",
        "\n",
        "model.median = af.UniformPrior(lower_limit=0.0, upper_limit=100.0)\n",
        "model.scatter = af.UniformPrior(lower_limit=0.0, upper_limit=50.0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis + Search__\n",
        "\n",
        "We now create the Analysis class above which fits a parent 1D gaussian and create a Nautilus search in order to fit\n",
        "it to the 1D inferred list of `slope`'s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = Analysis(data=mp_slopes, errors=error_list)\n",
        "search = af.Nautilus(n_live=150)\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results of this fit tell us the most probable values for the `median` and `scatter` of the 1D parent Gaussian fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "median = samples.median_pdf().median\n",
        "\n",
        "u1_error = samples.values_at_upper_sigma(sigma=1.0).median\n",
        "l1_error = samples.values_at_lower_sigma(sigma=1.0).median\n",
        "\n",
        "u3_error = samples.values_at_upper_sigma(sigma=3.0).median\n",
        "l3_error = samples.values_at_lower_sigma(sigma=3.0).median\n",
        "\n",
        "print(\n",
        "    f\"Inferred value of the hierarchical median via simple fit to {total_datasets} datasets: \\n \"\n",
        ")\n",
        "print(f\"{median} ({l1_error} {u1_error}) [1.0 sigma confidence intervals]\")\n",
        "print(f\"{median} ({l3_error} {u3_error}) [3.0 sigma confidence intervals]\")\n",
        "print()\n",
        "\n",
        "scatter = samples.median_pdf().scatter\n",
        "\n",
        "u1_error = samples.values_at_upper_sigma(sigma=1.0).scatter\n",
        "l1_error = samples.values_at_lower_sigma(sigma=1.0).scatter\n",
        "\n",
        "u3_error = samples.values_at_upper_sigma(sigma=3.0).scatter\n",
        "l3_error = samples.values_at_lower_sigma(sigma=3.0).scatter\n",
        "\n",
        "print(\n",
        "    f\"Inferred value of the hierarchical scatter via simple fit to {total_datasets} datasets: \\n \"\n",
        ")\n",
        "print(f\"{scatter} ({l1_error} {u1_error}) [1.0 sigma confidence intervals]\")\n",
        "print(f\"{scatter} ({l3_error} {u3_error}) [3.0 sigma confidence intervals]\")\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can compare these values to those inferred in `tutorial_4_hierarchical_model`, which fits all datasets and the\n",
        "hierarchical values of the parent Gaussian simultaneously.,\n",
        " \n",
        "The errors for the fit performed in this tutorial are much larger. This is because of how in a graphical model\n",
        "the \"datasets talk to one another\", which is described fully in that tutorials subsection \"Benefits of Graphical Model\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}