{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 5: Expectation Propagation\n",
        "===================================\n",
        "\n",
        "In the previous tutorial, we fitted graphical models to a dataset comprising 3 images of strong lenses, which had a\n",
        "shared and global value of `slope` for the lens's mass distribution, or assumed their slopes were hierarchically drawn\n",
        "from a parent Gaussian distribution. This provides the basis of composing and fitting complex graphical models to large\n",
        "datasets.\n",
        "\n",
        "We concluded by discussing that one would soon hit a ceiling scaling these graphical models up to extremely large\n",
        "datasets. One would soon find that the parameter space is too complex to sample, and computational limits would\n",
        "ultimately cap how many datasets one could feasible fit.\n",
        "\n",
        "This tutorial introduces expectation propagation (EP), the solution to this problem, which inspects a factor graph\n",
        "and partitions the model-fit into many simpler fits of sub-components of the graph to individual datasets. This\n",
        "overcomes the challenge of model complexity, and mitigates computational restrictions that may occur if one tries to\n",
        "fit every dataset simultaneously.\n",
        "\n",
        "This tutorial fits a global model with a shared parameter and does not use a hierarchical model. Using a\n",
        "hierarchical model uses the same API introduced in tutorial 3, whereby a `HierarchicalFactor` is created\n",
        "and passed to the `FactorGraphModel`.\n",
        "\n",
        "The model fitted in this tutorial is the simpler model fitted in tutorials 1 & 2, where the weighted average\n",
        "proivided an accurate estimate of the shared parameter. We fit the same simple model here to illustrate EP, and will\n",
        "fit a more challenging model that is only possible because of EP in the next tutorial.\n",
        "\n",
        "__Sample Simulation__\n",
        "\n",
        "The dataset fitted in this example script is simulated imaging data of a sample of 3 galaxies.\n",
        "\n",
        "This data is not automatically provided with the autogalaxy workspace, and must be first simulated by running the\n",
        "script `autolens_workspace/scripts/simulators/imaging/samples/simple__no_lens_light.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import autolens as al\n",
        "import autofit as af\n",
        "from os import path"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "The following steps repeat all the initial steps performed in tutorial 2 and 3:\n",
        "\n",
        "This data is not automatically provided with the autogalaxy workspace, and must be first simulated by running the \n",
        "script `autolens_workspace/scripts/simulators/imaging/samples/simple__no_lens_light.py`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_label = \"samples\"\n",
        "dataset_type = \"imaging\"\n",
        "dataset_sample_name = \"simple__no_lens_light__mass_sis\"\n",
        "\n",
        "dataset_path = path.join(\"dataset\", dataset_type, dataset_label, dataset_sample_name)\n",
        "\n",
        "total_datasets = 3\n",
        "\n",
        "dataset_list = []\n",
        "\n",
        "for dataset_index in range(total_datasets):\n",
        "    dataset_sample_path = path.join(dataset_path, f\"dataset_{dataset_index}\")\n",
        "\n",
        "    dataset_list.append(\n",
        "        al.Imaging.from_fits(\n",
        "            data_path=path.join(dataset_sample_path, \"data.fits\"),\n",
        "            psf_path=path.join(dataset_sample_path, \"psf.fits\"),\n",
        "            noise_map_path=path.join(dataset_sample_path, \"noise_map.fits\"),\n",
        "            pixel_scales=0.1,\n",
        "        )\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Mask__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "masked_imaging_list = []\n",
        "\n",
        "for dataset in dataset_list:\n",
        "    mask = al.Mask2D.circular(\n",
        "        shape_native=dataset.shape_native, pixel_scales=dataset.pixel_scales, radius=3.0\n",
        "    )\n",
        "\n",
        "    masked_imaging_list.append(dataset.apply_mask(mask=mask))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Paths__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "path_prefix = path.join(\"imaging\", \"hierarchical\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "We compose our model using `Model` objects, which represent the lenses we fit to our data.\n",
        "\n",
        "This graphical model creates a non-linear parameter space that has parameters for every lens and source galaxy in our \n",
        "sample. In this example, there are 3 lenses each with their own model, therefore:\n",
        "\n",
        " - Each lens galaxy's total mass distribution is an `PowerLawSph` with its centre fixed to its true value of \n",
        " (0.0, 0.0) [2 parameters].\n",
        " \n",
        " - Each source galaxy's light is a linear parametric `ExponentialSph` [3 parameters].\n",
        "\n",
        " - There are three lenses in our graphical model [3 x 1 = 3 parameters]. \n",
        "\n",
        " - There are three source in our graphical model [3 x 4 = 12 parameters]. \n",
        "\n",
        "The overall dimensionality of each parameter space fitted separately via EP is therefore N=6.\n",
        "\n",
        "In total, the graph has N = 3 x 6 = 18 free parameters, albeit EP knows the `slope` is shared and fits it in the special\n",
        "way described below to account for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "slope_shared_prior = af.GaussianPrior(mean=2.0, sigma=1.0)\n",
        "\n",
        "model_list = []\n",
        "\n",
        "for model_index in range(total_datasets):\n",
        "    lens = af.Model(al.Galaxy, redshift=0.5, mass=al.mp.PowerLawSph)\n",
        "    lens.mass.centre = (0.0, 0.0)\n",
        "    lens.mass.einstein_radius = af.GaussianPrior(mean=1.8, sigma=0.3, lower_limit=0.0)\n",
        "\n",
        "    # This slope is shared across all lens galaxies\n",
        "    lens.mass.slope = slope_shared_prior\n",
        "\n",
        "    source = af.Model(al.Galaxy, redshift=1.0, bulge=al.lp_linear.ExponentialCore)\n",
        "\n",
        "    source.bulge.ell_comps.ellipitcal_comps_0 = 0.0\n",
        "    source.bulge.ell_comps.ellipitcal_comps_1 = 0.0\n",
        "\n",
        "    source.bulge.centre_0 = af.GaussianPrior(mean=0.0, sigma=0.3)\n",
        "    source.bulge.centre_1 = af.GaussianPrior(mean=0.0, sigma=0.3)\n",
        "    source.bulge.effective_radius = af.GaussianPrior(\n",
        "        mean=1.0, sigma=1.0, lower_limit=0.01, upper_limit=3.0\n",
        "    )\n",
        "\n",
        "    model = af.Collection(galaxies=af.Collection(lens=lens, source=source))\n",
        "\n",
        "    model_list.append(model)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_list = []\n",
        "\n",
        "for masked_dataset in masked_imaging_list:\n",
        "    analysis = al.AnalysisImaging(dataset=masked_dataset)\n",
        "\n",
        "    analysis_list.append(analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis Factors__\n",
        "\n",
        "Now we have our `Analysis` classes and graphical model, we can compose our `AnalysisFactor`'s, just like we did in the\n",
        "previous tutorial.\n",
        "\n",
        "However, unlike the previous tutorial, each `AnalysisFactor` is now assigned its own `search`. This is because the EP \n",
        "framework performs a model-fit to each node on the factor graph (e.g. each `AnalysisFactor`). Therefore, each node \n",
        "requires its own non-linear search. \n",
        "\n",
        "For complex graphs consisting of many  nodes, one could easily use different searches for different nodes on the factor \n",
        "graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Nautilus = af.Nautilus(\n",
        "    path_prefix=path.join(\"imaging\", \"hierarchical\"),\n",
        "    name=\"tutorial_5_expectation_propagation\",\n",
        "    n_live=150,\n",
        ")\n",
        "\n",
        "analysis_factor_list = []\n",
        "dataset_index = 0\n",
        "\n",
        "for model, analysis in zip(model_list, analysis_list):\n",
        "    dataset_name = f\"dataset_{dataset_index}\"\n",
        "    dataset_index += 1\n",
        "\n",
        "    analysis_factor = af.AnalysisFactor(\n",
        "        prior_model=model, analysis=analysis, optimiser=Nautilus, name=dataset_name\n",
        "    )\n",
        "\n",
        "    analysis_factor_list.append(analysis_factor)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We again combine our `AnalysisFactors` into one, to compose the factor graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "factor_graph = af.FactorGraphModel(*analysis_factor_list)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The factor graph model `info` attribute shows the model which we fit via expectaton propagation (note that we do\n",
        "not use `global_prior_model` below when performing the fit)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(factor_graph.global_prior_model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Expectation Propagation__\n",
        "\n",
        "In the previous tutorials, we used the `global_prior_model` of the `factor_graph` to fit the global model. In this \n",
        "tutorial, we instead fit the `factor_graph` using the EP framework, which fits the graphical model composed in this \n",
        "tutorial as follows:\n",
        "\n",
        "1) Go to the first node on the factor graph (e.g. `analysis_factor_list[0]`) and fit its model to its dataset. This is \n",
        "simply a fit of the first lens model to the first lens data dataset, the model-fit we are used to performing by now.\n",
        "\n",
        "2) Once the model-fit is complete, inspect the model for parameters that are shared with other nodes on the factor\n",
        "graph. In this example, the `slope` of the mass model fitted to the first dataset is global, and therefore connects\n",
        "to two other nodes on the factor graph (the `AnalysisFactor`'s) of the second and first lens datasets.\n",
        "\n",
        "3) The EP framework now creates a 'message' that is to be passed to the connecting nodes on the factor graph. This\n",
        "message informs them of the results of the model-fit, so they can update their priors on the slope accordingly and, \n",
        "more importantly, update their posterior inference and therefore estimate of the global slope.\n",
        "\n",
        "For example, the model fitted to the first lens dataset includes the global slope. Therefore, after the model is \n",
        "fitted, the EP framework creates a 'message' informing the factor graph about its inference on that lens model's slope,\n",
        "thereby updating our overall inference on this shared parameter. This is termed 'message passing'.\n",
        "\n",
        "__Cyclic Fitting__\n",
        "\n",
        "After every `AnalysisFactor` has been fitted (e.g. all 3 datasets in this example), we have a new estimate of the \n",
        "shared parameter `slope`. This updates our priors on the shared parameter `slope`, which needs to be reflected in \n",
        "each model-fit we perform on each `AnalysisFactor`. \n",
        "\n",
        "The EP framework therefore performs a second iteration of model-fits. It again cycles through each `AnalysisFactor` \n",
        "and refits the model, using updated priors on shared parameters like the `slope`. At the end of each fit, we again \n",
        "create messages that update our knowledge about other parameters on the graph.\n",
        "\n",
        "This process is repeated multiple times, until a convergence criteria is met whereby continued cycles are expected to\n",
        "produce the same estimate of the shared parameter `slope`. \n",
        "\n",
        "When we fit the factor graph a `name` is passed, which determines the folder all results of the factor graph are\n",
        "stored in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "laplace = af.LaplaceOptimiser()\n",
        "\n",
        "paths = af.DirectoryPaths(\n",
        "    name=path.join(path_prefix, \"tutorial_5_expectation_propagation_2\")\n",
        ")\n",
        "\n",
        "factor_graph_result = factor_graph.optimise(\n",
        "    optimiser=laplace, paths=paths, ep_history=af.EPHistory(kl_tol=0.05), max_steps=5\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Output__\n",
        "\n",
        "The results of the factor graph, using the EP framework and message passing, are contained in the folder \n",
        "`output/graphical/imaging/tutorial_5_expectation_propagation`. \n",
        "\n",
        "The following folders and files are worth of note:\n",
        "\n",
        " - `graph.info`: this provides an overall summary of the graphical model that is fitted, including every parameter, \n",
        " how parameters are shared across `AnalysisFactor`'s and the priors associated to each individual parameter.\n",
        " \n",
        " - The 3 folders titled `gaussian_x1_#__low_snr` correspond to the three `AnalysisFactor`'s and therefore signify \n",
        " repeated non-linear searches that are performed to fit each dataset.\n",
        " \n",
        " - Inside each of these folders are `optimization_#` folders, corresponding to each model-fit performed over cycles of\n",
        " the EP fit. A careful inspection of the `model.info` files inside each folder reveals how the priors are updated\n",
        " over each cycle, whereas the `model.results` file should indicate the improved estimate of model parameters over each\n",
        " cycle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "print(factor_graph_result)\n",
        "\n",
        "print(factor_graph_result.updated_ep_mean_field.mean_field)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Output__\n",
        "\n",
        "The MeanField object representing the posterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(factor_graph_result.updated_ep_mean_field.mean_field)\n",
        "print()\n",
        "\n",
        "print(factor_graph_result.updated_ep_mean_field.mean_field.variables)\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The logpdf of the posterior at the point specified by the dictionary values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# factor_graph_result.updated_ep_mean_field.mean_field(values=None)\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A dictionary of the mean with variables as keys."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(factor_graph_result.updated_ep_mean_field.mean_field.mean)\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A dictionary of the variance with variables as keys."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(factor_graph_result.updated_ep_mean_field.mean_field.variance)\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A dictionary of the s.d./variance**0.5 with variables as keys."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(factor_graph_result.updated_ep_mean_field.mean_field.scale)\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "self.updated_ep_mean_field.mean_field[v: Variable] gives the Message/approximation of the posterior for an \n",
        "individual variable of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# factor_graph_result.updated_ep_mean_field.mean_field[\"help\"]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}