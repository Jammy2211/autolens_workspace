{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results: Samples\n",
        "================\n",
        "\n",
        "After fitting strong lens data a search returns a `result` variable. We have used this throughout the\n",
        "examples scripts to plot the maximum log likelihood tracer and fits.\n",
        "\n",
        "This `Result` object contains a lot more information which the results tutorials illustrate.\n",
        "\n",
        "This script describes the non-linear search samples of a model-fit, which for a `DynestyStatic` fit corresponds\n",
        "to every accepted live point in parameter space.\n",
        "\n",
        "These are used to compute quantities like the maximum likelihood model, the errors on the parameters and visualization\n",
        "showing the parameter degeneracies.\n",
        "\n",
        "__Units__\n",
        "\n",
        "In this example, all quantities are **PyAutoLens**'s internal unit coordinates, with spatial coordinates in\n",
        "arc seconds, luminosities in electrons per second and mass quantities (e.g. convergence) are dimensionless.\n",
        "\n",
        "The results example `units_and_cosmology.ipynb` illustrates how to convert these quantities to physical units like\n",
        "kiloparsecs, magnitudes and solar masses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import autofit as af\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fit__\n",
        "\n",
        "The code below (which we have omitted comments from for brevity) performs a lens model-fit using dynesty. You should\n",
        "be familiar enough with lens modeling to understand this, if not you should go over the beginner model-fit script again!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"simple__no_lens_light\"\n",
        "dataset_path = path.join(\"dataset\", \"imaging\", dataset_name)\n",
        "\n",
        "dataset = al.Imaging.from_fits(\n",
        "    data_path=path.join(dataset_path, \"data.fits\"),\n",
        "    psf_path=path.join(dataset_path, \"psf.fits\"),\n",
        "    noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "    pixel_scales=0.1,\n",
        ")\n",
        "\n",
        "mask = al.Mask2D.circular(\n",
        "    shape_native=dataset.shape_native, pixel_scales=dataset.pixel_scales, radius=3.0\n",
        ")\n",
        "\n",
        "dataset = dataset.apply_mask(mask=mask)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Composition__\n",
        "\n",
        "The code below composes the model fitted to the data (the API is described in the `modeling/start_here.py` example).\n",
        "\n",
        "The way the model is composed below (e.g. that the model is called `cti` and includes a `trap_list` and `ccd`) should \n",
        "be noted, as it will be important when inspecting certain results later in this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(\n",
        "    galaxies=af.Collection(\n",
        "        lens=af.Model(al.Galaxy, redshift=0.5, mass=al.mp.Isothermal),\n",
        "        source=af.Model(al.Galaxy, redshift=1.0, bulge=al.lp.Sersic),\n",
        "    )\n",
        ")\n",
        "\n",
        "search = af.DynestyStatic(\n",
        "    path_prefix=path.join(\"imaging\", \"modeling\"),\n",
        "    name=\"mass[sie]_source[bulge]\",\n",
        "    unique_tag=dataset_name,\n",
        "    nlive=50,\n",
        ")\n",
        "\n",
        "analysis = al.AnalysisImaging(dataset=dataset)\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Info__\n",
        "\n",
        "As seen throughout the workspace, the `info` attribute shows the result in a readable format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Plot__\n",
        "\n",
        "We now have the `Result` object we will cover in this script. \n",
        "\n",
        "As a reminder, in the `modeling` scripts we use the `max_log_likelihood_tracer` and `max_log_likelihood_fit` to plot \n",
        "the results of the fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tracer_plotter = aplt.TracerPlotter(\n",
        "    tracer=result.max_log_likelihood_tracer, grid=mask.derive_grid.all_false_sub_1\n",
        ")\n",
        "tracer_plotter.subplot_tracer()\n",
        "fit_plotter = aplt.FitImagingPlotter(fit=result.max_log_likelihood_fit)\n",
        "fit_plotter.subplot_fit()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results tutorials 2 and 3 will expand on the `max_log_likelihood_tracer` and `max_log_likelihood_fit`, showing how \n",
        "they can be used to inspect many aspects of a model.\n",
        "\n",
        "__Samples__\n",
        "\n",
        "The result's `Samples` object contains the complete set of non-linear search dynesty samples, where each sample \n",
        "corresponds to a set of a model parameters that were evaluated and accepted. \n",
        "\n",
        "This also includes their log likelihoods, which are used for computing additional information about the model-fit,\n",
        "for example the error on every parameter. \n",
        "\n",
        "Our model-fit used the nested sampling algorithm Dynesty, so the `Samples` object returned is a `SamplesNest` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "print(\"Nest Samples: \\n\")\n",
        "print(samples)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Parameters__\n",
        "\n",
        "The `Samples` class contains all the parameter samples, which is a list of lists where:\n",
        "\n",
        " - The outer list is the size of the total number of samples.\n",
        " - The inner list is the size of the number of free parameters in the fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"All parameters of the very first sample\")\n",
        "print(samples.parameter_lists[0])\n",
        "print(\"The fourth parameter of the tenth sample\")\n",
        "print(samples.parameter_lists[9][3])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Figures of Merit__\n",
        "\n",
        "The `Samples` class contains the log likelihood, log prior, log posterior and weight_list of every sample, where:\n",
        "\n",
        " - The log likelihood is the value evaluated from the likelihood function (e.g. -0.5 * chi_squared + the noise \n",
        "   normalization).\n",
        "\n",
        " - The log prior encodes information on how the priors on the parameters maps the log likelihood value to the log\n",
        "   posterior value.\n",
        "\n",
        " - The log posterior is log_likelihood + log_prior.\n",
        "\n",
        " - The weight gives information on how samples should be combined to estimate the posterior. The weight values \n",
        "   depend on the sampler used. For example for an MCMC search they will all be 1`s whereas for the nested sampling\n",
        "   method used in this example they are weighted as a combination of the log likelihood value and prior.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"log(likelihood), log(prior), log(posterior) and weight of the tenth sample.\")\n",
        "print(samples.log_likelihood_list[9])\n",
        "print(samples.log_prior_list[9])\n",
        "print(samples.log_posterior_list[9])\n",
        "print(samples.weight_list[9])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Instances__\n",
        "\n",
        "The `Samples` contains many results which are returned as an instance of the model, using the Python class structure\n",
        "of the model composition.\n",
        "\n",
        "For example, we can return the model parameters corresponding to the maximum log likelihood sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "max_lh_instance = samples.max_log_likelihood()\n",
        "print(\"Maximum Log Likelihood Model Instance: \\n\")\n",
        "print(max_lh_instance, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A model instance contains all the model components of our fit, for example the list of galaxies we specified during \n",
        "model composition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(max_lh_instance.galaxies)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These galaxies will be named according to the model fitted by the search (in this case, `lens` and `source`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(max_lh_instance.galaxies.lens)\n",
        "print(max_lh_instance.galaxies.source)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Their `LightProfile`'s and `MassProfile`'s are also named according to the search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(max_lh_instance.galaxies.lens.mass)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use this list of galaxies to create the maximum log likelihood `Tracer`, which, funnily enough, \n",
        "is the property of the result we've used up to now!\n",
        "\n",
        "Using this tracer will be expanded upon in the next results tutorial.\n",
        "\n",
        "(If we had the `Imaging` available we could easily use this to create the maximum log likelihood `FitImaging`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "max_lh_tracer = al.Tracer.from_galaxies(galaxies=max_lh_instance.galaxies)\n",
        "\n",
        "print(max_lh_tracer)\n",
        "print(mask.derive_grid.all_false_sub_1)\n",
        "\n",
        "tracer_plotter = aplt.TracerPlotter(\n",
        "    tracer=max_lh_tracer, grid=mask.derive_grid.all_false_sub_1\n",
        ")\n",
        "tracer_plotter.subplot_tracer()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Vectors__\n",
        "\n",
        "All results can alternatively be returned as a 1D vector of values, by passing `as_instance=False`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "max_lh_vector = samples.max_log_likelihood(as_instance=False)\n",
        "print(\"Max Log Likelihood Model Parameters: \\n\")\n",
        "print(max_lh_vector, \"\\n\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Labels__\n",
        "\n",
        "Vectors return a lists of all model parameters, but do not tell us which values correspond to which parameters.\n",
        "\n",
        "The following quantities are available in the `Model`, where the order of their entries correspond to the parameters \n",
        "in the `ml_vector` above:\n",
        "\n",
        " - `paths`: a list of tuples which give the path of every parameter in the `Model`.\n",
        " - `parameter_names`: a list of shorthand parameter names derived from the `paths`.\n",
        " - `parameter_labels`: a list of parameter labels used when visualizing non-linear search results (see below).\n",
        "\n",
        "For simple models like the one fitted in this tutorial, the quantities below are somewhat redundant. For the\n",
        "more complex models illustrated in other tutorials their utility will become clear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = samples.model\n",
        "\n",
        "print(model.paths)\n",
        "print(model.parameter_names)\n",
        "print(model.parameter_labels)\n",
        "print(model.model_component_and_parameter_names)\n",
        "print(\"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Posterior / PDF__\n",
        "\n",
        "PDF stands for \"Probability Density Function\" and it quantifies the PDF stands for \"Probability Density Function\" and it quantifies the probability of each model sampled. It \n",
        "therefore enables error estimation via a process called marginalization.\n",
        "\n",
        "We can access the `median pdf` model, which is the model computed by marginalizing over the samples of every \n",
        "parameter in 1D and taking the median of this PDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "median_pdf_instance = samples.median_pdf()\n",
        "\n",
        "print(\"Median PDF Model Instances: \\n\")\n",
        "print(median_pdf_instance, \"\\n\")\n",
        "print(median_pdf_instance.galaxies.source.bulge)\n",
        "print()\n",
        "\n",
        "median_pdf_vector = samples.median_pdf(as_instance=False)\n",
        "\n",
        "print(\"Median PDF Model Parameter Lists: \\n\")\n",
        "print(median_pdf_vector, \"\\n\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Errors__\n",
        "\n",
        "We can compute the model parameters at a given sigma value (e.g. at 3.0 sigma limits).\n",
        "\n",
        "These parameter values and error estimates do not fully account for covariance between model parameters, which is\n",
        "explained in the \"Derived Errors\" section below.\n",
        "\n",
        "The `uv3` below signifies this is an upper value at 3 sigma confidence, with `lv3` indicating a the lower value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "uv3_instance = samples.values_at_upper_sigma(sigma=3.0)\n",
        "lv3_instance = samples.values_at_lower_sigma(sigma=3.0)\n",
        "\n",
        "print(\"Errors Instances: \\n\")\n",
        "print(uv3_instance, \"\\n\")\n",
        "print(lv3_instance, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can compute the upper and lower errors on each parameter at a given sigma limit.\n",
        "\n",
        "The `ue3` below signifies the upper error at 3 sigma. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ue3_instance = samples.errors_at_upper_sigma(sigma=3.0)\n",
        "le3_instance = samples.errors_at_lower_sigma(sigma=3.0)\n",
        "\n",
        "print(\"Errors Instances: \\n\")\n",
        "print(ue3_instance, \"\\n\")\n",
        "print(le3_instance, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search Plots__\n",
        "\n",
        "The Probability Density Functions of the results can be plotted using Dynesty's in-built visualization tools, \n",
        "which are wrapped via the `DynestyPlotter` object.\n",
        "\n",
        "The plot is labeled with short hand parameter names (e.g. `sersic_index` is mapped to the short hand \n",
        "parameter `n`). These mappings ate specified in the `config/notation.yaml` file and can be customized by users.\n",
        "\n",
        "The superscripts of labels correspond to the name each component was given in the model (e.g. for the `Isothermal`\n",
        "mass its name `mass` defined when making the `Model` above is used)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search_plotter = aplt.DynestyPlotter(samples=result.samples)\n",
        "search_plotter.cornerplot()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Sample Instance__\n",
        "\n",
        "A dynesty search retains every model that is accepted during the model-fit.\n",
        "\n",
        "We can create an instance of any lens model -- below we create an instance of the 100th last accepted model, and we\n",
        "can compare its parameters to the maximum log likelihood model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instance = samples.from_sample_index(sample_index=-10)\n",
        "\n",
        "print(max_lh_instance.galaxies.lens.mass)\n",
        "print(instance.galaxies.lens.mass)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Maximum Likelihood__\n",
        "\n",
        "The maximum log likelihood value of the model-fit can be estimated by simple taking the maximum of all log\n",
        "likelihoods of the samples.\n",
        "\n",
        "If different models are fitted to the same dataset, this value can be compared to determine which model provides\n",
        "the best fit (e.g. which model has the highest maximum likelihood)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Maximum Log Likelihood: \\n\")\n",
        "print(max(samples.log_likelihood_list))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Bayesian Evidence__\n",
        "\n",
        "Nested sampling algorithms like dynesty also estimate the Bayesian evidence (estimated via the nested sampling \n",
        "algorithm).\n",
        "\n",
        "The Bayesian evidence accounts for \"Occam's Razor\", whereby it penalizes models for being more complex (e.g. if a model\n",
        "has more parameters it needs to fit the da\n",
        "\n",
        "The Bayesian evidence is a better quantity to use to compare models, because it penalizes models with more parameters\n",
        "for being more complex (\"Occam's Razor\"). Comparisons using the maximum likelihood value do not account for this and\n",
        "therefore may unjustly favour more complex models.\n",
        "\n",
        "Using the Bayesian evidence for model comparison is well documented on the internet, for example the following\n",
        "wikipedia page: https://en.wikipedia.org/wiki/Bayes_factor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Maximum Log Likelihood and Log Evidence: \\n\")\n",
        "print(samples.log_evidence)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Latex__\n",
        "\n",
        "If you are writing results up in a paper, you can use PyAutoFit's inbuilt latex tools to create latex \n",
        "table code which you can copy to your .tex document.\n",
        "\n",
        "By combining this with the filtering tools below, specific parameters can be included or removed from the latex.\n",
        "\n",
        "The superscripts of each parameter's latex string are loaded from the config file `notation/label.yaml`. Editing this\n",
        "config file provides high levels of customization for each parameter appears in the latex table. \n",
        "\n",
        "This is especially useful if your lens model uses the same model components with the same parameter, which \n",
        "therefore need to be distinguished via superscripts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "latex = af.text.Samples.latex(\n",
        "    samples=result.samples,\n",
        "    median_pdf_model=True,\n",
        "    sigma=3.0,\n",
        "    name_to_label=True,\n",
        "    include_name=True,\n",
        "    include_quickmath=True,\n",
        "    prefix=\"Example Prefix \",\n",
        "    suffix=r\"\\\\[-2pt]\",\n",
        ")\n",
        "\n",
        "print(latex)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Wrap Up__\n",
        "\n",
        "This tutorial illustrated how to analyse the non-linear samples of a model-fit. We saw that the API used for model \n",
        "composition produced the instances of the results after fitting (for example, how we accessed the galaxy model \n",
        "as `instance.galaxies.lens.mass`).\n",
        "\n",
        "The remainder of this script provides advanced samples calculations which:\n",
        "\n",
        " - Calculate the most probable value with errors of a derived quantity by computing its PDF. For example, computing \n",
        "   the axis-ratio of the model with errors (the axis-ratio is not directly accessible from the model).\n",
        " \n",
        " - Filtering `Samples` objects to remove certain parameters or components from the model. This can help with,\n",
        "   for example, creating LaTex tables of specific parts of a model.\n",
        "\n",
        "__Derived Errors (Advanced)__\n",
        "\n",
        "Computing the errors of a quantity like the `einstein_radius` is simple, because it is sampled by the non-linear \n",
        "search. Errors are accessible using the `Samples` object's `errors_from` methods, which marginalize over the \n",
        "parameters via the 1D Probability Density Function (PDF).\n",
        "\n",
        "Computing errors on derived quantitys is more tricky, because it is not sampled directly by the non-linear search. \n",
        "For example, what if we want the error on the axis-ratio of the mass model? In order to do this we need to create the \n",
        "PDF of that derived quantity, which we can then marginalize over using the same function we use to marginalize model \n",
        "parameters.\n",
        "\n",
        "Below, we compute the axis-ratio of every accepted model sampled by the non-linear search and use this determine the PDF \n",
        "of the axis-ratio. When combining the axis-ratio's we weight each value by its `weight`. For Dynesty, a nested sampling \n",
        "algorithm, the weight of every sample is different and thus must be included.\n",
        "\n",
        "In order to pass these samples to the function `marginalize`, which marginalizes over the PDF of the axis-ratio to \n",
        "compute its error, we also pass the weight list of the samples.\n",
        "\n",
        "Note again how because when creating the model above using the input names `lens` and `mass` we access the instance\n",
        "below using these."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "axis_ratio_list = []\n",
        "\n",
        "for sample in samples.sample_list:\n",
        "    instance = sample.instance_for_model(model=samples.model)\n",
        "\n",
        "    ell_comps = instance.galaxies.lens.mass.ell_comps\n",
        "\n",
        "    axis_ratio = al.convert.axis_ratio_from(ell_comps=ell_comps)\n",
        "\n",
        "    axis_ratio_list.append(axis_ratio)\n",
        "\n",
        "median_axis_ratio, upper_axis_ratio, lower_axis_ratio = af.marginalize(\n",
        "    parameter_list=axis_ratio_list, sigma=3.0, weight_list=samples.weight_list\n",
        ")\n",
        "\n",
        "print(f\"axis_ratio = {median_axis_ratio} ({upper_axis_ratio} {lower_axis_ratio}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Samples Filtering (Advanced)__\n",
        "\n",
        "The samples object has the results for all model parameter. It can be filtered to contain the results of specific \n",
        "parameters of interest.\n",
        "\n",
        "The basic form of filtering specifies parameters via their path, which was printed above via the model and is printed \n",
        "again below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "print(\"Parameter paths in the model which are used for filtering:\")\n",
        "print(samples.model.paths)\n",
        "\n",
        "print(\"All parameters of the very first sample\")\n",
        "print(samples.parameter_lists[0])\n",
        "\n",
        "samples = samples.with_paths(\n",
        "    [\n",
        "        (\"galaxies\", \"lens\", \"mass\", \"einstein_radius\"),\n",
        "        (\"galaxies\", \"source\", \"bulge\", \"sersic_index\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"All parameters of the very first sample (containing only the lens mass's einstein radius and \"\n",
        "    \"source bulge's sersic index).\"\n",
        ")\n",
        "print(samples.parameter_lists[0])\n",
        "\n",
        "print(\n",
        "    \"Maximum Log Likelihood Model Instances (containing only the lens mass's einstein radius and \"\n",
        "    \"source bulge's sersic index):\\n\"\n",
        ")\n",
        "print(samples.max_log_likelihood(as_instance=False))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above, we specified each path as a list of tuples of strings. \n",
        "\n",
        "This is how the PyAutoFit source code stores the path to different components of the model, but it is not in-line \n",
        "with the PyAutoLens API used to compose a model.\n",
        "\n",
        "We can alternatively use the following API:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "samples = samples.with_paths(\n",
        "    [\"galaxies.lens.mass.einstein_radius\", \"galaxies.source.bulge.sersic_index\"]\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"All parameters of the very first sample (containing only the lens mass's einstein radius and \"\n",
        "    \"source bulge's sersic index).\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can alternatively filter the `Samples` object by removing all parameters with a certain path. Below, we remove\n",
        "the centres of the mass model to be left with 10 parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "print(\"Parameter paths in the model which are used for filtering:\")\n",
        "print(samples.model.paths)\n",
        "\n",
        "print(\"Parameters of first sample\")\n",
        "print(samples.parameter_lists[0])\n",
        "\n",
        "print(samples.model.total_free_parameters)\n",
        "\n",
        "samples = samples.without_paths(\n",
        "    [\n",
        "        # \"galaxies.lens.mass.centre\"),\n",
        "        \"galaxies.lens.mass.centre.centre_0\",\n",
        "        # \"galaxies.lens.mass.centre.centre_1),\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Parameters of first sample without the lens mass centre.\")\n",
        "print(samples.parameter_lists[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can keep and remove entire paths of the samples, for example keeping only the parameters of the lens or \n",
        "removing all parameters of the source's bulge."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "samples = samples.with_paths([\"galaxies.lens\"])\n",
        "print(\"Parameters of the first sample of the lens galaxy\")\n",
        "print(samples.parameter_lists[0])\n",
        "\n",
        "samples = result.samples\n",
        "samples = samples.without_paths([\"galaxies.source.bulge\"])\n",
        "print(\"Parameters of the first sample without the source's bulge\")\n",
        "print(samples.parameter_lists[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fin."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}