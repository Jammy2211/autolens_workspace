{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modeling: Start Here\n",
        "====================\n",
        "\n",
        "This script is the starting point for lens modeling of CCD imaging data (E.g. Hubble Space Telescope, Euclid) with\n",
        "**PyAutoLens** and it provides an overview of the lens modeling API.\n",
        "\n",
        "After reading this script, the `features`, `customize` and `searches` folders provide example for performing lens\n",
        "modeling in different ways and customizing the analysis.\n",
        "\n",
        "__Model__\n",
        "\n",
        "This script fits an `Imaging` dataset of a 'galaxy-scale' strong lens with a model where:\n",
        "\n",
        " - The lens galaxy's light is a parametric `Sersic` bulge and `Exponential` disk.\n",
        " - The lens galaxy's total mass distribution is an `Isothermal` and `ExternalShear`.\n",
        " - The source galaxy's light is a parametric `Sersic`.\n",
        "\n",
        "This lens model is simple and computationally fast to fit, and therefore acts as a good starting point for new\n",
        "users.\n",
        "\n",
        "__Plotters__\n",
        "\n",
        "To produce images of the data `Plotter` objects are used, which are high-level wrappers of matplotlib\n",
        "code which produce high quality visualization of strong lenses.\n",
        "\n",
        "The `PLotter` API is described in the script `autolens_workspace/*/plot/start_here.py`.\n",
        "\n",
        "__Simulation__\n",
        "\n",
        "This script fits a simulated `Imaging` dataset of a strong lens, which is produced in the\n",
        "script `autolens_workspace/*/imaging/simulators/start_here.py`\n",
        "\n",
        "__Data Preparation__\n",
        "\n",
        "The `Imaging` dataset fitted in this example confirms to a number of standard that make it suitable to be fitted in\n",
        "**PyAutoLens**.\n",
        "\n",
        "If you are intending to fit your own strong lens data, you will need to ensure it conforms to these standards, which are\n",
        "described in the script `autolens_workspace/*/imaging/data_preparation/start_here.ipynb`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import autofit as af\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "Load the strong lens dataset `simple` via .fits files, which is a data format used by astronomers to store images.\n",
        "\n",
        "The `pixel_scales` define the arc-second to pixel conversion factor of the image, which for the dataset we are using \n",
        "is 0.1\" / pixel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"simple\"\n",
        "dataset_path = path.join(\"dataset\", \"imaging\", dataset_name)\n",
        "\n",
        "dataset = al.Imaging.from_fits(\n",
        "    data_path=path.join(dataset_path, \"data.fits\"),\n",
        "    psf_path=path.join(dataset_path, \"psf.fits\"),\n",
        "    noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "    pixel_scales=0.1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use an `ImagingPlotter` the plot the data, including: \n",
        "\n",
        " - `data`: The image of the strong lens.\n",
        " - `noise_map`: The noise-map of the image, which quantifies the noise in every pixel as their RMS values.\n",
        " - `psf`: The point spread function of the image, which describes the blurring of the image by the telescope optics.\n",
        " - `signal_to_noise_map`: Quantifies the signal-to-noise in every pixel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Mask__\n",
        "\n",
        "The model-fit requires a `Mask2D` defining the regions of the image we fit the lens model to the data. \n",
        "\n",
        "Below, we create a 3.0 arcsecond circular mask and apply it to the `Imaging` object that the lens model fits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mask = al.Mask2D.circular(\n",
        "    shape_native=dataset.shape_native, pixel_scales=dataset.pixel_scales, radius=3.0\n",
        ")\n",
        "\n",
        "dataset = dataset.apply_mask(mask=mask)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we plot the masked data, the mask removes the exterior regions of the image where there is no emission from the \n",
        "lens and lensed source galaxies.\n",
        "\n",
        "The mask used to fit the data can be customized, as described in \n",
        "the script `autolens_workspace/*/imaging/modeling/customize/mask.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "In this example we compose a lens model where:\n",
        "\n",
        " - The lens galaxy's light is a parametric `Sersic` bulge and `Exponential` disk, the centres of \n",
        " which are aligned [11 parameters].\n",
        " \n",
        " - The lens galaxy's total mass distribution is an `Isothermal` and `ExternalShear` [7 parameters].\n",
        " \n",
        " - The source galaxy's light is a parametric `Sersic` [7 parameters].\n",
        "\n",
        "The number of free parameters and therefore the dimensionality of non-linear parameter space is N=25.\n",
        "\n",
        "__Model Composition__\n",
        "\n",
        "The API below for composing a lens model uses the `Model` and `Collection` objects, which are imported from \n",
        "**PyAutoLens**'s parent project **PyAutoFit** \n",
        "\n",
        "The API is is fairly self explanatory and is straight forward to extend, for example adding more light profiles\n",
        "to the lens and source or using a different mass profile.\n",
        "\n",
        "__Model Cookbook__\n",
        "\n",
        "A full description of model composition, including lens model customization, is provided by the model cookbook: \n",
        "\n",
        "https://pyautolens.readthedocs.io/en/latest/general/model_cookbook.html\n",
        "\n",
        "__Coordinates__\n",
        "\n",
        "**PyAutoLens** assumes that the lens galaxy centre is near the coordinates (0.0\", 0.0\"). \n",
        "\n",
        "If for your dataset the lens is not centred at (0.0\", 0.0\"), we recommend that you either: \n",
        "\n",
        " - Reduce your data so that the centre is (`autolens_workspace/*/data_preparation`). \n",
        " - Manually override the lens model priors (`autolens_workspace/*/imaging/modeling/customize/priors.py`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Lens:\n",
        "\n",
        "bulge = af.Model(al.lp.Sersic)\n",
        "disk = af.Model(al.lp.Exponential)\n",
        "bulge.centre = disk.centre\n",
        "\n",
        "mass = af.Model(al.mp.Isothermal)\n",
        "\n",
        "shear = af.Model(al.mp.ExternalShear)\n",
        "\n",
        "lens = af.Model(al.Galaxy, redshift=0.5, bulge=bulge, disk=disk, mass=mass, shear=shear)\n",
        "\n",
        "# Source:\n",
        "\n",
        "source = af.Model(al.Galaxy, redshift=1.0, bulge=al.lp.Sersic)\n",
        "\n",
        "# Overall Lens Model:\n",
        "\n",
        "model = af.Collection(galaxies=af.Collection(lens=lens, source=source))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `info` attribute shows the model in a readable format.\n",
        "\n",
        "The `info` below may not display optimally on your computer screen, for example the whitespace between parameter\n",
        "names on the left and parameter priors on the right may lead them to appear across multiple lines. This is a\n",
        "common issue in Jupyter notebooks.\n",
        "\n",
        "The`info_whitespace_length` parameter in the file `config/general.yaml` in the [output] section can be changed to \n",
        "increase or decrease the amount of whitespace (The Jupyter notebook kernel will need to be reset for this change to \n",
        "appear in a notebook)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "The lens model is fitted to the data using a non-linear search. \n",
        "\n",
        "All examples in the autolens workspace use the nested sampling algorithm \n",
        "Dynesty (https://dynesty.readthedocs.io/en/latest/), which extensive testing has revealed gives the most accurate\n",
        "and efficient  modeling results.\n",
        "\n",
        "We make the following changes to the Dynesty settings:\n",
        "\n",
        " - Increase the number of live points, `nlive`, from the default value of 50 to 100. \n",
        " - Increase the number of random walks per live point, `walks` from the default value of 5 to 10. \n",
        "\n",
        "These are the two main Dynesty parameters that trade-off slower run time for a more reliable and accurate fit.\n",
        "Increasing both of these parameter produces a more reliable fit at the expense of longer run-times.\n",
        "\n",
        "__Customization__\n",
        "\n",
        "The folders `autolens_workspace/*/imaging/modeling/searches` gives an overview of alternative non-linear searches,\n",
        "other than Dynesty, that can be used to fit lens models. They also provide details on how to customize the\n",
        "model-fit, for example the priors.\n",
        "\n",
        "The `name` and `path_prefix` below specify the path where results ae stored in the output folder:  \n",
        "\n",
        " `/autolens_workspace/output/imaging/modeling/simple/light[bulge_disk]_mass[sie]_source[bulge]/unique_identifier`.\n",
        "\n",
        "__Unique Identifier__\n",
        "\n",
        "In the path above, the `unique_identifier` appears as a collection of characters, where this identifier is generated \n",
        "based on the model, search and dataset that are used in the fit.\n",
        " \n",
        "An identical combination of model and search generates the same identifier, meaning that rerunning the script will use \n",
        "the existing results to resume the model-fit. In contrast, if you change the model or search, a new unique identifier \n",
        "will be generated, ensuring that the model-fit results are output into a separate folder.\n",
        "\n",
        "We additionally want the unique identifier to be specific to the dataset fitted, so that if we fit different datasets\n",
        "with the same model and search results are output to a different folder. We achieve this below by passing \n",
        "the `dataset_name` to the search's `unique_tag`.\n",
        "\n",
        "__Number Of Cores__\n",
        "\n",
        "We include an input `number_of_cores`, which when above 1 means that Dynesty uses parallel processing to sample multiple \n",
        "lens models at once on your CPU. When `number_of_cores=2` the search will run roughly two times as\n",
        "fast, for `number_of_cores=3` three times as fast, and so on. The downside is more cores on your CPU will be in-use\n",
        "which may hurt the general performance of your computer.\n",
        "\n",
        "You should experiment to figure out the highest value which does not give a noticeable loss in performance of your \n",
        "computer. If you know that your processor is a quad-core processor you should be able to use `number_of_cores=4`. \n",
        "\n",
        "Above `number_of_cores=4` the speed-up from parallelization diminishes greatly. We therefore recommend you do not\n",
        "use a value above this.\n",
        "\n",
        "For users on a Windows Operating system, using `number_of_cores>1` may lead to an error, in which case it should be \n",
        "reduced back to 1 to fix it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.DynestyStatic(\n",
        "    path_prefix=path.join(\"imaging\", \"modeling\"),\n",
        "    name=\"light[bulge_disk]_mass[sie]_source[bulge]\",\n",
        "    unique_tag=dataset_name,\n",
        "    nlive=100,\n",
        "    walks=10,\n",
        "    number_of_cores=1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "We next create an `AnalysisImaging` object, which can be given many inputs customizing how the lens model is \n",
        "fitted to the data (in this example they are omitted for simplicity).\n",
        "\n",
        "Internally, this object defines the `log_likelihood_function` used by the non-linear search to fit the model to \n",
        "the `Imaging` dataset. \n",
        "\n",
        "It is not vital that you as a user understand the details of how the `log_likelihood_function` fits a lens model to \n",
        "data, but interested readers can find a step-by-step guide of the likelihood \n",
        "function at ``autolens_workspace/*/imaging/log_likelihood_function`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = al.AnalysisImaging(dataset=dataset)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Run Times__\n",
        "\n",
        "Lens modeling can be a computationally expensive process. When fitting complex models to high resolution datasets \n",
        "run times can be of order hours, days, weeks or even months.\n",
        "\n",
        "Run times are dictated by two factors:\n",
        "\n",
        " - The log likelihood evaluation time: the time it takes for a single `instance` of the lens model to be fitted to \n",
        "   the dataset such that a log likelihood is returned.\n",
        " \n",
        " - The number of iterations (e.g. log likelihood evaluations) performed by the non-linear search: more complex lens\n",
        "   models require more iterations to converge to a solution.\n",
        "   \n",
        "The log likelihood evaluation time can be estimated before a fit using the `profile_log_likelihood_function` method,\n",
        "which returns two dictionaries containing the run-times and information about the fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "run_time_dict, info_dict = analysis.profile_log_likelihood_function(\n",
        "    instance=model.random_instance()\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The overall log likelihood evaluation time is given by the `fit_time` key.\n",
        "\n",
        "For this example, it is ~0.01 seconds, which is extremely fast for lens modeling. More advanced lens\n",
        "modeling features (e.g. shapelets, multi Gaussian expansions, pixelizations) have slower log likelihood evaluation\n",
        "times (1-3 seconds), and you should be wary of this when using these features.\n",
        "\n",
        "Feel free to go ahead a print the full `run_time_dict` and `info_dict` to see the other information they contain. The\n",
        "former has a break-down of the run-time of every individual function call in the log likelihood function, whereas the \n",
        "latter stores information about the data which drives the run-time (e.g. number of image-pixels in the mask, the\n",
        "shape of the PSF, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Log Likelihood Evaluation Time (second) = {run_time_dict['fit_time']}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To estimate the expected overall run time of the model-fit we multiply the log likelihood evaluation time by an \n",
        "estimate of the number of iterations the non-linear search will perform. \n",
        "\n",
        "Estimating this quantity is more tricky, as it varies depending on the lens model complexity (e.g. number of parameters)\n",
        "and the properties of the dataset and model being fitted.\n",
        "\n",
        "For this example, we conservatively estimate that the non-linear search will perform ~10000 iterations per free \n",
        "parameter in the model. This is an upper limit, with models typically converging in far fewer iterations.\n",
        "\n",
        "If you perform the fit over multiple CPUs, you can divide the run time by the number of cores to get an estimate of\n",
        "the time it will take to fit the model. However, above ~6 cores the speed-up from parallelization is less efficient and\n",
        "does not scale linearly with the number of cores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    \"Estimated Run Time Upper Limit (seconds) = \",\n",
        "    (run_time_dict[\"fit_time\"] * model.total_free_parameters * 10000)\n",
        "    / search.number_of_cores,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model-Fit__\n",
        "\n",
        "We can now begin the model-fit by passing the model and analysis object to the search, which performs the \n",
        "dynesty non-linear search in order to find which models fit the data with the highest likelihood.\n",
        "\n",
        "__Output Folder__\n",
        "\n",
        "Now this is running you should checkout the `autolens_workspace/output` folder.\n",
        "\n",
        "This is where the results of the search are written to your hard-disk (in the `tutorial_1_non_linear_search` folder). \n",
        "When its completed, images, results and information about the fit appear in this folder, meaning that you don't need \n",
        "to keep running Python code to see the result.\n",
        "\n",
        "__On The Fly Outputs__\n",
        "\n",
        "Even when the search is running, information about the highest likelihood model inferred by the search so-far \n",
        "is output to this folder on-the-fly. \n",
        "\n",
        "If you navigate to the folder: \n",
        "\n",
        " `output/howtolens/chapter_1/tutorials_1_non_linear_search/unique_identifier` \n",
        " \n",
        "Even before the search has finished, you will see:\n",
        "\n",
        " 1) The `images` folder, where images of the highest likelihood lens model are output on-the-fly. This includes the\n",
        " `FitImaging` subplot we plotted in the previous chapter, which therefore gives a real sense of 'how good' the model\n",
        " fit is.\n",
        " \n",
        " 2) The `samples` folder, which contains a `.csv` table of every sample of the non-linear search as well as other \n",
        " information. \n",
        " \n",
        " 3) The `model.info` file, which lists the lens model, its parameters and their priors (discussed in the next tutorial).\n",
        " \n",
        " 4) The `model.results` file, which lists the highest likelihood lens model and the most probable lens model with \n",
        " errors (this outputs on-the-fly).\n",
        " \n",
        " 5) The `search.summary` file, which provides a summary of the non-linear search settings and statistics on how well\n",
        " it is performing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "The search returns a result object, which whose `info` attribute shows the result in a readable format.\n",
        "\n",
        "[Above, we discussed that the `info_whitespace_length` parameter in the config files could b changed to make \n",
        "the `model.info` attribute display optimally on your computer. This attribute also controls the whitespace of the\n",
        "`result.info` attribute.]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Result` object also contains:\n",
        "\n",
        " - The model corresponding to the maximum log likelihood solution in parameter space.\n",
        " - The corresponding maximum log likelihood `Plane` and `FitImaging` objects.\n",
        " - Information on the posterior as estimated by the `Dynesty` non-linear search. \n",
        " \n",
        "Checkout `autolens_workspace/*/imaging/results` for a full description of analysing results in **PyAutoLens**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.max_log_likelihood_instance)\n",
        "\n",
        "tracer_plotter = aplt.TracerPlotter(\n",
        "    tracer=result.max_log_likelihood_tracer, grid=result.grid\n",
        ")\n",
        "tracer_plotter.subplot_tracer()\n",
        "\n",
        "fit_plotter = aplt.FitImagingPlotter(fit=result.max_log_likelihood_fit)\n",
        "fit_plotter.subplot_fit()\n",
        "\n",
        "search_plotter = aplt.DynestyPlotter(samples=result.samples)\n",
        "search_plotter.cornerplot()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This script gives a concise overview of the PyAutoLens modeling API, fitting one the simplest lens models possible.\n",
        "So, what next? \n",
        "\n",
        "__Features__\n",
        "\n",
        "The following examples in the `modeling` package illustrate the advanced features of PyAutoLens's lens modeling:\n",
        "\n",
        "- `autolens_workspace/*/imaging/modeling/features`: using PyAutoLens's advanced lens modeling \n",
        "  features, such as pixelized source reconstructions, multi Gaussian lens light models and fitting double Einstein rings.\n",
        "\n",
        "- `autolens_workspace/*/imaging/modeling/searches`: using different non-linear searches to fit lens models, such as\n",
        "  the MCMC algorithm `Emcee`, as well as how to customize the priors of the lens model.\n",
        "\n",
        "- `autolens_workspace/*/imaging/modeling/customize`: customizing how the model is fitted to data via the analysis \n",
        "  object, for example using custom masks.\n",
        "\n",
        "__HowToLens__\n",
        "\n",
        "This `start_here.py` script, and the features examples above, do not explain many details of how lens modeling is \n",
        "performed, for example:\n",
        "\n",
        " - How does PyAutoLens perform ray-tracing and lensing calculations in order to fit a lens model?\n",
        " - How is a lens model fitted to data? What quantifies the goodness of fit (e.g. how is a log likelihood computed?).\n",
        " - How does dynesty find the highest likelihood lens models? What exactly is a \"non-linear search\"?\n",
        "\n",
        "You do not need to be able to answer these questions in order to fit lens models with PyAutoLens and do science.\n",
        "However, having a deeper understanding of how it all works is both interesting and will benefit you as a scientist\n",
        "\n",
        "This deeper insight is offered by the **HowToLens** Jupyter notebook lectures, found \n",
        "at `autolens_workspace/*/howtolens`. \n",
        "\n",
        "I recommend that you check them out if you are interested in more details!\n",
        "\n",
        "__Data Preparation__\n",
        "\n",
        "You may now be looking to fit your own CD imaging data of a strong lens. \n",
        "\n",
        "In this case, you should checkout the `autolens_workspace/*/imaging/data_preparation/start_here.ipynb` script for an\n",
        "overview of how data should be prepared before being modeled via **PyAutoLens**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}