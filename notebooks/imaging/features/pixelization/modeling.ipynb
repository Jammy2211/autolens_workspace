{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Features: Pixelization\n",
        "======================\n",
        "\n",
        "A pixelization reconstructs the source's light using a pixel-grid, which is regularized using a prior that forces\n",
        "the solution to have a degree of smoothness.\n",
        "\n",
        "This script fits a source galaxy model which uses a pixelization to reconstruct the source's light.\n",
        "\n",
        "A Voronoi mesh and constant regularization scheme are used, which are the simplest forms of mesh and regularization\n",
        "with provide computationally fast and accurate solutions in **PyAutoLens**.\n",
        "\n",
        "For simplicity, the lens galaxy's light is omitted from the model and is not present in the simulated data. It is\n",
        "straightforward to include the lens galaxy's light in the model.\n",
        "\n",
        "You may wish to first read the `pixelization/fit.py` example in order to see how a pixelization is fitted\n",
        "to an individual data.\n",
        "\n",
        "Pixelizations are covered in detail in chapter 4 of the **HowToLens** lectures.\n",
        "\n",
        "__JAX GPU Run Times__\n",
        "\n",
        "Pixelizations run time depends on how modern GPU hardware is. GPU acceleration only provides fast run times on\n",
        "modern GPUs with large amounts of VRAM, or when the number of pixels in the mesh are low (e.g. < 500 pixels).\n",
        "\n",
        "This script's default setup uses an adaptive 20 x 20 rectangular mesh (400 pixels), which is relatively low resolution\n",
        "and may not provide the most accurate lens modeling results. On most GPU hardware it will run in ~ 10 minutes,\n",
        "however if your laptop has a large VRAM (GPU > 20 GB) or you can access a GPU cluster with better hardware you should use these\n",
        "to perform modeling with increased mesh resolution.\n",
        "\n",
        "__CPU Run Times__\n",
        "\n",
        "JAX is not natively designed to provide significant CPU speed up, therefore users using CPUs to perform pixelization\n",
        "analysis will not see fast run times using JAX (unlike GPUs).\n",
        "\n",
        "The example `pixelization/cpu` shows how to set up a pixelization to use efficient CPU calculations via the library\n",
        "`numba`.\n",
        "\n",
        "__Contents__\n",
        "\n",
        "**Advantages & Disadvantages:** Benefits and drawbacks of using an MGE.\n",
        "**Positive Only Solver:** How a positive solution to the light profile intensities is ensured.\n",
        "**Chaining:** How the advanced modeling feature, non-linear search chaining, can significantly improve lens modeling with pixelizaitons.\n",
        "**Dataset & Mask:** Standard set up of imaging dataset that is fitted.\n",
        "**Pixelization:** How to create a pixelization, including a description of its inputs.\n",
        "**Model:** Composing a model using a pixelization and how it changes the number of free parameters.\n",
        "**Search & Analysis:** Standard set up of non-linear search and analysis.\n",
        "**Positions Likelihood:** Removing unphysical pixelized source solutions using a likelihood penalty using the lensed multiple images.\n",
        "**Run Time:** Profiling of pixelization run times and discussion of how they compare to standard light profiles.\n",
        "**Model-Fit:** Performs the model fit using standard API.\n",
        "**Result:** Pixelization results and visualizaiton.\n",
        "**Interpolated Source:** Interpolate the source reconstruction from an irregular Voronoi mesh to a uniform square grid and output to a .fits file.\n",
        "**Reconstruction CSV:** Output the source reconstruction to a .csv file, which can be used to perform calculations on the source reconstruction.\n",
        "**Voronoi:** Using a Voronoi mesh pixelizaiton (instead of Voronoi), which allows one to manipulate the source reconstruction without autolens installed.\n",
        "**Result (Advanced):** API for various pixelization outputs (magnifications, mappings) which requires some polishing.\n",
        "**Simulate (Advanced):** Simulating a strong lens dataset with the inferred pixelized source.\n",
        "\n",
        "__Advantages__\n",
        "\n",
        "Many strongly lensed source galaxies are complex, and have asymmetric and irregular morphologies. These morphologies\n",
        "cannot be well approximated by a parametric light profiles like a Sersic, or many Sersics, and thus a pixelization\n",
        "is required to reconstruct the source's irregular light.\n",
        "\n",
        "Even basis functions like shapelets or a multi-Gaussian expansion cannot reconstruct a source-plane accurately\n",
        "if there are multiple source galaxies, or if the source galaxy has a very complex morphology.\n",
        "\n",
        "To infer detailed components of a lens mass model (e.g. its density slope, whether there's a dark matter subhalo, etc.)\n",
        "then pixelized source models are required, to ensure the mass model is fitting all of the lensed source light.\n",
        "\n",
        "There are also many science cases where one wants to study the highly magnified light of the source galaxy in detail,\n",
        "to learnt about distant and faint galaxies. A pixelization reconstructs the source's unlensed emission and thus\n",
        "enables this.\n",
        "\n",
        "__Disadvantages__\n",
        "\n",
        "Pixelizations are computationally slow and run times are typically longer than a parametric source model. It is not\n",
        "uncommon for lens models using a pixelization to take hours or even days to fit high resolution imaging\n",
        "data (e.g. Hubble Space Telescope imaging).\n",
        "\n",
        "Lens modeling with pixelizations is also more complex than parametric source models, with there being more things\n",
        "that can go wrong. For example, there are solutions where a demagnified version of the lensed source galaxy is\n",
        "reconstructed, using a mass model which effectively has no mass or too much mass. These are described in detail below.\n",
        "\n",
        "It will take you longer to learn how to successfully fit lens models with a pixelization than other methods illustrated\n",
        "in the workspace!\n",
        "\n",
        "__Positive Only Solver__\n",
        "\n",
        "Many codes which use linear algebra typically rely on a linear algabra solver which allows for positive and negative\n",
        "values of the solution (e.g. `np.linalg.solve`), because they are computationally fast.\n",
        "\n",
        "This is problematic, as it means that negative surface brightnesses values can be computed to represent a galaxy's\n",
        "light, which is clearly unphysical. For a pixelizaiton, this often produces negative source pixels which over-fit\n",
        "the data, producing unphysical solutions.\n",
        "\n",
        "All pixelized source reconstructions use a positive-only solver, meaning that every source-pixel is only allowed\n",
        "to reconstruct positive flux values. This ensures that the source reconstruction is physical and that we don't\n",
        "reconstruct negative flux values that don't exist in the real source galaxy (a common systematic solution in lens\n",
        "analysis).\n",
        "\n",
        "It may be surprising to hear that this is a feature worth pointing out, but it turns out setting up the linear algebra\n",
        "to enforce positive reconstructions is difficult to make efficient. A lot of development time went into making this\n",
        "possible, where a bespoke fast non-negative linear solver was developed to achieve this.\n",
        "\n",
        "Other methods in the literature often do not use a positive only solver, and therefore suffer from these\n",
        "unphysical solutions, which can degrade the results of lens model in general.\n",
        "\n",
        "__Chaining__\n",
        "\n",
        "Due to the complexity of fitting with a pixelization, it is often best to use **PyAutoLens**'s non-linear chaining\n",
        "feature to compose a pipeline which begins by fitting a simpler model using a parametric source.\n",
        "\n",
        "More information on chaining is provided in the `autolens_workspace/notebooks/imaging/advanced/chaining` folder,\n",
        "chapter 3 of the **HowToLens** lectures.\n",
        "\n",
        "The script `autolens_workspace/scripts/imaging/advanced/chaining/parametric_to_pixelization.py` explitly uses chaining\n",
        "to link a lens model using a light profile source to one which then uses a pixelization.\n",
        "\n",
        "__Model__\n",
        "\n",
        "This script fits an `Imaging` dataset of a 'galaxy-scale' strong lens with a model where:\n",
        "\n",
        " - The lens galaxy's light is omitted (and is not present in the simulated data).\n",
        " - The lens galaxy's total mass distribution is an `Isothermal` and `ExternalShear`.\n",
        " - The source galaxy's surface-brightness is reconstructed using a `Voronoi` mesh, `Overlay` image-mesh\n",
        "   and `Constant` regularization scheme.\n",
        "\n",
        "__Start Here Notebook__\n",
        "\n",
        "If any code in this script is unclear, refer to the `modeling/start_here.ipynb` notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import autofit as af\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "Load and plot the strong lens dataset `simple__no_lens_light` via .fits files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"simple__no_lens_light\"\n",
        "dataset_path = Path(\"dataset\") / \"imaging\" / dataset_name\n",
        "\n",
        "dataset = al.Imaging.from_fits(\n",
        "    data_path=dataset_path / \"data.fits\",\n",
        "    psf_path=dataset_path / \"psf.fits\",\n",
        "    noise_map_path=dataset_path / \"noise_map.fits\",\n",
        "    pixel_scales=0.1,\n",
        ")\n",
        "\n",
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Mask__\n",
        "\n",
        "Define a 3.0\" circular mask, which includes the emission of the lens and source galaxies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mask_radius = 3.0\n",
        "\n",
        "mask = al.Mask2D.circular(\n",
        "    shape_native=dataset.shape_native,\n",
        "    pixel_scales=dataset.pixel_scales,\n",
        "    radius=mask_radius,\n",
        ")\n",
        "\n",
        "dataset = dataset.apply_mask(mask=mask)\n",
        "\n",
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Over Sampling__\n",
        "\n",
        "A pixelization uses a separate grid for ray tracing, with its own over sampling scheme, which below we set to a \n",
        "uniform grid of values of 2. \n",
        "\n",
        "The pixelization only reconstructs the source galaxy, therefore the adaptive over sampling used for the lens galaxy's \n",
        "light in other examples is not applied to the pixelization. \n",
        "\n",
        "This example does not model lens light, for examples which combine lens light and a pixelization both over sampling \n",
        "schemes should be used, with the lens light adaptive and the pixelization uniform.\n",
        "\n",
        "Note that the over sampling is input into the `over_sample_size_pixelization` because we are using a `Pixelization`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset = dataset.apply_over_sampling(\n",
        "    over_sample_size_pixelization=4,\n",
        ")\n",
        "\n",
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__JAX & Preloads__\n",
        "\n",
        "In JAX, calculations must use static shaped arrays with known and fixed indexes. For certain calculations in the\n",
        "pixelization, this information has to be passed in before the pixelization is performed. Below, we do this for 3\n",
        "inputs:\n",
        "\n",
        "- `total_linear_light_profiles`: The number of linear light profiles in the model. This is 0 because we are not\n",
        "  fitting any linear light profiles to the data, primarily because the lens light is omitted.\n",
        "\n",
        "- `total_mapper_pixels`: The number of source pixels in the rectangular pixelization mesh. This is required to set up \n",
        "  the arrays that perform the linear algebra of the pixelization.\n",
        "\n",
        "- `source_pixel_zeroed_indices`: The indices of source pixels on its edge, which when the source is reconstructed \n",
        "  are forced to values of zero, a technique tests have shown are required to give accruate lens models.\n",
        "\n",
        "The `image_mesh` can be ignored, it is legacy API from previous versions which may or may not be reintegrated in future\n",
        "versions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "image_mesh = None\n",
        "mesh_shape = (20, 20)\n",
        "total_mapper_pixels = mesh_shape[0] * mesh_shape[1]\n",
        "\n",
        "total_linear_light_profiles = 0\n",
        "\n",
        "preloads = al.Preloads(\n",
        "    mapper_indices=al.mapper_indices_from(\n",
        "        total_linear_light_profiles=total_linear_light_profiles,\n",
        "        total_mapper_pixels=total_mapper_pixels,\n",
        "    ),\n",
        "    source_pixel_zeroed_indices=al.util.mesh.rectangular_edge_pixel_list_from(\n",
        "        total_linear_light_profiles=total_linear_light_profiles,\n",
        "        shape_native=mesh_shape,\n",
        "    ),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "We compose our lens model using `Model` objects, which represent the galaxies we fit to our data.  In this \n",
        "example fits a lens model where:\n",
        "\n",
        " - The lens galaxy's total mass distribution is an `Isothermal` and `ExternalShear` [7 parameters].\n",
        "\n",
        " - The source-galaxy's light uses a `Voronoi` mesh [0 parameters].\n",
        "\n",
        " - The mesh centres of the `Voronoi` mesh are computed using a `Overlay` image-mesh, with a fixed resolution of \n",
        "   30 x 30 pixels [0 parameters].\n",
        "\n",
        " - This pixelization is regularized using a `Constant` scheme which smooths every source pixel equally [1 parameter]. \n",
        "\n",
        "The number of free parameters and therefore the dimensionality of non-linear parameter space is N=6. \n",
        "\n",
        "It is worth noting the `Pixelization`  use significantly fewer parameters (1 parameter) than \n",
        "fitting the source using `LightProfile`'s (7+ parameters). \n",
        "\n",
        "The lens model therefore includes a mesh and regularization scheme, which are used together to create the \n",
        "pixelization. \n",
        "\n",
        "__Model Cookbook__\n",
        "\n",
        "A full description of model composition is provided by the model cookbook: \n",
        "\n",
        "https://pyautolens.readthedocs.io/en/latest/general/model_cookbook.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Lens:\n",
        "\n",
        "mass = af.Model(al.mp.PowerLaw)\n",
        "shear = af.Model(al.mp.ExternalShear)\n",
        "\n",
        "lens = af.Model(al.Galaxy, redshift=0.5, mass=mass, shear=shear)\n",
        "\n",
        "# Source:\n",
        "mesh = af.Model(al.mesh.RectangularMagnification, shape=mesh_shape)\n",
        "regularization = af.Model(al.reg.Constant)\n",
        "\n",
        "pixelization = af.Model(\n",
        "    al.Pixelization, image_mesh=image_mesh, mesh=mesh, regularization=regularization\n",
        ")\n",
        "\n",
        "source = af.Model(al.Galaxy, redshift=1.0, pixelization=pixelization)\n",
        "\n",
        "# Overall Lens Model:\n",
        "\n",
        "model = af.Collection(galaxies=af.Collection(lens=lens, source=source))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `info` attribute shows the model in a readable format (if this does not display clearly on your screen refer to\n",
        "`start_here.ipynb` for a description of how to fix this).\n",
        "\n",
        "This confirms that the source galaxy's has a mesh and regularization scheme, which are combined into a pixelization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "The model is fitted to the data using the nested sampling algorithm Nautilus (see `start.here.py` for a \n",
        "full description)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.Nautilus(\n",
        "    path_prefix=Path(\"features\"),\n",
        "    name=\"pixelization\",\n",
        "    unique_tag=dataset_name,\n",
        "    n_live=100,\n",
        "    n_batch=20,\n",
        "    iterations_per_quick_update=50000,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Position Likelihood__\n",
        "\n",
        "We add a penalty term ot the likelihood function, which penalizes models where the brightest multiple images of\n",
        "the lensed source galaxy do not trace close to one another in the source plane. This removes \"demagnified source\n",
        "solutions\" from the source pixelization, which one is likely to infer without this penalty.\n",
        "\n",
        "A comprehensive description of why we do this is given at the following readthedocs page. I strongly recommend you \n",
        "read this page in full if you are not familiar with the positions likelihood penalty and demagnified source \n",
        "reconstructions:\n",
        "\n",
        " https://pyautolens.readthedocs.io/en/latest/general/demagnified_solutions.html\n",
        "\n",
        "__Brief Description__\n",
        "\n",
        "Unlike other example scripts, we also pass the `AnalysisImaging` object below a `PositionsLH` object, which\n",
        "includes the positions we loaded above, alongside a `threshold`.\n",
        "\n",
        "This is because `Inversion`'s suffer a bias whereby they fit unphysical lens models where the source galaxy is \n",
        "reconstructed as a demagnified version of the lensed source. \n",
        "\n",
        "To prevent these solutions biasing the model-fit we specify a `position_threshold` of 0.5\", which requires that a \n",
        "mass model traces the four (y,x) coordinates specified by our positions (that correspond to the brightest regions of the \n",
        "lensed source) within 0.5\" of one another in the source-plane. If this criteria is not met, a large penalty term is\n",
        "added to likelihood that massively reduces the overall likelihood. This penalty is larger if the ``positions``\n",
        "trace further from one another.\n",
        "\n",
        "This ensures the unphysical solutions that bias a pixelization have a lower likelihood that the physical solutions\n",
        "we desire. Furthermore, the penalty term reduces as the image-plane multiple image positions trace closer in the \n",
        "source-plane, ensuring Nautilus converges towards an accurate mass model. It does this very fast, as \n",
        "ray-tracing just a few multiple image positions is computationally cheap. \n",
        "\n",
        "The threshold of 0.3\" is large. For an accurate lens model we would anticipate the positions trace within < 0.01\" of\n",
        "one another. The high threshold ensures only the initial mass models at the start of the fit are penalized.\n",
        "\n",
        "Position thresholding is described in more detail in the \n",
        "script `autolens_workspace/*/guides/modeling/customize`\n",
        "\n",
        "The arc-second positions of the multiply imaged lensed source galaxy were drawn onto the\n",
        "image via the GUI described in the file `autolens_workspace/*/data_preparation/imaging/gui/positions.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "positions = al.Grid2DIrregular(\n",
        "    al.from_json(file_path=Path(dataset_path, \"positions.json\"))\n",
        ")\n",
        "\n",
        "positions_likelihood = al.PositionsLH(positions=positions, threshold=0.3)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "Create the `AnalysisImaging` object defining how the via Nautilus the model is fitted to the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = al.AnalysisImaging(\n",
        "    dataset=dataset, positions_likelihood_list=[positions_likelihood], preloads=preloads\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Run Time__\n",
        "\n",
        "The run time of a pixelization are fast provided that the GPU VRAM exceeds the amount of memory required to perform\n",
        "a likelihood evaluation.\n",
        "\n",
        "Assuming the use of a 20 x 20 mesh grid above means this is the case, the run times of this model-fit on a GPU\n",
        "should take under 10 minutes. If VRAM is exceeded, the run time will be significantly longer (3+ hours). CPU run\n",
        "times are also of order hours, but can be sped up using the `numba` library (see the `pixelization/cpu` example).\n",
        "\n",
        "The run times of pixelizations slow down as the data becomes higher resolution. In this example, data with a pixel\n",
        "scale of 0.1\" gives of order 10 minute run times (when VRAM is under control), for a pixel scale of 0.05\" this\n",
        "becomes around 30 minutes, and an hour for 0.03\".\n",
        "\n",
        "__Model-Fit__\n",
        "\n",
        "We begin the model-fit by passing the model and analysis object to the non-linear search (checkout the output folder\n",
        "for on-the-fly visualization and results)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "The search returns a result object, which whose `info` attribute shows the result in a readable format (if this\n",
        "does not display clearly on your screen refer to `start_here.ipynb` for a description of how to fix this):\n",
        "\n",
        "This confirms that the source galaxy's has a mesh and regularization scheme, which are combined into a pixelization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We plot the maximum likelihood fit, tracer images and posteriors inferred via Nautilus.\n",
        "\n",
        "The end of this example provides a detailed description of all result options for a pixelization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.max_log_likelihood_instance)\n",
        "\n",
        "tracer_plotter = aplt.TracerPlotter(\n",
        "    tracer=result.max_log_likelihood_tracer, grid=result.grids.lp\n",
        ")\n",
        "tracer_plotter.subplot_tracer()\n",
        "\n",
        "fit_plotter = aplt.FitImagingPlotter(fit=result.max_log_likelihood_fit)\n",
        "fit_plotter.subplot_fit()\n",
        "\n",
        "plotter = aplt.NestPlotter(samples=result.samples)\n",
        "plotter.corner_anesthetic()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The example `pixelization/fit` provides a full description of the different calculations that can be performed\n",
        "with the result of a pixelization model-fit.\n",
        "\n",
        "__Mask Extra Galaxies__\n",
        "\n",
        "There may be extra galaxies nearby the lens and source galaxies, whose emission blends with the lens and source.\n",
        "\n",
        "If their emission is significant, and close enough to the lens and source, we may simply remove it from the data\n",
        "to ensure it does not impact the model-fit. A standard masking approach would be to remove the image pixels containing\n",
        "the emission of these galaxies altogether. This is analogous to what the circular masks used throughout the examples\n",
        "does.\n",
        "\n",
        "For fits using a pixelization, masking regions of the image in a way that removes their image pixels entirely from\n",
        "the fit. This can produce discontinuities in the pixelixation used to reconstruct the source and produce unexpected\n",
        "systematics and unsatisfactory results. In this case, applying the mask in a way where the image pixels are not\n",
        "removed from the fit, but their data and noise-map values are scaled such that they contribute negligibly to the fit,\n",
        "is a better approach.\n",
        "\n",
        "We illustrate the API for doing this below, using the `extra_galaxies` dataset which has extra galaxies whose emission\n",
        "needs to be removed via scaling in this way. We apply the scaling and show the subplot imaging where the extra\n",
        "galaxies mask has scaled the data values to zeros, increasing the noise-map values to large values and in turn made\n",
        "the signal to noise of its pixels effectively zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"extra_galaxies\"\n",
        "dataset_path = Path(\"dataset\") / \"imaging\" / dataset_name\n",
        "\n",
        "dataset = al.Imaging.from_fits(\n",
        "    data_path=dataset_path / \"data.fits\",\n",
        "    psf_path=dataset_path / \"psf.fits\",\n",
        "    noise_map_path=dataset_path / \"noise_map.fits\",\n",
        "    pixel_scales=0.1,\n",
        ")\n",
        "\n",
        "mask_extra_galaxies = al.Mask2D.from_fits(\n",
        "    file_path=Path(dataset_path, \"mask_extra_galaxies.fits\"),\n",
        "    pixel_scales=0.1,\n",
        "    invert=True,  # Note that we invert the mask here as `True` means a pixel is scaled.\n",
        ")\n",
        "\n",
        "dataset = dataset.apply_noise_scaling(mask=mask_extra_galaxies)\n",
        "\n",
        "mask = al.Mask2D.circular(\n",
        "    shape_native=dataset.shape_native, pixel_scales=0.1, centre=(0.0, 0.0), radius=6.0\n",
        ")\n",
        "\n",
        "dataset = dataset.apply_mask(mask=mask)\n",
        "\n",
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We do not explictly fit this data, for the sake of brevity, however if your data has these nearby galaxies you should\n",
        "apply the mask as above before fitting the data.\n",
        "\n",
        "__Result Use__\n",
        "\n",
        "There are many things you can do with the result of a pixelixaiton, including analysing the reconstructing source, \n",
        "magnification calculations of the source and much more.\n",
        "\n",
        "These are documented in the `fit.py` example.\n",
        "\n",
        "This example centres on the `inversion` object which is easily accessed from the result of a model-fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "inversion = result.max_log_likelihood_fit.inversion\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Reconstruction CSV__\n",
        "\n",
        "In the results `image` folder there is a .csv file called `source_plane_reconstruction_0.csv` which contains the\n",
        "y and x coordinates of the pixelization mesh, the reconstruct values and the noise map of these values.\n",
        "\n",
        "This file is provides all information on the source reconstruciton in a format that does not depend autolens\n",
        "and therefore be easily loaded to create images of the source or shared collaobrations who do not have PyAutoLens\n",
        "installed.\n",
        "\n",
        "First, lets load `source_plane_reconstruction_0.csv` as a dictionary, using basic `csv` functionality in Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import csv\n",
        "\n",
        "with open(\n",
        "        search.paths.image_path / \"source_plane_reconstruction_0.csv\", mode=\"r\"\n",
        ") as file:\n",
        "    reader = csv.reader(file)\n",
        "    header_list = next(reader)  # ['y', 'x', 'reconstruction', 'noise_map']\n",
        "\n",
        "    reconstruction_dict = {header: [] for header in header_list}\n",
        "\n",
        "    for row in reader:\n",
        "        for key, value in zip(header_list, row):\n",
        "            reconstruction_dict[key].append(float(value))\n",
        "\n",
        "    # Convert lists to NumPy arrays\n",
        "    for key in reconstruction_dict:\n",
        "        reconstruction_dict[key] = np.array(reconstruction_dict[key])\n",
        "\n",
        "print(reconstruction_dict[\"y\"])\n",
        "print(reconstruction_dict[\"x\"])\n",
        "print(reconstruction_dict[\"reconstruction\"])\n",
        "print(reconstruction_dict[\"noise_map\"])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can now use standard libraries to performed calculations with the reconstruction on the mesh, again avoiding\n",
        "the need to use autolens.\n",
        "\n",
        "For example, we can create a RectangularMagnification mesh using the scipy.spatial library, which is a triangulation\n",
        "of the y and x coordinates of the pixelization mesh. This is useful for visualizing the pixelization\n",
        "and performing calculations on the mesh."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import scipy\n",
        "\n",
        "points = np.stack(arrays=(reconstruction_dict[\"x\"], reconstruction_dict[\"y\"]), axis=-1)\n",
        "\n",
        "mesh = scipy.spatial.RectangularMagnification(points)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interpolating the result to a uniform grid is also possible using the scipy.interpolate library, which means the result\n",
        "can be turned into a uniform 2D image which can be useful to analyse the source with tools which require an uniform grid.\n",
        "\n",
        "Below, we interpolate the result onto a 201 x 201 grid of pixels with the extent spanning -1.0\" to 1.0\", which\n",
        "capture the majority of the source reconstruction without being too high resolution.\n",
        "\n",
        "It should be noted this inteprolation may not be as optimal as the interpolation perforemd above using `MapperValued`, \n",
        "which uses specifc interpolation methods for a RectangularMagnification mesh which are more accurate, but it should be sufficent for\n",
        "most use-cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy.interpolate import griddata\n",
        "\n",
        "values = reconstruction_dict[\"reconstruction\"]\n",
        "\n",
        "interpolation_grid = al.Grid2D.from_extent(\n",
        "    extent=(-1.0, 1.0, -1.0, 1.0), shape_native=(201, 201)\n",
        ")\n",
        "\n",
        "interpolated_array = griddata(points=points, values=values, xi=interpolation_grid)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Wrap Up__\n",
        "\n",
        "Pixelizations are the most complex but also most powerful way to model a source galaxy.\n",
        "\n",
        "Whether you need to use them or not depends on the science you are doing. If you are only interested in measuring a\n",
        "simple quantity like the Einstein radius of a lens, you can get away with using light profiles like a Sersic, MGE or \n",
        "shapelets to model the source. Low resolution data also means that using a pixelization is not necessary, as the\n",
        "complex structure of the source galaxy is not resolved anyway.\n",
        "\n",
        "However, fitting complex mass models (e.g. a power-law, stellar / dark model or dark matter substructure) requires \n",
        "this level of complexity in the source model. Furthermore, if you are interested in studying the properties of the\n",
        "source itself, you won't find a better way to do this than using a pixelization.\n",
        "\n",
        "__Chaining__\n",
        "\n",
        "If your pixelization fit does not go well, or you want for faster computational run-times, you may wish to use\n",
        "search chaining which breaks the model-fit into multiple Nautilus runs. This is described for the specific case of \n",
        "linking a (computationally fast) light profile fit to a pixelization in the script:\n",
        "\n",
        "`autolens_workspace/scripts/imaging/advanced/chaining/parametric_to_pixelization.py`\n",
        "\n",
        "__HowToLens__\n",
        "\n",
        "A full description of how pixelizations work, which comes down to a lot of linear algebra, Bayesian statistics and\n",
        "2D geometry, is provided in chapter 4 of the **HowToLens** lectures.\n",
        "\n",
        "__Future Ideas / Contributions__\n",
        "\n",
        "Here are a list of things I would like to add to this tutorial but haven't found the time. If you are interested\n",
        "in having a go at adding them contact me on SLACK! :)\n",
        "\n",
        "- More magnification calculations.\n",
        "- Source gradient calculations.\n",
        "- A calculation which shows differential lensing effects (e.g. magnification across the source plane)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}