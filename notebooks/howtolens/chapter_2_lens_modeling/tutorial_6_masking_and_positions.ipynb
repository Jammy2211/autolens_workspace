{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 6: Masking and Positions\n",
        "=================================\n",
        "\n",
        "We have learnt everything we need to know about non-linear searches to model a strong lens and infer a good lens\n",
        "model solution. Now, lets consider masking in more detail, something we have not given much consideration previously.\n",
        "We'll also learn a neat trick to improve the speed and accuracy of a non-linear search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import autolens as al\n",
        "import autolens.plot as aplt\n",
        "import autofit as af"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Initial Setup__\n",
        "\n",
        "we'll use the same strong lensing data as tutorials 1 & 2, where:\n",
        "\n",
        " - The lens galaxy's total mass distribution is a `IsothermalSph`.\n",
        " - The source galaxy's light is a `ExponentialSph`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"simple__no_lens_light__mass_sis\"\n",
        "dataset_path = path.join(\"dataset\", \"imaging\", dataset_name)\n",
        "\n",
        "dataset = al.Imaging.from_fits(\n",
        "    data_path=path.join(dataset_path, \"data.fits\"),\n",
        "    noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "    psf_path=path.join(dataset_path, \"psf.fits\"),\n",
        "    pixel_scales=0.1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Mask__\n",
        "\n",
        "In this tutorial, our data only contained the light from the lensed source. We therefore want our mask to include all \n",
        "of the source's light. There are different shaped masks that we can use to achieve this, and we can compare them by\n",
        "plotting masks on the imaging data.\n",
        "\n",
        "Below, we use a 'circular_annular' mask, which unlike the 'circular' masks we have used previous includes an additional\n",
        "input parameter 'inner_radius' that removes the central regions of the image. Below, we use too large of a value of\n",
        "`inner_radius=1.4`, which cuts into the lensed source galaxy's light, clearly this isn't a good mask."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mask = al.Mask2D.circular_annular(\n",
        "    shape_native=dataset.shape_native,\n",
        "    pixel_scales=dataset.pixel_scales,\n",
        "    inner_radius=1.4,\n",
        "    outer_radius=2.7,\n",
        ")\n",
        "\n",
        "dataset = dataset.apply_mask(mask=mask)\n",
        "\n",
        "dataset_plotter = aplt.ImagingPlotter(\n",
        "    dataset=dataset, visuals_2d=aplt.Visuals2D(mask=mask)\n",
        ")\n",
        "dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can decrease the `inner_radius` to correct for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mask = al.Mask2D.circular_annular(\n",
        "    shape_native=dataset.shape_native,\n",
        "    pixel_scales=dataset.pixel_scales,\n",
        "    inner_radius=0.4,\n",
        "    outer_radius=2.7,\n",
        ")\n",
        "\n",
        "dataset = dataset.apply_mask(mask=mask)\n",
        "\n",
        "dataset_plotter = aplt.ImagingPlotter(\n",
        "    dataset=dataset, visuals_2d=aplt.Visuals2D(mask=mask)\n",
        ")\n",
        "dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model + Analysis__\n",
        "\n",
        "Lets fit the data using this mask, by creating the search as per usual. \n",
        "\n",
        "Note that the `imaging` data with this mask applied is passed into the `AnalysisImaging` object, ensuring that this \n",
        "is the mask the model-fit uses. \n",
        "\n",
        "we also use a linear light profile, simplifying the model as discussed in the previous tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(\n",
        "    galaxies=af.Collection(\n",
        "        lens=af.Model(al.Galaxy, redshift=0.5, mass=al.mp.IsothermalSph),\n",
        "        source=af.Model(al.Galaxy, redshift=1.0, bulge=al.lp_linear.ExponentialCoreSph),\n",
        "    ),\n",
        ")\n",
        "\n",
        "search = af.Nautilus(\n",
        "    path_prefix=path.join(\"howtolens\", \"chapter_2\"),\n",
        "    name=\"tutorial_6_with_custom_mask\",\n",
        "    unique_tag=dataset_name,\n",
        "    n_live=80,\n",
        "    number_of_cores=1,\n",
        ")\n",
        "\n",
        "analysis = al.AnalysisImaging(dataset=dataset)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Run Time__\n",
        "\n",
        "Smaller masks speed up the log likelihood evaluation time, because fewer image pixels mean fewer calculations are\n",
        "required to evaluate the model image's fit to the data. \n",
        "\n",
        "This includes computing fewer deflection angles from the mass profiles and performing fewer 2D convolutions of the PSF\n",
        "to account for blurring."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "run_time_dict, info_dict = analysis.profile_log_likelihood_function(\n",
        "    instance=model.random_instance()\n",
        ")\n",
        "\n",
        "print(f\"Log Likelihood Evaluation Time (second) = {run_time_dict['fit_time']}\")\n",
        "print(\n",
        "    \"Estimated Run Time Upper Limit (seconds) = \",\n",
        "    (run_time_dict[\"fit_time\"] * model.total_free_parameters * 10000)\n",
        "    / search.number_of_cores,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "Run the non-linear search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Discussion__\n",
        "\n",
        "So, we can choose the mask we use in a model-fit. We know that we want the mask to not remove any of the lensed source \n",
        "galaxy's light, but is this the 'right' mask? What is the 'right' mask? Maybe we want a bigger mask? a smaller mask?\n",
        "\n",
        "When it comes to choosing a mask, we are essentially balancing two things: computational run-time and accuracy. When we\n",
        "use a bigger mask the model-fit will take longer to perform. Why? Because a bigger mask includes more image-pixels \n",
        "in the analysis, and for every additional image-pixel we have to compute its deflection angles, trace it to the source\n",
        "plane, fit its light, etc.\n",
        " \n",
        "If run-time was not a consideration we would always choose a bigger mask, for two reasons:\n",
        "\n",
        " 1) The lensed source galaxy may have very faint emission that when you choose the mask you simply do not notice. \n",
        " Overly aggressive masking runs the risk of us inadvertantly masking out some of the source's light, which would \n",
        " otherwise better constrain the lens model!\n",
        "    \n",
        " 2) When the data is fitted with a model image, the fit is performed only within the masked region. For certain lens\n",
        " models it is possible that it may produce extraneous source emission outside of the masked region that is not actually\n",
        " observed in the lens data itself. If this region had not been masked-out, the model would create residuals in these\n",
        " locations and reduce the value of likelihood appropriately, whereas if it is masked out this reduction in likelihood is \n",
        " not fed through to the analysis. \n",
        "\n",
        "As you use **PyAutoLens** more you will get a feel for how fast a model-fit will run given the quality of data, lens \n",
        "model complexity, non-linear search settings, etc. As you develop this intuition, I recommend that you always aim to \n",
        "use as large of a mask as possible (whilst still achieving reasonable run-times). Aggressive masking will make \n",
        "**PyAutoLens** run very fast, but could lead you to infer an incorrect lens model! \n",
        "\n",
        "If your data includes the light of the foreground lens galaxy you pretty much have no choice but to use a large \n",
        "circular mask, because it is important to capture the lens galaxy's extended emission. This will probably extend well \n",
        "beyond the light of the source. Nevertheless, the size of this circular mask will still play an important role in the\n",
        "overall run time!\n",
        "\n",
        "__Positions Thresholding__\n",
        "\n",
        "We can manually specify a set of image-plane (y,x) coordinates corresponding to the multiple images of a strong lens's \n",
        "source-galaxy. If we supply these positions, every time a lens model is fitted **PyAutoLens** will first check that \n",
        "these pixels trace within a specified arc-second threshold of one another in the source-plane (which is controlled \n",
        "by an input `position_threshold`). If they do not trace within this threshold of one another, a large penalty term\n",
        "is added to the likelihood.\n",
        "\n",
        "This provides two benefits:\n",
        "\n",
        " 1) The model-fit is faster as the non-linear search as when searching regions of parameter space where the mass-model \n",
        " is clearly not accurate, a computationally fast ray-tracing of 2 or 4 image-pixels is performed.\n",
        "    \n",
        " 2) By removing these solutions, a global-maximum solution may be reached instead of a local-maxima. This is \n",
        " because removing the incorrect mass models makes the non-linear parameter space less complex.\n",
        "\n",
        "Once we begin to sample accurate lens models, the position threshold will be met and no penalty term will be included\n",
        "in the likelihood function.\n",
        "\n",
        "We can create the image-plane positions using the `Grid2DIrregular` object, which defines a grid of $(y,x)$ coordinates\n",
        "which are on an irregular grid (in contrast to the `Grid2D` object which assumes the $(y,x)$ coordinates are on a \n",
        "regular uniform grid). T"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "positions = al.Grid2DIrregular(\n",
        "    values=[(1.6, 0.0), (0.0, 1.6), (-1.6, 0.0), (0.0, -1.6)]\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To check the image-positions are accurate we can plot them using the `Visuals2D` object (they are the magenta dots on \n",
        "the image)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "visuals = aplt.Visuals2D(positions=positions)\n",
        "\n",
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset, visuals_2d=visuals)\n",
        "dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the positions to be used in a model-fit, we create a `PositionsLHPenalty` object which we pass to \n",
        "the `AnalysisImaging` class. \n",
        "\n",
        "This object includes the `threshold` that the positions must trace within during model-fit for a lens model \n",
        "to be accepted and not resampled. \n",
        "\n",
        "The threshold of 1.0\" is large. For an accurate lens model we would anticipate the positions trace within < 0.01\" of\n",
        "one another. However, we only want the threshold to aid the non-linear with the choice of mass model in the intiial \n",
        "fit.\n",
        "\n",
        "We choose an excessively large threshold for two reasons:\n",
        "\n",
        " 1) Even with such a large value, we still apply a likelihood penalty to the *vast majority* of unphysical lens models \n",
        " which trace the pixels even further away in the source plane.\n",
        " \n",
        " 2) If we set the threshold too low, there is a risk we may apply the penalty to physically plausible solutions. \n",
        " A conservative value of 1.0\" therefore a wise choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "positions_likelihood = al.PositionsLHPenalty(positions=positions, threshold=1.0)\n",
        "\n",
        "analysis = al.AnalysisImaging(\n",
        "    dataset=dataset, positions_likelihood=positions_likelihood\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Run Time__\n",
        "\n",
        "The likelihood evaluation time of the position threshold is below 0.001 seconds, meaning that the initial parameter \n",
        "space sampling is extremely efficient leading to a significant speed up of the model-fit (not accounted for in\n",
        "the run-time estimate below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "run_time_dict, info_dict = analysis.profile_log_likelihood_function(\n",
        "    instance=model.random_instance()\n",
        ")\n",
        "\n",
        "print(f\"Log Likelihood Evaluation Time (second) = {run_time_dict['fit_time']}\")\n",
        "print(\n",
        "    \"Estimated Run Time Upper Limit (seconds) = \",\n",
        "    (run_time_dict[\"fit_time\"] * model.total_free_parameters * 10000)\n",
        "    / search.number_of_cores,\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "Run the non-linear search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.Nautilus(\n",
        "    path_prefix=path.join(\"howtolens\", \"chapter_2\"),\n",
        "    name=\"tutorial_6_with_positions\",\n",
        "    unique_tag=dataset_name,\n",
        "    n_live=80,\n",
        "    number_of_cores=1,\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"The non-linear search has begun running - checkout the workspace/output/t7_with_positions\"\n",
        "    \"  folder for live output of the results, images and lens model.\"\n",
        "    \"  This Jupyter notebook cell with progress once search has completed - this could take some time!\"\n",
        ")\n",
        "\n",
        "search.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"Search has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Wrap Up__\n",
        "\n",
        "You may now wish to checkout the `autolens_workspace/*/data_preparation/imaging` package. This includes tools for \n",
        "creating custom masks and marking the positions on a strong lens (via a GUI) so you can use them in a model-fit.\n",
        "\n",
        "There are two things you should bare in mind in terms of masking and positions:\n",
        "\n",
        " 1) Customizing the mask and positions for the analysis of one strong lens gets the analysis running fast and can \n",
        " provide accurate non-linear sampling. However, for a large sample of lenses, this high level of customization may take \n",
        " a lot of time. \n",
        "    \n",
        " 2) When using positions, we must be *extremely* careful. It is often unclear whether a lensed source consists of one \n",
        " or more galaxies, and each galaxy may have multiple distinct sources of light. When you mark the positions, they may \n",
        " not truly originate from the same location in the source-plane. This is why the conservative positions threshold of\n",
        " 1.0\" is wise, as it is sufficiently large that even if the detailed structure of the source means we mark the positions\n",
        " incorrectly it will not mean that we remove physically plausible mass models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}