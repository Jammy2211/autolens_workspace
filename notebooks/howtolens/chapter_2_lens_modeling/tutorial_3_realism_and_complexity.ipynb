{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 3: Realism and Complexity\n",
        "==================================\n",
        "\n",
        "In the previous two tutorials, we fitted a fairly crude and unrealistic model: the lens's mass was spherical, as was\n",
        "the source's light. Given most lens galaxies are literally called 'elliptical galaxies' we should probably model their\n",
        "mass as elliptical! Furthermore, we have completely omitted the lens galaxy's light, which in real observations\n",
        "outshines the source's light and therefore must be included in the lens model.\n",
        "\n",
        "In this tutorial, we'll use a more realistic lens model, which consists of the following light and mass profiles:\n",
        "\n",
        " - An `Sersic` light profile for the lens galaxy's light [7 parameters].\n",
        " - A `Isothermal` mass profile for the lens galaxy's mass [5 parameters].\n",
        " - An `ExternalShear` which accounts for additional lensing by other galaxies nearby the lens [2 parameters].\n",
        " - An `Exponential` light profile for the source-galaxy's light (this is probably still too simple for most\n",
        " strong lenses, but we will worry about that later) [6 parameters].\n",
        "\n",
        "This lens model has 20 free parameters, meaning that the parameter space and likelihood function it defines has a\n",
        "dimensionality of N=20. This is over double the number of parameters and dimensions of the models we fitted in the\n",
        "previous tutorials and in future exercises, we will fit even more complex models with some 30+ parameters.\n",
        "\n",
        "Therefore, take note, as we make our lens model more realistic, we also make its parameter space more complex, this is\n",
        "an important concept to keep in mind for the remainder of this chapter!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import autolens as al\n",
        "import autolens.plot as aplt\n",
        "import autofit as af"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Initial Setup__\n",
        "\n",
        "we'll use new strong lensing data, where:\n",
        "\n",
        " - The lens galaxy's light is an `Sersic`.\n",
        " - The lens galaxy's total mass distribution is an `Isothermal` and `ExternalShear`.\n",
        " - The source galaxy's light is an `Exponential`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"lens_sersic\"\n",
        "dataset_path = path.join(\"dataset\", \"imaging\", dataset_name)\n",
        "\n",
        "dataset = al.Imaging.from_fits(\n",
        "    data_path=path.join(dataset_path, \"data.fits\"),\n",
        "    noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "    psf_path=path.join(dataset_path, \"psf.fits\"),\n",
        "    pixel_scales=0.1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Mask__\n",
        "\n",
        "We'll create and use a 2.5\" `Mask2D`, which is slightly smaller than the masks we used in previous tutorials."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mask = al.Mask2D.circular(\n",
        "    shape_native=dataset.shape_native, pixel_scales=dataset.pixel_scales, radius=2.5\n",
        ")\n",
        "\n",
        "dataset = dataset.apply_mask(mask=mask)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When plotted, the lens light`s is clearly visible in the centre of the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_plotter = aplt.ImagingPlotter(\n",
        "    dataset=dataset, visuals_2d=aplt.Visuals2D(mask=mask)\n",
        ")\n",
        "dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "Now lets fit the dataset using a search.\n",
        "\n",
        "Below, we write the `model` using the **PyAutoFit** concise API, which means that we do not have to specify that\n",
        "each component of the model is a `Model` object. This is because we are passing the light and mass profiles directly \n",
        "to the `Collection` object, which assumes they are `Model` objects.\n",
        "\n",
        "We will use this consistent API throughout the chapter, so you should get used to it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = af.Collection(\n",
        "    galaxies=af.Collection(\n",
        "        lens=af.Model(\n",
        "            al.Galaxy,\n",
        "            redshift=0.5,\n",
        "            bulge=al.lp.Sersic,\n",
        "            mass=al.mp.Isothermal,\n",
        "            shear=al.mp.ExternalShear,\n",
        "        ),\n",
        "        source=af.Model(al.Galaxy, redshift=1.0, bulge=al.lp.ExponentialCore),\n",
        "    ),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search + Analysis__\n",
        "\n",
        "We set up `Nautilus` as we did in the previous tutorial, however given the increase in model complexity we'll use\n",
        "a higher `n_live` value of 150 to ensure we sample the complex parameter space efficiently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.Nautilus(\n",
        "    path_prefix=path.join(\"howtolens\", \"chapter_2\"),\n",
        "    name=\"tutorial_3_realism_and_complexity\",\n",
        "    unique_tag=dataset_name,\n",
        "    n_live=200,\n",
        "    number_of_cores=1,\n",
        ")\n",
        "\n",
        "analysis = al.AnalysisImaging(dataset=dataset)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Run Time__\n",
        "\n",
        "The run time of the `log_likelihood_function` is a little bit more than previous tutorials, because we added more\n",
        "light and mass profiles to the model. It is the increased number of parameters that increases the expected run time \n",
        "more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "run_time_dict, info_dict = analysis.profile_log_likelihood_function(\n",
        "    instance=model.random_instance()\n",
        ")\n",
        "\n",
        "print(f\"Log Likelihood Evaluation Time (second) = {run_time_dict['fit_time']}\")\n",
        "print(\n",
        "    \"Estimated Run Time Upper Limit (seconds) = \",\n",
        "    (run_time_dict[\"fit_time\"] * model.total_free_parameters * 10000)\n",
        "    / search.number_of_cores,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the non-linear search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    \"The non-linear search has begun running - checkout the autolens_workspace/output/howtolens/chapter_2/tutorial_3_realism_and_complexity\"\n",
        "    \" folder for live output of the results, images and lens model.\"\n",
        "    \" This Jupyter notebook cell with progress once search has completed - this could take some time!\"\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"Search has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "Inspection of the `info` summary of the result suggests the model has gone to reasonable values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And lets look at how well the model fits the imaging data, which as we are used to fits the data brilliantly!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit_plotter = aplt.FitImagingPlotter(fit=result.max_log_likelihood_fit)\n",
        "fit_plotter.subplot_fit()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Global and Local Maxima__\n",
        "\n",
        "Up to now, all our non-linear searches have successfully found lens models that provide visibly good fits to the data, \n",
        "minimizing residuals and inferring high log likelihood values. These optimal solutions, known as 'global maxima,' \n",
        "correspond to the highest likelihood regions across the entire parameter space. In other words, no other lens model \n",
        "in the parameter space would yield a higher likelihood. This is the ideal model we always aim to infer.\n",
        "\n",
        "However, non-linear searches do not always locate these global maxima. They may instead infer 'local maxima' \n",
        "solutions, which have high log likelihood values relative to nearby models in the parameter space, but \n",
        "significantly lower log likelihoods compared to the true global maxima situated elsewhere. \n",
        "\n",
        "Why might a non-linear search end up at these local maxima? As previously discussed, the search iterates through \n",
        "many models, focusing more on regions of the parameter space where previous guesses yielded higher likelihoods. \n",
        "The search gradually 'converges' around any solution with a higher likelihood than surrounding models. If the \n",
        "search is not thorough enough, it may settle on a local maxima, appearing to offer a high likelihood \n",
        "relative to nearby models but failing to reach the global maxima.\n",
        "\n",
        "Inferring local maxima is a failure of our non-linear search, and it's something we want to avoid. To illustrate \n",
        "this, we can intentionally infer a local maxima by reducing the number of live points (`n_live`) Nautilus uses to \n",
        "map out the parameter space. By using very few live points, the initial search over the parameter space has a low \n",
        "probability of approaching the global maxima, thus it converges on a local maxima."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.Nautilus(\n",
        "    path_prefix=path.join(\"howtolens\", \"chapter_2\"),\n",
        "    name=\"tutorial_3_realism_and_complexity__local_maxima\",\n",
        "    unique_tag=dataset_name,\n",
        "    n_live=75,\n",
        "    number_of_cores=1,\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"The non-linear search has begun running - checkout the autolens_workspace/output/3_realism_and_complexity\"\n",
        "    \" folder for live output of the results, images and lens model.\"\n",
        "    \" This Jupyter notebook cell with progress once search has completed - this could take some time!\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Run Time__\n",
        "\n",
        "Due to the decreased number of live points, the estimate of 10000 iterations per free parameter is now a significant\n",
        "overestimate. The actual run time of the model-fit will be much less than this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "run_time_dict, info_dict = analysis.profile_log_likelihood_function(\n",
        "    instance=model.random_instance()\n",
        ")\n",
        "\n",
        "print(f\"Log Likelihood Evaluation Time (second) = {run_time_dict['fit_time']}\")\n",
        "print(\n",
        "    \"Estimated Run Time Upper Limit (seconds) = \",\n",
        "    (run_time_dict[\"fit_time\"] * model.total_free_parameters * 10000)\n",
        "    / search.number_of_cores,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the non-linear search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_local_maxima = search.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"Search has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "Inspection of the `info` summary of the result suggests certain parameters have gone to different values to the fit\n",
        "performed above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result_local_maxima.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lats look at the fit to the `Imaging` data, which is clearly worse than our original fit above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit_plotter = aplt.FitImagingPlotter(fit=result_local_maxima.max_log_likelihood_fit)\n",
        "fit_plotter.subplot_fit()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, just to be sure we hit a local maxima, lets compare the maximum log likelihood values of the two results \n",
        "\n",
        "The local maxima value is significantly lower, confirming that our non-linear search simply failed to locate lens \n",
        "models which fit the data better when it searched parameter space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Likelihood of Global Model:\")\n",
        "print(result.max_log_likelihood_fit.log_likelihood)\n",
        "print(\"Likelihood of Local Model:\")\n",
        "print(result_local_maxima.max_log_likelihood_fit.log_likelihood)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Wrap Up__\n",
        "\n",
        "In this example, we intentionally made our non-linear search fail, by using so few live points it had no hope of \n",
        "sampling parameter space thoroughly. For modeling real lenses we wouldn't do this intentionally, but the risk of \n",
        "inferring a local maxima is still very real, especially as we make our lens model more complex.\n",
        "\n",
        "Lets think about *complexity*. As we make our lens model more realistic, we also made it more complex. For this \n",
        "tutorial, our non-linear parameter space went from 7 dimensions to 18. This means there was a much larger *volume* of \n",
        "parameter space to search. As this volume grows, there becomes a higher chance that our non-linear search gets lost \n",
        "and infers a local maxima, especially if we don't set it up with enough live points!\n",
        "\n",
        "At its core, lens modeling is all about learning how to get a non-linear search to find the global maxima region of \n",
        "parameter space, even when the lens model is complex. This will be the main theme throughout the rest of this chapter\n",
        "and is the main subject of chapter 3.\n",
        "\n",
        "In the next exercise, we'll learn how to deal with failure and begin thinking about how we can ensure our non-linear \n",
        "search finds the global-maximum log likelihood solution. First, think about the following:\n",
        "\n",
        " 1) When you look at an image of a strong lens, do you get a sense of roughly what values certain lens model \n",
        " parameters are?\n",
        "    \n",
        " 2) The non-linear search failed because parameter space was too complex. Could we make it less complex, whilst \n",
        " still keeping our lens model fairly realistic?\n",
        "    \n",
        " 3) The source galaxy in this example had only 7 non-linear parameters. Real source galaxies may have multiple \n",
        " components (e.g. a disk, bulge, bar, star-forming knot) and there may even be more than 1 source galaxy! Do you \n",
        " think there is any hope of us navigating a parameter space if the source contributes 20+ parameters by itself?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}