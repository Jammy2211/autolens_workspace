{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 2: Practicalities\n",
        "==========================\n",
        "\n",
        "In the last tutorial, we introduced foundational statistical concepts essential for model-fitting, such as parameter\n",
        "spaces, likelihoods, priors, and non-linear searches. Understanding these statistical concepts is crucial for\n",
        "performing model fits effectively.\n",
        "\n",
        "However, achieving successful model-fitting also requires practical skills, including how to manage outputs,\n",
        "review results, interpret model quality, and ensure run-times are efficient enough for your scientific needs.\n",
        "\n",
        "This tutorial will focus on these practical aspects of model-fitting, including:\n",
        "\n",
        "- How to save results to your hard disk.\n",
        "- How to navigate the output folder and examine model-fit results to assess quality.\n",
        "- How to estimate the run-time of a model-fit before initiating it, and change settings to make it faster or run the analysis in parallel.\n",
        "\n",
        "__Contents__\n",
        "\n",
        "This tutorial is split into the following sections:\n",
        "\n",
        " **PyAutoFit:** The parent package of PyAutoGalaxy, which handles practicalities of model-fitting.\n",
        " **Initial Setup:** Load the dataset we'll fit a model to using a non-linear search.\n",
        " **Mask:** Apply a mask to the dataset.\n",
        " **Model:** Introduce the model we will fit to the data.\n",
        " **Search:** Setup the non-linear search, Nautilus, used to fit the model to the data.\n",
        " **Search Settings:** Discuss the settings of the non-linear search, including the number of live points.\n",
        " **Number Of Cores:** Discuss how to use multiple cores to fit models faster in parallel.\n",
        " **Parallel Script:** Running the model-fit in parallel if a bug occurs in a Jupiter notebook.\n",
        " **Iterations Per Update:** How often the non-linear search outputs the current results to hard-disk.\n",
        " **Analysis:** Create the Analysis object which contains the `log_likelihood_function` that the non-linear search calls.\n",
        " **Model-Fit:** Fit the model to the data.\n",
        " **Result:** Print the results of the model-fit to the terminal.\n",
        " **Output Folder:** Inspect the output folder where results are stored.\n",
        " **Unique Identifier:** Discussion of the unique identifier of the model-fit which names the folder in the output directory.\n",
        " **Output Folder Contents:** What is output to the output folder (model results, visualization, etc.).\n",
        " **Result:** Plot the best-fit model to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__PyAutoFit__\n",
        "\n",
        "Modeling uses the probabilistic programming language\n",
        "[PyAutoFit](https://github.com/rhayes777/PyAutoFit), an open-source project that allows complex model\n",
        "fitting techniques to be straightforwardly integrated into scientific modeling software. \n",
        "\n",
        "The majority of tools that make model-fitting practical are provided by PyAutoFit, for example it handles\n",
        "all output of the non-linear search to hard-disk, the visualization of results and the estimation of run-times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import autofit as af"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Initial Setup__\n",
        "\n",
        "Lets first load the `Imaging` dataset we'll fit a model with using a non-linear search. \n",
        "\n",
        "This is the same dataset we fitted in the previous tutorial, and we'll repeat the same fit, as we simply want\n",
        "to illustrate the practicalities of model-fitting in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"simple__no_lens_light__mass_sis\"\n",
        "dataset_path = path.join(\"dataset\", \"imaging\", dataset_name)\n",
        "\n",
        "dataset = al.Imaging.from_fits(\n",
        "    data_path=path.join(dataset_path, \"data.fits\"),\n",
        "    noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "    psf_path=path.join(dataset_path, \"psf.fits\"),\n",
        "    pixel_scales=0.1,\n",
        ")\n",
        "\n",
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Mask__\n",
        "\n",
        "The non-linear fit also needs a `Mask2D`, lets use a 3.0\" circle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mask = al.Mask2D.circular(\n",
        "    shape_native=dataset.shape_native, pixel_scales=dataset.pixel_scales, radius=3.0\n",
        ")\n",
        "\n",
        "dataset = dataset.apply_mask(mask=mask)\n",
        "\n",
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "We compose the model using the same API as the previous tutorial.\n",
        "\n",
        "This model is the same as the previous tutorial, an `Isothermal` sphereical mass profile representing the lens\n",
        "galaxy and a `ExponentialCoreSph` light profile representing the source galaxy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Lens:\n",
        "\n",
        "mass = af.Model(al.mp.IsothermalSph)\n",
        "\n",
        "lens = af.Model(al.Galaxy, redshift=0.5, mass=mass)\n",
        "\n",
        "# Source:\n",
        "\n",
        "bulge = af.Model(al.lp_linear.ExponentialCoreSph)\n",
        "\n",
        "source = af.Model(al.Galaxy, redshift=1.0, bulge=bulge)\n",
        "\n",
        "# Overall Lens Model:\n",
        "\n",
        "model = af.Collection(galaxies=af.Collection(lens=lens, source=source))\n",
        "\n",
        "print(model.info)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "To fit the model, we now create the non-linear search object, selecting the nested sampling algorithm, Nautilus, \n",
        "as recommended in the previous tutorial.\n",
        "\n",
        "We set up the `Nautilus` object with these parameters:\n",
        "\n",
        "- **`path_prefix`**: specifies the output directory, here set to `autolens_workspace/output/howtogalaxy/chapter_2`.\n",
        "  \n",
        "- **`name`**: gives the search a descriptive name, which creates the full output path \n",
        "as `autolens_workspace/output/howtogalaxy/chapter_2/tutorial_2_practicalities`.\n",
        "\n",
        "- **`n_live`**: controls the number of live points Nautilus uses to sample parameter space.\n",
        "\n",
        "__Search Settings__\n",
        "\n",
        "Nautilus samples parameter space by placing \"live points\" representing different galaxy models. Each point has an \n",
        "associated `log_likelihood` that reflects how well it fits the data. By mapping where high-likelihood solutions are \n",
        "located, it can focus on searching those regions.\n",
        "\n",
        "The main setting to balance is the **number of live points**. More live points allow Nautilus to map parameter space \n",
        "more thoroughly, increasing accuracy but also runtime. Fewer live points reduce run-time but may make the search less \n",
        "reliable, possibly getting stuck in local maxima.\n",
        "\n",
        "The ideal number of live points depends on model complexity. More parameters generally require more live points, but \n",
        "the default of 200 is sufficient for most lens models. Lower values can still yield reliable results, particularly \n",
        "for simpler models. For this example (7 parameters), we reduce the live points to 100 to speed up runtime without \n",
        "compromising accuracy.\n",
        "\n",
        "Tuning non-linear search settings (e.g., the number of live points) to match model complexity is essential. We aim \n",
        "for enough live points to ensure accurate results (i.e., finding a global maximum) but not so many that runtime \n",
        "is excessive.\n",
        "\n",
        "In practice, the optimal number of live points is often found through trial and error, guided by summary statistics \n",
        "on how well the search is performing, which we\u2019ll cover below. For this single Sersic model with a linear light \n",
        "profile, 80 live points is sufficient to achieve reliable results.\n",
        "\n",
        "__Number Of Cores__\n",
        "\n",
        "We may include an input `number_of_cores`, which when above 1 means that Nautilus uses parallel processing to sample \n",
        "multiple models at once on your CPU. When `number_of_cores=2` the search will run roughly two times as\n",
        "fast, for `number_of_cores=3` three times as fast, and so on. The downside is more cores on your CPU will be in-use\n",
        "which may hurt the general performance of your computer.\n",
        "\n",
        "You should experiment to figure out the highest value which does not give a noticeable loss in performance of your \n",
        "computer. If you know that your processor is a quad-core processor you should be able to use `number_of_cores=4`. \n",
        "\n",
        "Above `number_of_cores=4` the speed-up from parallelization diminishes greatly. We therefore recommend you do not\n",
        "use a value above this.\n",
        "\n",
        "For users on a Windows Operating system, using `number_of_cores>1` may lead to an error, in which case it should be \n",
        "reduced back to 1 to fix it.\n",
        "\n",
        "__Parallel Script__\n",
        "\n",
        "Depending on the operating system (e.g. Linux, Mac, Windows), Python version, if you are running a Jupyter notebook \n",
        "and other factors, this script may not run a successful parallel fit (e.g. running the script \n",
        "with `number_of_cores` > 1 will produce an error). It is also common for Jupyter notebooks to not run in parallel \n",
        "correctly, requiring a Python script to be run, often from a command line terminal.\n",
        "\n",
        "To fix these issues, the Python script needs to be adapted to use an `if __name__ == \"__main__\":` API, as this allows\n",
        "the Python `multiprocessing` module to allocate threads and jobs correctly. An adaptation of this example script \n",
        "is provided at `autolens_workspace/scripts/modeling/imaging/customize/parallel.py`, which will hopefully run \n",
        "successfully in parallel on your computer!\n",
        "\n",
        "Therefore if paralellization for this script doesn't work, check out the `parallel.py` example. You will need to update\n",
        "all scripts you run to use the this format and API. \n",
        "\n",
        "__Iterations Per Update__\n",
        "\n",
        "Every N iterations, the non-linear search outputs the current results to the folder `autolens_workspace/output`,\n",
        "which includes producing visualization. \n",
        "\n",
        "Depending on how long it takes for the model to be fitted to the data (see discussion about run times below), \n",
        "this can take up a large fraction of the run-time of the non-linear search.\n",
        "\n",
        "For this fit, the fit is very fast, thus we set a high value of `iterations_per_update=10000` to ensure these updates\n",
        "so not slow down the overall speed of the model-fit. \n",
        "\n",
        "**If the iteration per update is too low, the model-fit may be significantly slowed down by the time it takes to\n",
        "output results and visualization frequently to hard-disk. If your fit is consistent displaying a log saying that it\n",
        "is outputting results, try increasing this value to ensure the model-fit runs efficiently.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.Nautilus(\n",
        "    path_prefix=path.join(\"howtolens\", \"chapter_2\"),\n",
        "    name=\"tutorial_2_practicalities\",\n",
        "    unique_tag=dataset_name,\n",
        "    n_live=100,\n",
        "    iterations_per_update=2500,\n",
        "    # number_of_cores=1, # Try uncommenting this line to run in parallel but see \"Parallel Script\" above.\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "We again create the `AnalysisImaging` object which contains the `log_likelihood_function` that the non-linear search\n",
        "calls to fit the model to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = al.AnalysisImaging(dataset=dataset)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Run Times__\n",
        "\n",
        "Lens modeling can be a computationally expensive process. When fitting complex models to high resolution datasets \n",
        "run times can be of order hours, days, weeks or even months.\n",
        "\n",
        "Run times are dictated by two factors:\n",
        "\n",
        " - **The log likelihood evaluation time:** the time it takes for a single `instance` of the model to be fitted to \n",
        "   the dataset such that a log likelihood is returned.\n",
        "\n",
        " - **The number of iterations (e.g. log likelihood evaluations) performed by the non-linear search:** more complex lens\n",
        "   models require more iterations to converge to a solution (and as discussed above, settings like the number of live\n",
        "   points also control this).\n",
        "\n",
        "The log likelihood evaluation time can be estimated before a fit using the `profile_log_likelihood_function` method,\n",
        "which returns two dictionaries containing the run-times and information about the fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "run_time_dict, info_dict = analysis.profile_log_likelihood_function(\n",
        "    instance=model.random_instance()\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The overall log likelihood evaluation time is given by the `fit_time` key.\n",
        "\n",
        "For this example, it is ~0.05 seconds, which is extremely fast for lens modeling. \n",
        "\n",
        "The more advanced fitting techniques discussed at the end of chapter 1 (e.g. shapelets, multi Gaussian expansions, \n",
        "pixelizations) have longer log likelihood evaluation times (1-3 seconds) and therefore may require more efficient \n",
        "search settings to keep the overall run-time feasible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Log Likelihood Evaluation Time (second) = {run_time_dict['fit_time']}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To estimate the expected overall run time of the model-fit we multiply the log likelihood evaluation time by an \n",
        "estimate of the number of iterations the non-linear search will perform. \n",
        "\n",
        "Estimating this is tricky, as it depends on the lens model complexity (e.g. number of parameters)\n",
        "and the properties of the dataset and model being fitted. With 7 free parameters, this gives an estimate \n",
        "of ~7000 iterations, which at ~0.05 seconds per iteration gives a total run-time of ~180 seconds (or ~3 minutes).\n",
        "\n",
        "For this example, we conservatively estimate that the non-linear search will perform ~1000 iterations per free \n",
        "parameter in the model. This is an upper limit, with models typically converging in fewer iterations.\n",
        "\n",
        "If you perform the fit over multiple CPUs, you can divide the run time by the number of cores to get an estimate of\n",
        "the time it will take to fit the model. Parallelization with Nautilus scales well, it speeds up the model-fit by the \n",
        "`number_of_cores` for N < 8 CPUs and roughly `0.5*number_of_cores` for N > 8 CPUs. This scaling continues \n",
        "for N> 50 CPUs, meaning that with super computing facilities you can always achieve fast run times!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    \"Estimated Run Time Upper Limit (seconds) = \",\n",
        "    (run_time_dict[\"fit_time\"] * model.total_free_parameters * 1000)\n",
        "    / search.number_of_cores,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model-Fit__\n",
        "\n",
        "To begin the model-fit, we pass the model and analysis objects to the search, which performs a non-linear search to \n",
        "identify models that best fit the data.\n",
        "\n",
        "Running model fits via non-linear search can take significant time. While the fit in this tutorial should \n",
        "complete in a few minutes, more complex models may require longer run times. In Jupyter notebooks, this can be \n",
        "limiting, as the notebook cell will only complete once the fit finishes, preventing you from advancing through the \n",
        "tutorial or running additional code.\n",
        "\n",
        "To work around this, we recommend running tutorial scripts as standalone Python scripts, found \n",
        "in `autolens_workspace/scripts/howtogalaxy`. These scripts mirror the notebook tutorials but run independently of \n",
        "Jupyter notebooks. For example, you can start a script with:\n",
        "\n",
        "`python3 scripts/howtogalaxy/chapter_2_modeling/tutorial_2_practicalities.py`\n",
        "\n",
        "Using scripts allows results to be saved to the hard drive in the `output` folder, enabling you to inspect results \n",
        "immediately once the script completes. When rerun, the script loads results directly from disk, so any Jupyter \n",
        "notebook cells will quickly load and display the complete model-fit results if they\u2019re already saved.\n",
        "\n",
        "This approach not only avoids the slowdowns associated with notebook cells during lengthy runs but is also essential \n",
        "for using super-computers for fitting tasks, as they require separate Python scripts.\n",
        "\n",
        "For tasks like loading results, inspecting data, plotting, and interpreting results, Jupyter notebooks remain ideal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    \"The non-linear search has begun running - checkout the autolens_workspace/output/\"\n",
        "    \" folder for live output of the results, images and lens model.\"\n",
        "    \" This Jupyter notebook cell with progress once search has completed - this could take some time!\"\n",
        ")\n",
        "\n",
        "result = search.fit(model=model, analysis=analysis)\n",
        "\n",
        "print(\"Search has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result Info__\n",
        "\n",
        "A concise readable summary of the results is given by printing its `info` attribute.\n",
        "\n",
        "[Above, we discussed that the `info_whitespace_length` parameter in the config files could b changed to make \n",
        "the `model.info` attribute display optimally on your computer. This attribute also controls the whitespace of the\n",
        "`result.info` attribute.]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Output Folder__\n",
        "\n",
        "Now checkout the `autolens_workspace/output` folder.\n",
        "\n",
        "This is where the results of the search are written to hard-disk (in the `tutorial_2_practicalities` folder). \n",
        "\n",
        "Once completed images, results and information about the fit appear in this folder, meaning that you don't need \n",
        "to keep running Python code to see the result.\n",
        "\n",
        "__Unique Identifier__\n",
        "\n",
        "In the output folder, you will note that results are in a folder which is a collection of random characters. This acts \n",
        "as a `unique_identifier` of the model-fit, where this identifier is generated based on the model, search and dataset \n",
        "that are used in the fit.\n",
        " \n",
        "An identical combination of model, search and dataset generates the same identifier, meaning that rerunning the\n",
        "script will use the existing results to resume the model-fit. In contrast, if you change the model, search or dataset,\n",
        "a new unique identifier will be generated, ensuring that the model-fit results are output into a separate folder. \n",
        "\n",
        "We additionally want the unique identifier to be specific to the dataset fitted, so that if we fit different datasets\n",
        "with the same model and search results are output to a different folder. We achieved this for the fit above by passing \n",
        "the `dataset_name` to the search's `unique_tag`.\n",
        "\n",
        "__Output Folder Contents__\n",
        "\n",
        "Now this is running you should checkout the `autolens_workspace/output` folder. This is where the results of the \n",
        "search are written to hard-disk (in the `start_here` folder), where all outputs are human readable (e.g. as .json,\n",
        ".csv or text files).\n",
        "\n",
        "As the fit progresses, results are written to the `output` folder on the fly using the highest likelihood model found\n",
        "by the non-linear search so far. This means you can inspect the results of the model-fit as it runs, without having to\n",
        "wait for the non-linear search to terminate.\n",
        " \n",
        "The `output` folder includes:\n",
        "\n",
        " - `model.info`: Summarizes the model, its parameters and their priors discussed in the next tutorial.\n",
        " \n",
        " - `model.results`: Summarizes the highest likelihood model inferred so far including errors.\n",
        " \n",
        " - `images`: Visualization of the highest likelihood model-fit to the dataset, (e.g. a fit subplot showing the \n",
        " galaxies, model data and residuals).\n",
        " \n",
        " - `files`: A folder containing .fits files of the dataset, the model as a human-readable .json file, \n",
        " a `.csv` table of every non-linear search sample and other files containing information about the model-fit.\n",
        " \n",
        " - `search.summary`: A file providing summary statistics on the performance of the non-linear search.\n",
        " \n",
        " - `search_internal`: Internal files of the non-linear search (in this case Nautilus) used for resuming the fit and\n",
        "  visualizing the search.\n",
        "\n",
        "__Result__\n",
        "\n",
        "The `search.fit` method produces a `result` object, packed with useful information about the model fit that we\u2019ll \n",
        "explore in detail in a later tutorial.\n",
        "\n",
        "One component of the `result` object we\u2019ll use now is the `FitImaging` object, which corresponds to the set of model \n",
        "parameters that yielded the maximum log-likelihood solution. Plotting this object lets us visually inspect how well \n",
        "the model fits the data.\n",
        "\n",
        "In this example, the fit to the data is excellent, with residuals near zero, as expected since the same model was \n",
        "used both to simulate and fit the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit_plotter = aplt.FitImagingPlotter(fit=result.max_log_likelihood_fit)\n",
        "fit_plotter.subplot_fit()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Probability Density Functions (PDF's) of the results can be plotted using an in-built visualization \n",
        "library, which is wrapped via the `NestPlotter` object.\n",
        "\n",
        "The PDF shows the 1D and 2D probabilities estimated for every parameter after the model-fit. The two dimensional \n",
        "figures can show the degeneracies between different parameters, for example how increasing the intensity $I$ of the\n",
        "source galaxy and decreasing its effective radius $R_{Eff}$ lead to similar likelihoods and probabilities.\n",
        "\n",
        "This PDF will be discussed more in the next tutorial.\n",
        "\n",
        "The plot is labeled with short hand parameter names (e.g. `sersic_index` is mapped to the short hand \n",
        "parameter `n`). These mappings ate specified in the `config/notation.yaml` file and can be customized by users.\n",
        "\n",
        "The superscripts of labels correspond to the name each component was given in the model (e.g. for the `Isothermal`\n",
        "mass its name `mass` defined when making the `Model` above is used)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plotter = aplt.NestPlotter(samples=result.samples)\n",
        "plotter.corner_anesthetic()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Other Practicalities__\n",
        "\n",
        "The following are examples of other practicalities which I will document fully in this example script in the future,\n",
        "but so far have no found the time:\n",
        "\n",
        "- `config`: The files in `autogalaxy_workspace/config` which control many aspects of how PyAutoGalaxy runs,\n",
        " including visualization, the non-linear search settings.\n",
        "\n",
        "- `config/priors`: Folder containing the default priors on all model components.\n",
        "\n",
        "- `results`: How to load the results of a model-fit from the output folder to a Python script or Jupyter notebook.\n",
        "\n",
        "- `output.yaml`: What files are output to control file size.\n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "This tutorial has illustrated how to handle a number of practicalities that are key to performing model-fitting\n",
        "effectively. These include:\n",
        "\n",
        "- How to save results to your hard disk.\n",
        "\n",
        "- How to navigate the output folder and examine model-fit results to assess quality.\n",
        "\n",
        "- How to estimate the run-time of a model-fit before initiating it, and change settings to make it faster or run the\n",
        "  analysis in parallel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}