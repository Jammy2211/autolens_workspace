{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 4: Noise-Map Scaling 1\n",
        "===============================\n",
        "\n",
        "In tutorial 1, we discussed how when an inversion did not fit a compact source well we had skewed and undesirable\n",
        "chi-squared distribution. A small subset of the lensed source's brightest pixels were fitted poorly, contributing\n",
        "to the majority of our chi-squared signal. In terms of lens modeling, this meant that we would over-fit these regions\n",
        "of the image. We would prefer that our lens model provides a global fit to the entire lensed source galaxy.\n",
        "\n",
        "With our adaptive pixelization and regularization we are now able to fit the data to the noise-limit and remove this\n",
        "skewed chi-squared distribution. So, why do we need to introduce noise-map scaling? Well, we achieve a good fit when\n",
        "our lens's mass model is accurate (in the previous tutorials we used the *correct* lens mass model). But, what if our\n",
        "lens mass model isn't accurate? We'll have residuals which will cause the same problem as before; a skewed chi-squared\n",
        "distribution and an inability to fit the data to the noise level.\n",
        "\n",
        "So, lets simulate an image and fit it with a slightly incorrect mass model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Initial Setup__\n",
        "\n",
        "we'll use the same strong lensing data as the previous tutorial, where:\n",
        "\n",
        " - The lens galaxy's light is omitted.\n",
        " - The lens galaxy's total mass distribution is an `Isothermal` and `ExternalShear`.\n",
        " - The source galaxy's light is an `Sersic`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"simple__no_lens_light\"\n",
        "dataset_path = path.join(\"dataset\", \"imaging\", dataset_name)\n",
        "\n",
        "dataset = al.Imaging.from_fits(\n",
        "    data_path=path.join(dataset_path, \"data.fits\"),\n",
        "    noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "    psf_path=path.join(dataset_path, \"psf.fits\"),\n",
        "    pixel_scales=0.1,\n",
        ")\n",
        "\n",
        "mask = al.Mask2D.circular(\n",
        "    shape_native=dataset.shape_native,\n",
        "    pixel_scales=dataset.pixel_scales,\n",
        "    sub_size=2,\n",
        "    radius=3.0,\n",
        ")\n",
        "\n",
        "dataset = dataset.apply_mask(mask=mask)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we're going to fit the image using our magnification based grid. To perform the fit, we'll use a convenience \n",
        "function to fit the lens data we simulated above.\n",
        "\n",
        "In this fitting function, we have changed the lens galaxy's einstein radius to 1.5 from the `true` simulated value of \n",
        "1.6. Thus, we are going to fit the data with an *incorrect* mass model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "def fit_imaging_with_source_galaxy(dataset, source_galaxy):\n",
        "    lens_galaxy = al.Galaxy(\n",
        "        redshift=0.5,\n",
        "        mass=al.mp.Isothermal(\n",
        "            centre=(0.0, 0.0),\n",
        "            einstein_radius=1.5,\n",
        "            ell_comps=al.convert.ell_comps_from(axis_ratio=0.9, angle=45.0),\n",
        "        ),\n",
        "        shear=al.mp.ExternalShear(gamma_1=0.05, gamma_2=0.05),\n",
        "    )\n",
        "\n",
        "    tracer = al.Tracer.from_galaxies(galaxies=[lens_galaxy, source_galaxy])\n",
        "\n",
        "    return al.FitImaging(\n",
        "        dataset=dataset,\n",
        "        tracer=tracer,\n",
        "        settings_inversion=al.SettingsInversion(use_w_tilde=False),\n",
        "    )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now, we'll use the same magnification based source to fit this data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pixelization = al.Pixelization(\n",
        "    mesh=al.mesh.DelaunayMagnification(shape=(30, 30)),\n",
        "    regularization=al.reg.Constant(coefficient=3.3),\n",
        ")\n",
        "\n",
        "source_magnification = al.Galaxy(redshift=1.0, pixelization=pixelization)\n",
        "\n",
        "fit = fit_imaging_with_source_galaxy(\n",
        "    dataset=dataset, source_galaxy=source_magnification\n",
        ")\n",
        "\n",
        "include = aplt.Include2D(mapper_image_plane_mesh_grid=True, mask=True)\n",
        "\n",
        "fit_plotter = aplt.FitImagingPlotter(fit=fit, include_2d=include)\n",
        "fit_plotter.subplot_fit()\n",
        "\n",
        "inversion_plotter = fit_plotter.inversion_plotter_of_plane(plane_index=1)\n",
        "inversion_plotter.figures_2d_of_pixelization(pixelization_index=0, reconstruction=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Hyper Image__\n",
        "\n",
        "The fit isn't great. The main structure of the lensed source is reconstructed, but there are residuals. These \n",
        "residuals are worse than we saw in the previous tutorials (when source's compact central structure was the problem). \n",
        "So, the obvious question is can our adaptive pixelization and regularization schemes address the problem?\n",
        "\n",
        "Lets find out, using this solution as our hyper-image. In this case, our hyper-image isn't a perfect fit to the data. \n",
        "This should not be too problematic, as the solution still captures the source's overall structure. The pixelization and \n",
        "regularization hyper parameters have enough flexibility in how they use this image to adapt themselves, thus hyper-image \n",
        "doesn`t *need* to be perfect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hyper_image_2d = fit.model_image.binned.slim\n",
        "hyper_image_lens = fit.model_images_of_planes_list[\n",
        "    0\n",
        "]  # This is the model image of the lens"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note again that the source galaxy receives two types of hyper-images, a `adapt_galaxy_image` and a `adapt_model_image`. \n",
        "I'll discuss why in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pixelization = al.Pixelization(\n",
        "    mesh=al.mesh.DelaunayBrightnessImage(\n",
        "        pixels=500, weight_floor=0.0, weight_power=5.0\n",
        "    ),\n",
        "    regularization=al.reg.AdaptiveBrightness(\n",
        "        inner_coefficient=0.001, outer_coefficient=0.2, signal_scale=2.0\n",
        "    ),\n",
        ")\n",
        "\n",
        "source_adaptive = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    pixelization=pixelization,\n",
        "    adapt_galaxy_image=hyper_image_2d,\n",
        "    adapt_model_image=hyper_image_2d,\n",
        ")\n",
        "\n",
        "fit = fit_imaging_with_source_galaxy(dataset=dataset, source_galaxy=source_adaptive)\n",
        "\n",
        "fit_plotter = aplt.FitImagingPlotter(fit=fit, include_2d=include)\n",
        "fit_plotter.subplot_fit()\n",
        "\n",
        "inversion_plotter = fit_plotter.inversion_plotter_of_plane(plane_index=1)\n",
        "inversion_plotter.figures_2d_of_pixelization(pixelization_index=0, reconstruction=True)\n",
        "\n",
        "print(\"Evidence = \", fit.log_evidence)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Noise Scaling__\n",
        "\n",
        "The solution is better, but far from perfect. Furthermore, this solution maximizes the Bayesian log evidence, meaning \n",
        "there is no reasonable way to change our source pixelization or regularization to better fit the data. The problem \n",
        "is with the lens's mass model!\n",
        "\n",
        "This poses a major problem for model-fitting. A small subset of our data has such large chi-squared values the \n",
        "non-linear search is going to seek solutions which reduce only these chi-squared values. For the image above, a \n",
        "small subset of our data (e.g. < 5% of pixels) contributes to the majority of our log_likelihood (e.g. > 95% of the \n",
        "overall chi-squared). This is *not* what we want, as it means that instead of using the entire surface brightness \n",
        "profile of the lensed source galaxy to constrain our lens model, we end up using only a small subset of its brightest \n",
        "pixels.\n",
        "\n",
        "This is even more problematic when we try and use the Bayesian evidence to objectively quantify the quality of the \n",
        "fit, as it cannot obtain a solution that provides a reduced chi-squared of 1 (e.g. that leaves only the Gaussian noise\n",
        "in the image).\n",
        "\n",
        "So, you're probably wondering, why can`t we just change the mass model to fit the data better? Surely if we \n",
        "actually modeled this image it wouldn't go to this solution anyway but instead infer the correct \n",
        "Einstein radius of 1.6? That`s true.\n",
        "\n",
        "However, for *real* strong gravitational lenses, there is no such thing as a `correct mass model`. Real galaxies are \n",
        "not elliptical isothermal mass profiles, or power-laws, or NFW`s, or any of the symmetric and smooth analytic profiles \n",
        "we assume to model their mass. For real strong lenses our mass model will pretty much always lead to source \n",
        "reconstruction residuals, producing these skewed chi-squared distributions.\n",
        "\n",
        "This is where noise-map scaling comes in. If we have no alternative, the best way to get a Gaussian distribution \n",
        "(e.g. more uniform) chi-squared fit is to increase the variances of image pixels with high chi-squared values. So, \n",
        "that`s what we're going to do, by making our source galaxy a `hyper-galaxy`, a galaxy which use`s its hyper-image to \n",
        "increase the noise in pixels where it has a large chi-squared value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pixelization = al.Pixelization(\n",
        "    mesh=al.mesh.DelaunayBrightnessImage(\n",
        "        pixels=500, weight_floor=0.0, weight_power=5.0\n",
        "    ),\n",
        "    regularization=al.reg.AdaptiveBrightness(\n",
        "        inner_coefficient=0.001, outer_coefficient=0.2, signal_scale=2.0\n",
        "    ),\n",
        ")\n",
        "\n",
        "source_hyper_galaxy = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    pixelization=pixelization,\n",
        "    hyper_galaxy=al.legacy.HyperGalaxy(\n",
        "        contribution_factor=1.0, noise_factor=1.5, noise_power=1.0\n",
        "    ),\n",
        "    adapt_galaxy_image=hyper_image_2d,\n",
        "    adapt_model_image=hyper_image_2d,\n",
        ")\n",
        "\n",
        "fit = fit_imaging_with_source_galaxy(dataset=dataset, source_galaxy=source_hyper_galaxy)\n",
        "\n",
        "fit_plotter = aplt.FitImagingPlotter(fit=fit, include_2d=include)\n",
        "fit_plotter.subplot_fit()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, the chi-squared distribution looks *alot* better. The chi-squareds have reduced from the 200's to the \n",
        "50's, because the variances were increased. This is what we want, so lets make sure we see an appropriate increase in \n",
        "Bayesian log evidence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Evidence using baseline variances = \", 3885.2797)\n",
        "\n",
        "print(\"Evidence using variances scaling by hyper-galaxy = \", fit.log_evidence)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Yep, a huge increase in the 1000's! Clearly, if our model doesn't fit the data well we *need* to increase the noise \n",
        "wherever the fit is poor to ensure that our use of the Bayesian log evidence is well defined.\n",
        "\n",
        "__How does the HyperGalaxy that we attached to the source-galaxy above actually scale the noise?__\n",
        "\n",
        "First, it creates a `contribution_map` from the hyper-galaxy-image of the lensed source galaxy. This uses the \n",
        "`adapt_model_image`, which is the overall model-image of the best-fit lens model. In this tutorial, because our \n",
        "strong lens imaging only has a source galaxy emitting light, the `adapt_galaxy_image` of the source galaxy is the same \n",
        "as the `adapt_model_image`. However, In the next tutorial, we'll introduce the lens galaxy's light, such that each \n",
        "hyper-galaxy image is different to the hyper-galaxy model image!\n",
        "\n",
        "We compute the contribution map as follows:\n",
        "\n",
        " 1) Add the `contribution_factor` hyper-parameter value to the `adapt_model_image`.\n",
        "  \n",
        " 2) Divide the `adapt_galaxy_image` by the image created in step 1).\n",
        "    \n",
        " 3) Divide the image created in step 2) by its maximum value, such that all pixels range between 0.0 and 1.0.\n",
        "\n",
        "Lets look at a few contribution maps, generated using hyper-galaxy's with different contribution factors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "source_contribution_factor_1 = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    hyper_galaxy=al.legacy.HyperGalaxy(contribution_factor=1.0),\n",
        "    adapt_galaxy_image=hyper_image_2d,\n",
        "    adapt_model_image=hyper_image_2d,\n",
        ")\n",
        "\n",
        "contribution_map = source_contribution_factor_1.hyper_galaxy.contribution_map_from(\n",
        "    adapt_model_image=hyper_image_2d, adapt_galaxy_image=hyper_image_2d\n",
        ")\n",
        "\n",
        "mat_plot = aplt.MatPlot2D(title=aplt.Title(label=\"Contribution Map\"))\n",
        "\n",
        "array_plotter = aplt.Array2DPlotter(array=contribution_map, mat_plot_2d=mat_plot)\n",
        "array_plotter.figure_2d()\n",
        "\n",
        "source_contribution_factor_3 = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    hyper_galaxy=al.legacy.HyperGalaxy(contribution_factor=3.0),\n",
        "    adapt_galaxy_image=hyper_image_2d,\n",
        "    adapt_model_image=hyper_image_2d,\n",
        ")\n",
        "\n",
        "contribution_map = source_contribution_factor_3.hyper_galaxy.contribution_map_from(\n",
        "    adapt_model_image=hyper_image_2d, adapt_galaxy_image=hyper_image_2d\n",
        ")\n",
        "\n",
        "array_plotter = aplt.Array2DPlotter(array=contribution_map, mat_plot_2d=mat_plot)\n",
        "array_plotter.figure_2d()\n",
        "\n",
        "source_hyper_galaxy = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    hyper_galaxy=al.legacy.HyperGalaxy(contribution_factor=5.0),\n",
        "    adapt_galaxy_image=hyper_image_2d,\n",
        "    adapt_model_image=hyper_image_2d,\n",
        ")\n",
        "\n",
        "contribution_map = source_hyper_galaxy.hyper_galaxy.contribution_map_from(\n",
        "    adapt_model_image=hyper_image_2d, adapt_galaxy_image=hyper_image_2d\n",
        ")\n",
        "\n",
        "array_plotter = aplt.Array2DPlotter(array=contribution_map, mat_plot_2d=mat_plot)\n",
        "array_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By increasing the contribution factor we allocate more pixels with higher contributions (e.g. values closer to 1.0) \n",
        "than pixels with lower values. This is all the `contribution_factor` does; it scales how we allocate contributions to \n",
        "the source galaxy. Now, we're going to use this contribution map to scale the noise-map, as follows:\n",
        "\n",
        " 1) Multiply the baseline (e.g. unscaled) noise-map of the image-data by the contribution map made in step 3) above. \n",
        " This means that only noise-map values where the contribution map has large values (e.g. near 1.0) are going to \n",
        " remain in this image, with the majority of values multiplied by contribution map values near 0.0.\n",
        "    \n",
        " 2) Raise the noise-map generated in step 1) above to the power of the hyper-parameter `noise_power`. Thus, for \n",
        " large values of noise_power, the largest noise-map values will be increased even more, raising their noise the most.\n",
        "    \n",
        " 3) Multiply the noise-map values generated in step 2) by the hyper-parameter `noise_factor`. Again, this is a\n",
        " means by which **PyAutoLens** is able to scale the noise-map values.\n",
        "\n",
        "Lets compare two fits, one where a hyper-galaxy scales the noise-map, and one where it does not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pixelization = al.Pixelization(\n",
        "    mesh=al.mesh.DelaunayBrightnessImage(\n",
        "        pixels=500, weight_floor=0.0, weight_power=5.0\n",
        "    ),\n",
        "    regularization=al.reg.AdaptiveBrightness(\n",
        "        inner_coefficient=0.001, outer_coefficient=0.2, signal_scale=2.0\n",
        "    ),\n",
        ")\n",
        "\n",
        "source_no_hyper_galaxy = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    pixelization=pixelization,\n",
        "    adapt_galaxy_image=hyper_image_2d,\n",
        ")\n",
        "\n",
        "fit = fit_imaging_with_source_galaxy(\n",
        "    dataset=dataset, source_galaxy=source_no_hyper_galaxy\n",
        ")\n",
        "\n",
        "fit_plotter = aplt.FitImagingPlotter(fit=fit, include_2d=include)\n",
        "fit_plotter.subplot_fit()\n",
        "\n",
        "\n",
        "print(\"Evidence using baseline variances = \", 3885.2797)\n",
        "\n",
        "pixelization = al.Pixelization(\n",
        "    mesh=al.mesh.DelaunayBrightnessImage(\n",
        "        pixels=500, weight_floor=0.0, weight_power=5.0\n",
        "    ),\n",
        "    regularization=al.reg.AdaptiveBrightness(\n",
        "        inner_coefficient=0.001, outer_coefficient=0.2, signal_scale=2.0\n",
        "    ),\n",
        ")\n",
        "\n",
        "source_hyper_galaxy = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    pixelization=pixelization,\n",
        "    hyper_galaxy=al.legacy.HyperGalaxy(\n",
        "        contribution_factor=1.0, noise_factor=1.5, noise_power=1.0\n",
        "    ),\n",
        "    adapt_galaxy_image=hyper_image_2d,\n",
        "    adapt_model_image=hyper_image_2d,\n",
        ")\n",
        "\n",
        "fit = fit_imaging_with_source_galaxy(dataset=dataset, source_galaxy=source_hyper_galaxy)\n",
        "\n",
        "fit_plotter = aplt.FitImagingPlotter(fit=fit, include_2d=include)\n",
        "fit_plotter.subplot_fit()\n",
        "\n",
        "\n",
        "print(\"Evidence using variances scaling by hyper-galaxy = \", fit.log_evidence)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Wrap Up__\n",
        "\n",
        "Feel free to play around with the `noise_factor` and `noise_power` hyper-parameters above. It should be fairly \n",
        "clear what they do; they simply change the amount by which the noise is increased.\n",
        "\n",
        "And with that, we've completed the first of two tutorials on noise-map scaling. To end, I want you to have a quick \n",
        "think, is there anything else that you can think of that would mean we need to scale the noise? In this tutorial, \n",
        "it was the inadequacy of our mass-model that lead to significant residuals and a skewed chi-squared distribution. \n",
        "What else might cause residuals? I'll give you a couple below;\n",
        "\n",
        " 1) A mismatch between our model of the imaging data's Point Spread Function (PSF) and the true PSF of the telescope \n",
        " optics of the data.\n",
        "    \n",
        " 2) Unaccounted for effects in our data-reduction of the image, in particular the correlated signals and noise arising\n",
        " during the data reduction.\n",
        "    \n",
        " 3) A sub-optimal background sky subtraction of the image, which can leave large levels of signal in the outskirts of \n",
        " the image that are not due to the strong lens system itself.\n",
        "\n",
        "Oh, there is on more thing that can cause much worse residuals than all the effects above. That'll be the topic of \n",
        "the next tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}