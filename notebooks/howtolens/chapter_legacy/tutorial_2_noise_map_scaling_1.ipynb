{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 5: Noise Map Scaling 2\n",
        "===============================\n",
        "\n",
        "Noise-map scaling is important when our mass model lead to an inaccurate source reconstruction . However, it serves an\n",
        "even more important use, when another component of the lens model does not fit the data well. Can you think what it is?\n",
        "What could leave significant residuals in our model-fit? What might happen to also be the highest S/N values in our\n",
        "image, meaning these residuals contribute *even more* to the chi-squared distribution?\n",
        "\n",
        "It`s the lens galaxy's light. Just like an overly simplified mass profile`s means we can not perfectly reconstruct\n",
        "the source's light, the same is true of the Sersic profiles we use to fit the lens galaxy's light."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Initial Setup__\n",
        "\n",
        "we'll use the same strong lensing data as the previous tutorial, where:\n",
        "\n",
        " - The lens galaxy's light is an `Sersic`.\n",
        " - The lens galaxy's total mass distribution is an `Isothermal` and `ExternalShear`.\n",
        " - The source galaxy's light is an `Sersic`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"lens_sersic\"\n",
        "dataset_path = path.join(\"dataset\", \"imaging\", dataset_name)\n",
        "\n",
        "dataset = al.Imaging.from_fits(\n",
        "    data_path=path.join(dataset_path, \"data.fits\"),\n",
        "    noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "    psf_path=path.join(dataset_path, \"psf.fits\"),\n",
        "    pixel_scales=0.1,\n",
        ")\n",
        "\n",
        "mask = al.Mask2D.circular(\n",
        "    shape_native=dataset.shape_native,\n",
        "    pixel_scales=dataset.pixel_scales,\n",
        "    sub_size=2,\n",
        "    radius=3.0,\n",
        ")\n",
        "\n",
        "dataset = dataset.apply_mask(mask=mask)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, we'll use a convenience function to fit the lens data we simulated above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "def fit_imaging_with_lens_and_source_galaxy(dataset, lens_galaxy, source_galaxy):\n",
        "    tracer = al.Tracer.from_galaxies(galaxies=[lens_galaxy, source_galaxy])\n",
        "\n",
        "    return al.FitImaging(\n",
        "        dataset=dataset,\n",
        "        tracer=tracer,\n",
        "        settings_inversion=al.SettingsInversion(use_w_tilde=False),\n",
        "    )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, lets use this function to fit the lens data. we'll use a lens model with the correct mass model but an incorrect \n",
        "lens `LightProfile`. The source will use a magnification based grid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lens_galaxy = al.Galaxy(\n",
        "    redshift=0.5,\n",
        "    bulge=al.lp.Sersic(\n",
        "        centre=(0.0, 0.0),\n",
        "        ell_comps=al.convert.ell_comps_from(axis_ratio=0.9, angle=45.0),\n",
        "        intensity=0.8,\n",
        "        effective_radius=0.8,\n",
        "        sersic_index=4.0,\n",
        "    ),\n",
        "    mass=al.mp.Isothermal(\n",
        "        centre=(0.0, 0.0),\n",
        "        einstein_radius=1.6,\n",
        "        ell_comps=al.convert.ell_comps_from(axis_ratio=0.9, angle=45.0),\n",
        "    ),\n",
        "    shear=al.mp.ExternalShear(gamma_1=0.05, gamma_2=0.05),\n",
        ")\n",
        "\n",
        "pixelization = al.Pixelization(\n",
        "    mesh=al.mesh.DelaunayMagnification(shape=(30, 30)),\n",
        "    regularization=al.reg.Constant(coefficient=3.3),\n",
        ")\n",
        "\n",
        "source_magnification = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    pixelization=pixelization,\n",
        ")\n",
        "\n",
        "fit = fit_imaging_with_lens_and_source_galaxy(\n",
        "    dataset=dataset, lens_galaxy=lens_galaxy, source_galaxy=source_magnification\n",
        ")\n",
        "\n",
        "print(\"Evidence using baseline variances = \", fit.log_evidence)\n",
        "\n",
        "include = aplt.Include2D(mapper_image_plane_mesh_grid=True, mask=True)\n",
        "\n",
        "fit_plotter = aplt.FitImagingPlotter(fit=fit, include_2d=include)\n",
        "fit_plotter.subplot_fit()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Lens Subtraction__\n",
        "\n",
        "Okay, so its clear that our poor lens light subtraction leaves residuals in the lens galaxy's centre. These pixels \n",
        "are extremely high S/N, so they contribute large chi-squared values. For a real strong lens, we could not fit these \n",
        "residual features using a more complex light profile. These types of residuals are extremely common and they are \n",
        "caused by nasty, irregular morphological structures in the lens galaxy; nuclear star emission, nuclear rings, bars, etc.\n",
        "\n",
        "This skewed chi-squared distribution will cause all the same problems we discussed in the previous tutorial, like \n",
        "over-fitting. However, for the source-reconstruction and Bayesian evidence the residuals are even more problematic \n",
        "than before. Why? Because when we compute the Bayesian evidence for the source-inversion these pixels are included \n",
        "like all the other image pixels. But, __they do not contain the source__. The Bayesian evidence is going to try \n",
        "improve the fit to these pixels by reducing the level of regularization,  but its __is going to fail miserably__, as \n",
        "they map nowhere near the source!\n",
        "\n",
        "This is a fundamental problem when simultaneously modeling the lens galaxy's light and source galaxy. The source \n",
        "inversion has no way to distinguish whether the flux it is reconstructing belongs to the lens or source. This is \n",
        "why contribution maps are so valuable; by creating a contribution map for every galaxy in the image **PyAutoLens** has a \n",
        "means by which to distinguish which flux belongs to each component in the image! This is further aided by the \n",
        "pixelization and regularizations  adapting to the source morphology, as not only are they adapting to where the \n",
        "source is located they are adapting to where it is not (and therefore where the lens galaxy is).\n",
        "\n",
        "__Contribution Maps__\n",
        "\n",
        "Lets now create our hyper-images and use them create the contribution maps of our lens and source galaxies. \n",
        "Note below that we now create separate model images for our lens and source galaxies. This allows us to create \n",
        "contribution maps for each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hyper_image_2d = fit.model_image.slim\n",
        "\n",
        "hyper_image_lens = fit.model_images_of_planes_list[\n",
        "    0\n",
        "]  # This is the model image of the lens\n",
        "\n",
        "hyper_image_source = fit.model_images_of_planes_list[\n",
        "    1\n",
        "]  # This is the model image of the source\n",
        "\n",
        "lens_galaxy_hyper = al.Galaxy(\n",
        "    redshift=0.5,\n",
        "    bulge=al.lp.Sersic(\n",
        "        centre=(0.0, 0.0),\n",
        "        ell_comps=al.convert.ell_comps_from(axis_ratio=0.9, angle=45.0),\n",
        "        intensity=0.8,\n",
        "        effective_radius=0.8,\n",
        "        sersic_index=4.0,\n",
        "    ),\n",
        "    mass=al.mp.Isothermal(\n",
        "        centre=(0.0, 0.0), ell_comps=(0.111111, 0.0), einstein_radius=1.6\n",
        "    ),\n",
        "    hyper_galaxy=al.legacy.HyperGalaxy(\n",
        "        contribution_factor=0.3, noise_factor=4.0, noise_power=1.5\n",
        "    ),\n",
        "    adapt_model_image=hyper_image_2d,\n",
        "    adapt_galaxy_image=hyper_image_lens,  # <- The lens get its own hyper-galaxy image.\n",
        ")\n",
        "\n",
        "pixelization = al.Pixelization(\n",
        "    mesh=al.mesh.DelaunayMagnification(shape=(30, 30)),\n",
        "    regularization=al.reg.Constant(coefficient=3.3),\n",
        ")\n",
        "\n",
        "source_magnification_hyper = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    pixelization=pixelization,\n",
        "    hyper_galaxy=al.legacy.HyperGalaxy(\n",
        "        contribution_factor=2.0, noise_factor=2.0, noise_power=3.0\n",
        "    ),\n",
        "    adapt_galaxy_image=hyper_image_2d,\n",
        "    adapt_model_image=hyper_image_source,  # <- The source get its own hyper-galaxy image.\n",
        ")\n",
        "\n",
        "fit = fit_imaging_with_lens_and_source_galaxy(\n",
        "    dataset=dataset, lens_galaxy=lens_galaxy, source_galaxy=source_magnification\n",
        ")\n",
        "fit_plotter = aplt.FitImagingPlotter(fit=fit, include_2d=include)\n",
        "fit_plotter.subplot_fit()\n",
        "\n",
        "lens_contribution_map = lens_galaxy_hyper.hyper_galaxy.contribution_map_from(\n",
        "    adapt_model_image=hyper_image_2d, adapt_galaxy_image=hyper_image_lens\n",
        ")\n",
        "\n",
        "\n",
        "array_plotter = aplt.Array2DPlotter(array=lens_contribution_map)\n",
        "array_plotter.set_title(\"Lens Contribution Map\")\n",
        "array_plotter.figure_2d()\n",
        "\n",
        "source_contribution_map = source_magnification_hyper.hyper_galaxy.contribution_map_from(\n",
        "    adapt_model_image=hyper_image_2d, adapt_galaxy_image=hyper_image_source\n",
        ")\n",
        "\n",
        "array_plotter = aplt.Array2DPlotter(array=source_contribution_map)\n",
        "array_plotter.set_title(\"Source Contribution Map\")\n",
        "array_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The contribution maps decompose the image into its different components. Next, we use each contribution map to scale \n",
        "different regions of the noise-map. From the fit above it was clear that both the lens and source required the noise to \n",
        "be scaled, but their different chi-squared values ( > 150 and ~ 30) means they require different levels of noise-scaling. \n",
        "\n",
        "Lets see how much our fit improves and Bayesian evidence increases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit = fit_imaging_with_lens_and_source_galaxy(\n",
        "    dataset=dataset,\n",
        "    lens_galaxy=lens_galaxy_hyper,\n",
        "    source_galaxy=source_magnification_hyper,\n",
        ")\n",
        "\n",
        "fit_plotter = aplt.FitImagingPlotter(fit=fit, include_2d=include)\n",
        "fit_plotter.subplot_fit()\n",
        "\n",
        "print(\"Evidence using baseline variances = \", 492.0612)\n",
        "\n",
        "print(\"Evidence using hyper-galaxy hyper variances = \", fit.log_evidence)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great, and with that, we've covered galaxies. You might be wondering, what happens if there are multiple lens \n",
        "galaxies? or multiple source galaxies? Well, as you`d expect, **PyAutoLens** will make each a hyper-galaxy and \n",
        "therefore scale the noise-map of that individual galaxy in the image. This is what we want, as different parts of \n",
        "the image require different levels of noise-map scaling.\n",
        "\n",
        "__Hyper Data__\n",
        "\n",
        "Finally, I want to quickly mention two more ways that we change our data during th fitting process. One scales the \n",
        "background noise and one scales the image's background sky. To do this, we use the `hyper_data` module in **PyAutoLens**.\n",
        "\n",
        "This module includes all components of the model that scale parts of the data. To scale the background sky in the \n",
        "image we use the `HyperImageSky` class and input a `sky_scale`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hyper_image_sky = al.legacy.hyper_data.HyperImageSky(sky_scale=1.0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The sky_scale is literally just a constant value we add to every pixel of the observed image before fitting it \n",
        "therefore increasing or decreasing the background sky level in the image. This means we can account for an \n",
        "inaccurate background sky subtraction in our data reduction during **PyAutoLens** model fitting.\n",
        "\n",
        "We can also scale the background noise in an analogous fashion, using the `HyperBackgroundNoise` class and the \n",
        "`noise_scale` hyper-galaxy-parameter. This value is added to every pixel in the noise-map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hyper_background_noise = al.legacy.hyper_data.HyperBackgroundNoise(noise_scale=1.0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To use these hyper-galaxy-instrument parameters, we pass them to a lens-fit just like we do our `Tracer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tracer = al.Tracer.from_galaxies(\n",
        "    galaxies=[lens_galaxy_hyper, source_magnification_hyper]\n",
        ")\n",
        "\n",
        "fit = al.legacy.FitImaging(\n",
        "    dataset=dataset,\n",
        "    tracer=tracer,\n",
        "    hyper_image_sky=hyper_image_sky,\n",
        "    hyper_background_noise=hyper_background_noise,\n",
        "    settings_inversion=al.SettingsInversion(use_w_tilde=False),\n",
        ")\n",
        "\n",
        "fit_plotter = aplt.FitImagingPlotter(fit=fit, include_2d=include)\n",
        "fit_plotter.subplot_fit()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Wrap Up__\n",
        "\n",
        "Is there any reason to scale the background noise other than if the background sky subtraction has a large \n",
        "correction? There is. Lots of pixels in an image do not contain the lensed source but are fitted by the \n",
        "inversion. As we've learnt in this chapter, this isn't problematic when we have our adaptive regularization scheme \n",
        "because the regularization coefficient will be increased to large values.\n",
        "\n",
        "However, if you ran a full **PyAutoLens** model-fit in hyper-mode (which we cover in the next tutorial), you will\n",
        "find the method still dedicates a lot of source-pixels to fit these regions of the image, __even though they have no \n",
        "source__. Why is this? Its because although these pixels have no source, they still have a relatively high S/N values \n",
        "(of order 5-10) due to the lens galaxy (e.g. its flux before it is subtracted). The inversion when reconstructing the \n",
        "data `sees` pixels with a S/N > 1 and therefore wants to fit them with a high resolution.\n",
        "\n",
        "By increasing the background noise these pixels will go to much lower S/N values (<  1). The adaptive pixelization will\n",
        "feel no need to fit them properly and begin to fit these regions of the source-plane with far fewer, much bigger \n",
        "source pixels! This will again give us a net increase in Bayesian evidence, but more importantly, it will dramatically \n",
        "reduce the number of source pixels we use to fit the data. And what does fewer source-pixels mean? Much, much faster\n",
        "run times. Yay!\n",
        "\n",
        "With that, we have introduced every feature of hyper-galaxy-mode. The only thing left for us to do is to bring it \n",
        "all together and consider how we use all of these features in **PyAutoLens** pipelines. That is what we'll discuss in \n",
        "the next tutorial, and then you'll be ready to perform your own adapt-fits!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}