{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 7: Fitting\n",
        "===================\n",
        "\n",
        "In previous tutorials, we used light profiles to create simulated images of tracer and visualized how these images\n",
        "would appear when captured by a CCD detector on a telescope like the Hubble Space Telescope.\n",
        "\n",
        "However, this simulation process is the reverse of what astronomers typically do when analyzing real data. Usually,\n",
        "astronomers start with an observation\u2014an actual image of a strong lens - and aim to infer detailed information about the\n",
        "lens\u2019s properties, such as its mass and unlensed source properties.\n",
        "\n",
        "To achieve this, we must fit the observed image data with a model, identifying the combination of light and mass \n",
        "profiles that best matches the lens's appearance in the image. In this tutorial, we'll illustrate this process using \n",
        "the imaging data simulated in the previous tutorial. Our goal is to demonstrate how we can recover the parameters of \n",
        "the light profiles that we used to create the original simulation, as a proof of concept for the fitting procedure.\n",
        "\n",
        "The process of fitting data introduces essential statistical concepts like the `model`, `residual_map`, `chi-squared`,\n",
        "`likelihood`, and `noise_map`. These terms are crucial for understanding how fitting works, not only in astronomy but\n",
        "also in any scientific field that involves data modeling. This tutorial will provide a detailed introduction to these\n",
        "concepts and show how they are applied in practice to analyze astronomical data.\n",
        "\n",
        "Here is an overview of what we'll cover in this tutorial:\n",
        "\n",
        "- **Dataset**: Load the imaging dataset that we previously simulated, consisting of the image, noise map, and PSF.\n",
        "- **Mask**: Apply a mask to the data, excluding regions with low signal-to-noise ratios from the analysis.\n",
        "- **Masked Grid**: Create a masked grid, which contains only the coordinates of unmasked pixels, to evaluate the\n",
        "  galaxy's light profile in only unmasked regions.\n",
        "- **Fitting**: Fit the data with a galaxy model, computing key quantities like the model image, residuals,\n",
        "  chi-squared, and log likelihood to assess the quality of the fit.\n",
        "- **Bad Fits**: Demonstrate how even small deviations from the true parameters can significantly impact the fit.\n",
        "- **Model Fitting**: Perform a basic model fit on a simple dataset, adjusting the model parameters to improve the\n",
        "  fit quality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from os import path\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "We begin by loading the imaging dataset that we will use for fitting in this tutorial. This dataset is identical to the \n",
        "one we simulated in the previous tutorial, representing how a lens would appear if captured by a CCD camera.\n",
        "\n",
        "In the previous tutorial, we saved this dataset as .fits files in the `autolens_workspace/dataset/imaging/howtolens` \n",
        "folder. The `.fits` format is commonly used in astronomy for storing image data along with metadata, making it a\n",
        "standard for CCD imaging.\n",
        "\n",
        "The `dataset_path` below specifies where these files are located: `autolens_workspace/dataset/imaging/howtolens/`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_path = path.join(\"dataset\", \"imaging\", \"howtolens\")\n",
        "\n",
        "dataset = al.Imaging.from_fits(\n",
        "    data_path=path.join(dataset_path, \"data.fits\"),\n",
        "    noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "    psf_path=path.join(dataset_path, \"psf.fits\"),\n",
        "    pixel_scales=0.1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Imaging` object contains three key components: `data`, `noise_map`, and `psf`:\n",
        "\n",
        "- `data`: The actual image of the lens, which we will analyze.\n",
        "\n",
        "- `noise_map`: A map indicating the uncertainty or noise level in each pixel of the image, reflecting how much the \n",
        "  observed signal in each pixel might fluctuate due to instrumental or background noise.\n",
        "  \n",
        "- `psf`: The Point Spread Function, which describes how a point source of light is spread out in the image by the \n",
        "  telescope's optics. It characterizes the blurring effect introduced by the instrument.\n",
        "\n",
        "Let's print some values from these components and plot a summary of the dataset to refresh our understanding of the \n",
        "imaging data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Value of first pixel in imaging data:\")\n",
        "print(dataset.data.native[0, 0])\n",
        "print(\"Value of first pixel in noise map:\")\n",
        "print(dataset.noise_map.native[0, 0])\n",
        "print(\"Value of first pixel in PSF:\")\n",
        "print(dataset.psf.native[0, 0])\n",
        "\n",
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Mask__\n",
        "\n",
        "The signal-to-noise map of the image highlights areas where the signal (light from the lens and source tracer) \n",
        "is detected above the  background noise. Values above 3.0 indicate regions where the light is detected with a \n",
        "signal-to-noise ratio of at least 3, while values below 3.0 are dominated by noise, where the light is not \n",
        "clearly distinguishable.\n",
        "\n",
        "To ensure the fitting process focuses only on meaningful data, we typically mask out regions with low signal-to-noise \n",
        "ratios, removing areas dominated by noise from the analysis. This allows the fitting process to concentrate on the \n",
        "regions where the lens is clearly detected.\n",
        "\n",
        "Here, we create a `Mask2D` to exclude certain regions of the image from the analysis. The mask defines which parts of \n",
        "the image will be used during the fitting process.\n",
        "\n",
        "For our simulated image, a circular 3\" mask centered at the center of the image is appropriate, since the simulated \n",
        "lens was positioned at the center."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mask = al.Mask2D.circular(\n",
        "    shape_native=dataset.shape_native,\n",
        "    pixel_scales=dataset.pixel_scales,\n",
        "    radius=3.0,  # The circular mask's radius in arc-seconds\n",
        "    centre=(0.0, 0.0),  # center of the image which is also the center of the lens\n",
        ")\n",
        "\n",
        "print(mask)  # 1 = True, meaning the pixel is masked. Edge pixels are indeed masked.\n",
        "print(mask[48:53, 48:53])  # Central pixels are `False` and therefore unmasked."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can visualize the mask over the strong lens image using an `ImagingPlotter`, which helps us adjust the mask as needed. \n",
        "This is useful to ensure that the mask appropriately covers the lens and source light and does not exclude important \n",
        "regions.\n",
        "\n",
        "To overlay objects like a mask onto a figure, we use the `Visuals2D` object. This tool allows us to add custom \n",
        "visuals to any plot, providing flexibility in creating tailored visual representations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "visuals = aplt.Visuals2D(mask=mask)\n",
        "\n",
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset, visuals_2d=visuals)\n",
        "dataset_plotter.set_title(\"Imaging Data With Mask\")\n",
        "dataset_plotter.figures_2d(data=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once we are satisfied with the mask, we apply it to the imaging data using the `apply_mask()` method. This ensures \n",
        "that only the unmasked regions are considered during the analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset = dataset.apply_mask(mask=mask)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we plot the masked imaging data again, the mask is now automatically included in the plot, even though we did \n",
        "not explicitly pass it using the `Visuals2D` object. The plot also zooms into the unmasked area, showing only the \n",
        "region where we will focus our analysis. This is particularly helpful when working with large images, as it centers \n",
        "the view on the regions where the strong lens's signal is detected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "dataset_plotter.set_title(\"Masked Imaging Data\")\n",
        "dataset_plotter.figures_2d(data=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The mask is now stored as an additional attribute of the `Imaging` object, meaning it remains attached to the \n",
        "dataset. This makes it readily available when we pass the dataset to a `FitImaging` object for the fitting process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Mask2D:\")\n",
        "print(dataset.mask)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In earlier tutorials, we discussed how grids and arrays have `native` and `slim` representations:\n",
        "\n",
        "- `native`: Represents the original 2D shape of the data, maintaining the full pixel array of the image.\n",
        "- `slim`: Represents a 1D array containing only the values from unmasked pixels, allowing for more efficient \n",
        "  processing when working with large images.\n",
        "\n",
        "After applying the mask, the `native` and `slim` representations change as follows:\n",
        "\n",
        "- `native`: The 2D array keeps its original shape, [total_y_pixels, total_x_pixels], but masked pixels (those where \n",
        "  the mask is True) are set to 0.0.\n",
        "- `slim`: This now only contains the unmasked pixel values, reducing the array size \n",
        "  from [total_y_pixels * total_x_pixels] to just the number of unmasked pixels.\n",
        "\n",
        "Let's verify this by checking the shape of the data in its `slim` representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Number of unmasked pixels:\")\n",
        "print(dataset.data.native.shape)\n",
        "print(\n",
        "    dataset.data.slim.shape\n",
        ")  # This should be lower than the total number of pixels, e.g., 100 x 100 = 10,000"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `mask` object also has a `pixels_in_mask` attribute, which gives the number of unmasked pixels. This should \n",
        "match the size of the `slim` data structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(dataset.data.mask.pixels_in_mask)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the `slim` attribute to print the first unmasked values from the image and noise map:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"First unmasked image value:\")\n",
        "print(dataset.data.slim[0])\n",
        "print(\"First unmasked noise map value:\")\n",
        "print(dataset.noise_map.slim[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Additionally, we can verify that the `native` data structure has zeros at the edges where the mask is applied and \n",
        "retains non-zero values in the central unmasked regions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Example masked pixel in the image's native representation at its edge:\")\n",
        "print(dataset.data.native[0, 0])\n",
        "print(\"Example unmasked pixel in the image's native representation at its center:\")\n",
        "print(dataset.data.native[48, 48])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Masked Grid__\n",
        "\n",
        "In tutorials 1 and 2, we emphasized that the `Grid2D` object is crucial for evaluating a lens's light profile. This grid \n",
        "contains (y, x) coordinates for each pixel in the image and is used to ray-trace to the source plane and map out the \n",
        "positions where the source galaxy's light is calculated.\n",
        "\n",
        "From a `Mask2D`, we derive a `masked_grid`, which consists only of the coordinates of unmasked pixels. This ensures \n",
        "that light profile calculations focus exclusively on regions where the strong lens's light is detected, saving \n",
        "computational time and improving efficiency.\n",
        "\n",
        "Below, we plot the masked grid:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "masked_grid = mask.derive_grid.unmasked\n",
        "\n",
        "grid_plotter = aplt.Grid2DPlotter(grid=masked_grid)\n",
        "grid_plotter.set_title(\"Masked Grid2D\")\n",
        "grid_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By plotting this masked grid over the lens image, we can see that the grid aligns with the unmasked pixels of the \n",
        "image.\n",
        "\n",
        "This alignment **is crucial** for accurate fitting because it ensures that when we evaluate a strong lens's light \n",
        "profile, the calculations occur only at positions where we have real data from."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "visuals = aplt.Visuals2D(grid=masked_grid)\n",
        "imaging_plotter = aplt.ImagingPlotter(dataset=dataset, visuals_2d=visuals)\n",
        "imaging_plotter.set_title(\"Image Data With 2D Grid Overlaid\")\n",
        "imaging_plotter.figures_2d(data=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Fitting__\n",
        "\n",
        "Now that our data is masked, we are ready to proceed with the fitting process.\n",
        "\n",
        "Fitting the data is done using the `Galaxy` and `Tracer objects that we introduced in previous tutorials. We will start by \n",
        "setting up a `Tracer`` object, using the same galaxy configuration that we previously used to simulate the \n",
        "imaging data. This setup will give us what is known as a 'perfect' fit, as the simulated and fitted models are identical."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lens_galaxy = al.Galaxy(\n",
        "    redshift=0.5,\n",
        "    mass=al.mp.Isothermal(\n",
        "        centre=(0.0, 0.0), einstein_radius=1.6, ell_comps=(0.17647, 0.0)\n",
        "    ),\n",
        ")\n",
        "\n",
        "source_galaxy = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    bulge=al.lp.Sersic(\n",
        "        centre=(0.1, 0.1),\n",
        "        ell_comps=(0.0, 0.111111),\n",
        "        intensity=1.0,\n",
        "        effective_radius=1.0,\n",
        "        sersic_index=2.5,\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n",
        "tracer = al.Tracer(galaxies=[lens_galaxy, source_galaxy])\n",
        "\n",
        "tracer_plotter = aplt.TracerPlotter(tracer=tracer, grid=dataset.grid)\n",
        "tracer_plotter.figures_2d(image=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, let's plot the image of the tracer. This should look familiar, as it is the same image we saw in \n",
        "previous tutorials. The difference now is that we use the dataset's `grid`, which corresponds to the `masked_grid` \n",
        "we defined earlier. This means that the tracer image is only evaluated in the unmasked region, skipping calculations \n",
        "in masked regions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tracer_plotter = aplt.TracerPlotter(tracer=tracer, grid=dataset.grid)\n",
        "tracer_plotter.set_title(\"Tracer Image To Be Fitted\")\n",
        "tracer_plotter.figures_2d(image=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we proceed to fit the image by passing both the `Imaging` and `Tracer` objects to a `FitImaging` object. \n",
        "This object will compute key quantities that describe the fit\u2019s quality:\n",
        "\n",
        "`image`: Creates an image of the tracer using their image_2d_from() method.\n",
        "`model_data`: Convolves the tracer image with the data's PSF to account for the effects of telescope optics.\n",
        "`residual_map`: The difference between the model data and observed data.\n",
        "`normalized_residual_map`: Residuals divided by noise values, giving units of noise.\n",
        "`chi_squared_map`: Squares the normalized residuals.\n",
        "`chi_squared` and `log_likelihood`: Sums the chi-squared values to compute chi_squared, and converts this into \n",
        "a log_likelihood, which measures how well the model fits the data (higher values indicate a better fit).\n",
        "\n",
        "Let's create the fit and inspect each of these attributes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit = al.FitImaging(dataset=dataset, tracer=tracer)\n",
        "fit_imaging_plotter = aplt.FitImagingPlotter(fit=fit)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `model_data` represents the tracer's image after accounting for effects like PSF convolution. \n",
        "\n",
        "An important technical note is that when we mask data, we discussed above how the image of the tracer is not evaluated\n",
        "outside the mask and is set to zero. This is a problem for PSF convolution, as the PSF blurs light from these regions\n",
        "outside the mask but at its edge into the mask. They must be correctly evaluated to ensure the model image accurately\n",
        "represents the image data.\n",
        "\n",
        "The `FitImaging` object handles this internally, but evaluating the model image in the additional regions outside the mask\n",
        "that are close enough to the mask edge to be blurred into the mask. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"First model image pixel:\")\n",
        "print(fit.model_data.slim[0])\n",
        "fit_imaging_plotter.figures_2d(model_image=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Even before computing other fit quantities, we can normally assess if the fit is going to be good by visually comparing\n",
        "the `data` and `model_data` and assessing if they look similar.\n",
        "\n",
        "In this example, the tracer used to fit the data are the same as the tracer used to simulate it, so the two\n",
        "look very similar (the only difference is the noise in the image)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit_imaging_plotter.figures_2d(data=True)\n",
        "fit_imaging_plotter.figures_2d(model_image=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `residual_map` is the different between the observed image and model image, showing where in the image the fit is\n",
        "good (e.g. low residuals) and where it is bad (e.g. high residuals).\n",
        "\n",
        "The expression for the residual map is simply:\n",
        "\n",
        "\\[ \\text{residual} = \\text{data} - \\text{model\\_data} \\]\n",
        "\n",
        "The residual-map is plotted below, noting that all values are very close to zero because the fit is near perfect.\n",
        "The only non-zero residuals are due to noise in the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "residual_map = dataset.data - fit.model_data\n",
        "print(\"First residual-map pixel:\")\n",
        "print(residual_map.slim[0])\n",
        "\n",
        "print(\"First residual-map pixel via fit:\")\n",
        "print(fit.residual_map.slim[0])\n",
        "\n",
        "fit_imaging_plotter.figures_2d(residual_map=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Are these residuals indicative of a good fit to the data? Without considering the noise in the data, it's difficult \n",
        "to ascertain. That is, its hard to ascenrtain if a residual value is large or small because this depends on the\n",
        "amount of noise in that pixel.\n",
        "\n",
        "The `normalized_residual_map` divides the residual-map by the noise-map, giving the residual in units of the noise.\n",
        "Its expression is:\n",
        "\n",
        "\\[ \\text{normalized\\_residual} = \\frac{\\text{residual\\_map}}{\\text{noise\\_map}} = \\frac{\\text{data} - \\text{model\\_data}}{\\text{noise\\_map}} \\]\n",
        "\n",
        "If you're familiar with the concept of standard deviations (sigma) in statistics, the normalized residual map represents \n",
        "how many standard deviations the residual is from zero. For instance, a normalized residual of 2.0 (corresponding \n",
        "to a 95% confidence interval) means that the probability of the model underestimating the data by that amount is only 5%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "normalized_residual_map = residual_map / dataset.noise_map\n",
        "\n",
        "print(\"First normalized residual-map pixel:\")\n",
        "print(normalized_residual_map.slim[0])\n",
        "\n",
        "print(\"First normalized residual-map pixel via fit:\")\n",
        "print(fit.normalized_residual_map.slim[0])\n",
        "\n",
        "fit_imaging_plotter.figures_2d(normalized_residual_map=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we define the `chi_squared_map`, which is obtained by squaring the `normalized_residual_map` and serves as a \n",
        "measure of goodness of fit.\n",
        "\n",
        "The chi-squared map is calculated as:\n",
        "\n",
        "\\[ \\chi^2 = \\left(\\frac{\\text{data} - \\text{model\\_data}}{\\text{noise\\_map}}\\right)^2 \\]\n",
        "\n",
        "Squaring the normalized residual map ensures all values are positive. For instance, both a normalized residual of -0.2 \n",
        "and 0.2 would square to 0.04, indicating the same quality of fit in terms of `chi_squared`.\n",
        "\n",
        "As seen from the normalized residual map, it's evident that the model provides a good fit to the data, in this\n",
        "case because the chi-squared values are close to zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "chi_squared_map = (normalized_residual_map) ** 2\n",
        "print(\"First chi-squared pixel:\")\n",
        "print(chi_squared_map.slim[0])\n",
        "\n",
        "print(\"First chi-squared pixel via fit:\")\n",
        "print(fit.chi_squared_map.slim[0])\n",
        "\n",
        "fit_imaging_plotter.figures_2d(chi_squared_map=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we consolidate all the information in our `chi_squared_map` into a single measure of goodness-of-fit \n",
        "called `chi_squared`. \n",
        "\n",
        "It is defined as the sum of all values in the `chi_squared_map` and is computed as:\n",
        "\n",
        "\\[ \\chi^2 = \\sum \\left(\\frac{\\text{data} - \\text{model\\_data}}{\\text{noise\\_map}}\\right)^2 \\]\n",
        "\n",
        "This summing process highlights why ensuring all values in the chi-squared map are positive is crucial. If we \n",
        "didn't square the values (making them positive), positive and negative residuals would cancel each other out, \n",
        "leading to an inaccurate assessment of the model's fit to the data.\n",
        "\n",
        "The lower the `chi_squared`, the fewer residuals exist between the model's fit and the data, indicating a better \n",
        "overall fit!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "chi_squared = np.sum(chi_squared_map)\n",
        "print(\"Chi-squared = \", chi_squared)\n",
        "print(\"Chi-squared via fit = \", fit.chi_squared)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The reduced chi-squared is the `chi_squared` value divided by the number of data points (e.g., the number of pixels\n",
        "in the mask). \n",
        "\n",
        "This quantity offers an intuitive measure of the goodness-of-fit, as it normalizes the `chi_squared` value by the\n",
        "number of data points. That is, a reduced chi-squared of 1.0 indicates that the model provides a good fit to the data,\n",
        "because every data point is fitted with a chi-squared value of 1.0.\n",
        "\n",
        "A reduced chi-squared value significantly greater than 1.0 indicates that the model is not a good fit to the data,\n",
        "whereas a value significantly less than 1.0 suggests that the model is overfitting the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "reduced_chi_squared = chi_squared / dataset.mask.pixels_in_mask\n",
        "print(\"Reduced Chi-squared = \", reduced_chi_squared)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another quantity that contributes to our final assessment of the goodness-of-fit is the `noise_normalization`.\n",
        "\n",
        "The `noise_normalization` is computed as the logarithm of the sum of squared noise values in our data: \n",
        "\n",
        "\\[\n",
        "\\text{{noise\\_normalization}} = \\sum \\log(2 \\pi \\text{{noise\\_map}}^2)\n",
        "\\]\n",
        "\n",
        "This quantity is fixed because the noise-map remains constant throughout the fitting process. Despite this, \n",
        "including the `noise_normalization` is considered good practice due to its statistical significance.\n",
        "\n",
        "Understanding the exact meaning of `noise_normalization` isn't critical for our primary goal of successfully \n",
        "fitting a model to a dataset. Essentially, it provides a measure of how well the noise properties of our data align \n",
        "with a Gaussian distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "noise_normalization = np.sum(np.log(2 * np.pi * dataset.noise_map**2))\n",
        "print(\"Noise Normalization = \", noise_normalization)\n",
        "print(\"Noise Normalization via fit = \", fit.noise_normalization)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the `chi_squared` and `noise_normalization`, we can define a final goodness-of-fit measure known as \n",
        "the `log_likelihood`. \n",
        "\n",
        "This measure is calculated by taking the sum of the `chi_squared` and `noise_normalization`, and then multiplying the \n",
        "result by -0.5:\n",
        "\n",
        "\\[ \\text{log\\_likelihood} = -0.5 \\times \\left( \\chi^2 + \\text{noise\\_normalization} \\right) \\]\n",
        "\n",
        "Don't worry about why we multiply by -0.5; it's a standard practice in statistics to ensure the log likelihood is\n",
        "defined correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "log_likelihood = -0.5 * (chi_squared + noise_normalization)\n",
        "print(\"Log Likelihood = \", log_likelihood)\n",
        "print(\"Log Likelihood via fit = \", fit.log_likelihood)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the previous discussion, we noted that a lower \\(\\chi^2\\) value indicates a better fit of the model to the \n",
        "observed data. \n",
        "\n",
        "When we calculate the log likelihood, we take the \\(\\chi^2\\) value and multiply it by -0.5. This means that a \n",
        "higher log likelihood corresponds to a better model fit. Our goal when fitting models to data is to maximize the \n",
        "log likelihood.\n",
        "\n",
        "The **reduced \\(\\chi^2\\)** value provides an intuitive measure of goodness-of-fit. Values close to 1.0 suggest a \n",
        "good fit, while values below or above 1.0 indicate potential underfitting or overfitting of the data, respectively. \n",
        "In contrast, the log likelihood values can be less intuitive. For instance, a log likelihood value printed above \n",
        "might be around 5300.\n",
        "\n",
        "However, log likelihoods become more meaningful when we compare them. For example, if we have two models, one with \n",
        "a log likelihood of 5300 and the other with 5310 we can conclude that the first model fits the data better \n",
        "because it has a higher log likelihood by 10.0. \n",
        "\n",
        "In fact, the difference in log likelihood between models can often be associated with a probability indicating how \n",
        "much better one model fits the data compared to another. This can be expressed in terms of standard deviations (sigma). \n",
        "\n",
        "As a rule of thumb:\n",
        "\n",
        "- A difference in log likelihood of **2.5** suggests that one model is preferred at the **2.0 sigma** level.\n",
        "- A difference in log likelihood of **5.0** indicates a preference at the **3.0 sigma** level.\n",
        "- A difference in log likelihood of **10.0** suggests a preference at the **5.0 sigma** level.\n",
        "\n",
        "All these metrics can be visualized together using the `FitImagingPlotter` object, which offers a comprehensive \n",
        "overview of the fit quality. It also shows separate model images for the lens and source galaxies, and the appearance\n",
        "of the source galaxy in the image and source planes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit = al.FitImaging(dataset=dataset, tracer=tracer)\n",
        "\n",
        "fit_bad_imaging_plotter = aplt.FitImagingPlotter(fit=fit)\n",
        "fit_bad_imaging_plotter.subplot_fit()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you're familiar with model-fitting, you've likely encountered terms like 'residuals', 'chi-squared', \n",
        "and 'log_likelihood' before. \n",
        "\n",
        "These metrics are standard ways to quantify the quality of a model fit. They are applicable not only to 1D data but \n",
        "also to more complex data structures like 2D images, 3D data cubes, or any other multidimensional datasets.\n",
        "\n",
        "__Incorrect Fit___\n",
        "\n",
        "In the previous section, we successfully created and fitted a lens model to the image data, resulting in an \n",
        "excellent fit. The residual map and chi-squared map showed no significant discrepancies, indicating that the \n",
        "strong lens's light was accurately captured by our model. This optimal solution translates to one of the highest log \n",
        "likelihood values possible, reflecting a good match between the model and the observed data.\n",
        "\n",
        "Now, let's modify our lens model to create a fit that is close to the correct solution but slightly off. \n",
        "Specifically, we will slightly offset the center of the source galaxy by half a pixel (0.05\") in both the x and y \n",
        "directions. This change will allow us to observe how even small deviations from the true parameters can impact the \n",
        "quality of the fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lens_galaxy = al.Galaxy(\n",
        "    redshift=0.5,\n",
        "    mass=al.mp.Isothermal(\n",
        "        centre=(0.0, 0.0), einstein_radius=1.6, ell_comps=(0.17647, 0.0)\n",
        "    ),\n",
        ")\n",
        "\n",
        "source_galaxy = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    bulge=al.lp.Sersic(\n",
        "        centre=(0.15, 0.15),\n",
        "        ell_comps=(0.0, 0.111111),\n",
        "        intensity=1.0,\n",
        "        effective_radius=1.0,\n",
        "        sersic_index=2.5,\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n",
        "tracer = al.Tracer(galaxies=[lens_galaxy, source_galaxy])\n",
        "\n",
        "tracer_plotter = aplt.TracerPlotter(tracer=tracer, grid=dataset.grid)\n",
        "tracer_plotter.figures_2d(image=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After implementing this slight adjustment, we can now plot the fit. In doing so, we observe that residuals have \n",
        "emerged at the multiple images of the lensed source, which indicates a mismatch between our model and the data. \n",
        "Consequently, this discrepancy results in increased chi-squared values, which in turn affects our log likelihood."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit_bad = al.FitImaging(dataset=dataset, tracer=tracer)\n",
        "\n",
        "fit_bad_imaging_plotter = aplt.FitImagingPlotter(fit=fit_bad)\n",
        "fit_bad_imaging_plotter.subplot_fit()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we can compare the log likelihood of our current model to the log likelihood value we computed previously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Previous Likelihood:\")\n",
        "print(fit.log_likelihood)\n",
        "print(\"New Likelihood:\")\n",
        "print(fit_bad.log_likelihood)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, we observe that the log likelihood has decreased! This decline confirms that our new model is indeed a \n",
        "worse fit to the data compared to the original model.\n",
        "\n",
        "Now, let\u2019s change our lens model once more, this time setting it to a position that is far from the true parameters. \n",
        "We will offset the source's center significantly to see how this extreme deviation affects the fit quality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lens_galaxy = al.Galaxy(\n",
        "    redshift=0.5,\n",
        "    mass=al.mp.Isothermal(\n",
        "        centre=(0.0, 0.0), einstein_radius=1.6, ell_comps=(0.17647, 0.0)\n",
        "    ),\n",
        ")\n",
        "\n",
        "source_galaxy = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    bulge=al.lp.Sersic(\n",
        "        centre=(0.5, 0.5),\n",
        "        ell_comps=(0.0, 0.111111),\n",
        "        intensity=1.0,\n",
        "        effective_radius=1.0,\n",
        "        sersic_index=2.5,\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n",
        "tracer = al.Tracer(galaxies=[lens_galaxy, source_galaxy])\n",
        "\n",
        "fit_very_bad = al.FitImaging(dataset=dataset, tracer=tracer)\n",
        "\n",
        "fit_very_bad_imaging_plotter = aplt.FitImagingPlotter(\n",
        "    fit=fit_very_bad,\n",
        ")\n",
        "fit_very_bad_imaging_plotter.subplot_fit()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is now evident that this model provides a terrible fit to the data. The tracer do not resemble a plausible \n",
        "representation of our simulated strong lens dataset, which we already anticipated given that we generated the data ourselves!\n",
        "\n",
        "As expected, the log likelihood has dropped dramatically with this poorly fitting model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Previous Likelihoods:\")\n",
        "print(fit.log_likelihood)\n",
        "print(fit_bad.log_likelihood)\n",
        "print(\"New Likelihood:\")\n",
        "print(fit_very_bad.log_likelihood)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Fitting__\n",
        "\n",
        "In the previous sections, we used the true model to fit the data, which resulted in a high log likelihood and minimal \n",
        "residuals. We also demonstrated how even small deviations from the true parameters can significantly degrade the fit \n",
        "quality, reducing the log likelihood.\n",
        "\n",
        "In practice, however, we don't know the \"true\" model. For example, we might have an image of a strong lens observed with \n",
        "the Hubble Space Telescope, but the values for parameters like its `einstein_radius` and others are \n",
        "unknown. The process of determining the best-fit model is called model fitting, and it is the main topic of \n",
        "Chapter 2 of *HowToGalaxy*.\n",
        "\n",
        "To conclude this section, let's perform a basic, hands-on model fit to develop some intuition about how we can find \n",
        "the best-fit model. We'll start by loading a simple dataset that was simulated without any lens light, using \n",
        "an `IsothermalSph` lens  mass profile and `ExponentialCoreSph` source light profile, but the true parameters of these \n",
        "profiles are unknown."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"simple__no_lens_light__mass_sis\"\n",
        "dataset_path = path.join(\"dataset\", \"imaging\", dataset_name)\n",
        "\n",
        "dataset = al.Imaging.from_fits(\n",
        "    data_path=path.join(dataset_path, \"data.fits\"),\n",
        "    psf_path=path.join(dataset_path, \"psf.fits\"),\n",
        "    noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "    pixel_scales=0.1,\n",
        ")\n",
        "\n",
        "mask = al.Mask2D.circular(\n",
        "    shape_native=dataset.shape_native,\n",
        "    pixel_scales=dataset.pixel_scales,\n",
        "    radius=3.0,\n",
        ")\n",
        "\n",
        "dataset = dataset.apply_mask(mask=mask)\n",
        "\n",
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, you'll try to determine the best-fit model for this image, corresponding to the parameters used to simulate the \n",
        "dataset.\n",
        "\n",
        "We'll use the simplest possible approach: try different combinations of light and mass profile parameters and adjust \n",
        "them based on how well each model fits the data. You\u2019ll quickly find that certain parameters produce a much better fit \n",
        "than others. For example, determining the correct values of the `centre` should not take too long.\n",
        "\n",
        "Pay attention to the `log_likelihood` and the `residual_map` as you adjust the parameters. These will guide you in \n",
        "determining if your model is providing a good fit to the data. Aim to increase the log likelihood and reduce the \n",
        "residuals.\n",
        "\n",
        "Keep experimenting with different values for a while, seeing how small you can make the residuals and how high you \n",
        "can push the log likelihood. Eventually, you\u2019ll likely reach a point where further improvements become difficult, \n",
        "even after trying many different parameter values. This is a good point to stop and reflect on your first experience \n",
        "with model fitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "lens_galaxy = al.Galaxy(\n",
        "    redshift=0.5,\n",
        "    mass=al.mp.IsothermalSph(\n",
        "        centre=(1.0, 1.0), einstein_radius=1.0\n",
        "    ),  # These are the lens parameters you need to adjust\n",
        ")\n",
        "\n",
        "source_galaxy = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    bulge=al.lp.ExponentialCoreSph(\n",
        "        centre=(1.0, 1.0),\n",
        "        intensity=1.0,\n",
        "        effective_radius=1.0,\n",
        "        radius_break=0.025,  # These are the source parameters you need to adjust\n",
        "    ),\n",
        ")\n",
        "\n",
        "tracer = al.Tracer(galaxies=[lens_galaxy, source_galaxy])\n",
        "\n",
        "fit = al.FitImaging(dataset=dataset, tracer=tracer)\n",
        "\n",
        "fit_plotter = aplt.FitImagingPlotter(\n",
        "    fit=fit,\n",
        ")\n",
        "fit_plotter.subplot_fit()\n",
        "\n",
        "print(\"Log Likelihood:\")\n",
        "print(fit.log_likelihood)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Manually guessing model parameters repeatedly is a very inefficient and slow way to find the best fit. If the model \n",
        "were more complex\u2014say, if the source galaxy had additional light profile components beyond just its `bulge` (like a \n",
        "second `Sersic` profile representing a `disk`)\u2014the model would become so intricate that this manual approach \n",
        "would be practically impossible. This is definitely not how model fitting is done in practice.\n",
        "\n",
        "However, this exercise has given you a basic intuition for how model fitting works. The statistical inference tools \n",
        "that are actually used for model fitting will be introduced in Chapter 2. Interestingly, these tools are not entirely \n",
        "different from the approach you just tried. Essentially, they also involve iteratively testing models until those \n",
        "with high log likelihoods are found. The key difference is that a computer can perform this process thousands of \n",
        "times, and it does so in a much more efficient and strategic way.\n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "In this tutorial, you have learned how to fit a lens model to imaging data, a fundamental process in astronomy\n",
        "and statistical inference. \n",
        "\n",
        "Let's summarize what we have covered:\n",
        "\n",
        "- **Dataset**: We loaded the imaging dataset that we previously simulated, consisting of the tracer image, noise map,\n",
        "  and PSF.\n",
        "  \n",
        "- **Mask**: We applied a circular mask to the data, excluding regions with low signal-to-noise ratios from the analysis.\n",
        "\n",
        "- **Masked Grid**: We created a masked grid, which contains only the coordinates of unmasked pixels, to evaluate the\n",
        "  tracer's light profile.\n",
        "  \n",
        "- **Fitting**: We fitted the data with a lens model, computing key quantities like the model image, residuals,\n",
        "  chi-squared, and log likelihood to assess the quality of the fit.\n",
        "  \n",
        "- **Bad Fits**: We demonstrated how even small deviations from the true parameters can significantly impact the fit\n",
        "  quality, leading to decreased log likelihood values.\n",
        "  \n",
        "- **Model Fitting**: We performed a basic model fit on a simple dataset, adjusting the model parameters to improve the\n",
        "  fit quality.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}