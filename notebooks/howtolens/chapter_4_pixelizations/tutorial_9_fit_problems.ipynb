{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 1: Fit Problems\n",
        "========================\n",
        "\n",
        "To begin, make sure you have read the `introduction` file carefully, as a clear understanding of how the Bayesian\n",
        "evidence works is key to understanding this chapter!\n",
        "\n",
        "In the previous chapter we investigated two pixelization's: `Rectangular` and `DelaunayMagnification`. We argued that the\n",
        "latter was better than the former, because it dedicated more source-pixels to the regions of the source-plane where we\n",
        "had more data, e.g, the high-magnification regions. Therefore, we could fit the data using fewer source pixels,\n",
        "which improved computational efficiency and increased the Bayesian evidence.\n",
        "\n",
        "So far, we've used just one regularization scheme; `Constant`. As the name suggests, this scheme applies just one\n",
        "regularization coefficient when comparing source pixel fluxes to apply smoothing. Here is a recap of our discussion\n",
        "about regularization from chapter 4:\n",
        "\n",
        "-------------------------------------------- \n",
        "\n",
        "When the inversion reconstructs the source, it does not *only* compute the set of source-pixel fluxes that best-fit\n",
        "the image. It also regularizes this solution, whereby it goes to every pixel on the rectangular source-plane grid\n",
        "and computes the different between the reconstructed flux values of every source pixel with its 4 neighboring pixels.\n",
        "If the difference in flux is large the solution is penalized, reducing its log likelihood. You can think of this as\n",
        "us applying a 'smoothness prior' on the reconstructed source galaxy's light.\n",
        "\n",
        "This smoothing adds a 'penalty term' to the log likelihood of an inversion which is the summed difference between the\n",
        "reconstructed fluxes of every source-pixel pair multiplied by the `coefficient`. By setting the regularization\n",
        "coefficient to zero, we set this penalty term to zero, meaning that regularization is completely omitted.\n",
        "\n",
        "Why do we need to regularize our solution? We just saw why, if we do not apply this smoothness prior to the source, we\n",
        "`over-fit` the image and reconstruct a noisy source with lots of extraneous features. This is what the  large flux\n",
        "values located at the exterior regions of the source reconstruction above are. If the inversions's sole aim is to\n",
        "maximize the log likelihood, it can do this by fitting *everything* accurately, including the noise.\n",
        "\n",
        "----------------------------------------------\n",
        "\n",
        "When using a `ConstantSplit` regularization scheme, we regularize the source by adding up the difference in fluxes between\n",
        "all source-pixels multiplied by one single value of the regularization coefficient. This means that every\n",
        "single source pixel receives the same `level` of regularization, regardless of whether it is reconstructing the\n",
        "bright central regions of the source or its faint exterior regions.\n",
        "\n",
        "\n",
        "In this tutorial, we'll learn why our magnification-based pixelization and constant regularization schemes are not\n",
        "optimal. We'll inspect fits to three strong lenses, simulated using the same mass p[rofile but with different\n",
        "sources whose light profiles become gradually more compact. For all 3 fits, we'll use the same source-plane resolution\n",
        "and a regularization_coefficient that maximize the Bayesian evidence. Thus, these are the `best` source reconstructions\n",
        "we can hope to achieve when adapting to the magnification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Initial Setup__\n",
        "\n",
        "we'll use 3 sources whose `effective_radius` and `sersic_index` are changed such that each is more compact that the last."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "source_galaxy_flat = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    bulge=al.lp.Sersic(\n",
        "        centre=(0.0, 0.0),\n",
        "        ell_comps=(0.0, 0.15),\n",
        "        intensity=0.2,\n",
        "        effective_radius=0.5,\n",
        "        sersic_index=1.0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "source_galaxy_compact = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    bulge=al.lp.Sersic(\n",
        "        centre=(0.0, 0.0),\n",
        "        ell_comps=(0.0, 0.15),\n",
        "        intensity=0.2,\n",
        "        effective_radius=0.2,\n",
        "        sersic_index=2.5,\n",
        "    ),\n",
        ")\n",
        "\n",
        "source_galaxy_super_compact = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    bulge=al.lp.Sersic(\n",
        "        centre=(0.0, 0.0),\n",
        "        ell_comps=(0.0, 0.15),\n",
        "        intensity=0.2,\n",
        "        effective_radius=0.1,\n",
        "        sersic_index=4.0,\n",
        "    ),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The function below uses each source galaxy to simulate imaging data. It performs the usual tasks we are used to \n",
        "seeing (make the PSF, galaxies, tracer, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "def simulate_for_source_galaxy(source_galaxy):\n",
        "    grid = al.Grid2D.uniform(shape_native=(150, 150), pixel_scales=0.05, sub_size=2)\n",
        "\n",
        "    psf = al.Kernel2D.from_gaussian(\n",
        "        shape_native=(11, 11), sigma=0.05, pixel_scales=0.05\n",
        "    )\n",
        "\n",
        "    lens_galaxy = al.Galaxy(\n",
        "        redshift=0.5,\n",
        "        mass=al.mp.Isothermal(\n",
        "            centre=(0.0, 0.0), ell_comps=(0.111111, 0.0), einstein_radius=1.6\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    tracer = al.Tracer.from_galaxies(galaxies=[lens_galaxy, source_galaxy])\n",
        "\n",
        "    simulator = al.SimulatorImaging(\n",
        "        exposure_time=300.0,\n",
        "        psf=psf,\n",
        "        background_sky_level=100.0,\n",
        "        add_poisson_noise=True,\n",
        "        noise_seed=1,\n",
        "    )\n",
        "\n",
        "    return simulator.via_tracer_from(tracer=tracer, grid=grid)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Mask__\n",
        "\n",
        "we'll use a 3.0\" mask to fit all three of our sources."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mask = al.Mask2D.circular(\n",
        "    shape_native=(150, 150), pixel_scales=0.05, sub_size=2, radius=3.0\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Simulator__\n",
        "\n",
        "Now, lets simulate all 3 of our source's as to create `Imaging` data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_source_flat = simulate_for_source_galaxy(source_galaxy=source_galaxy_flat)\n",
        "\n",
        "dataset_source_compact = simulate_for_source_galaxy(source_galaxy=source_galaxy_compact)\n",
        "\n",
        "dataset_source_super_compact = simulate_for_source_galaxy(\n",
        "    source_galaxy=source_galaxy_super_compact\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Fitting__\n",
        "\n",
        "we'll make one more convenience function which fits the simulated imaging data with a `DelaunayMagniication` \n",
        "pixelization and `Constant` regularization scheme.\n",
        "\n",
        "We'll input the `coefficient` of each fit, so that for each simulated source we regularize it at an appropriate level. \n",
        "There is nothing new in this function you haven't seen before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "def fit_data_with_delaunay_magnification_pixelization(dataset, mask, coefficient):\n",
        "    dataset = dataset.apply_mask(mask=mask)\n",
        "\n",
        "    lens_galaxy = al.Galaxy(\n",
        "        redshift=0.5,\n",
        "        mass=al.mp.Isothermal(\n",
        "            centre=(0.0, 0.0), ell_comps=(0.111111, 0.0), einstein_radius=1.6\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    pixelization = al.Pixelization(\n",
        "        mesh=al.mesh.DelaunayMagnification(shape=(30, 30)),\n",
        "        regularization=al.reg.Constant(coefficient=coefficient),\n",
        "    )\n",
        "\n",
        "    source_galaxy = al.Galaxy(redshift=1.0, pixelization=pixelization)\n",
        "\n",
        "    tracer = al.Tracer.from_galaxies(galaxies=[lens_galaxy, source_galaxy])\n",
        "\n",
        "    return al.FitImaging(dataset=dataset, tracer=tracer)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Fit Problems__\n",
        "\n",
        "Lets fit our first source which was simulated using the flattest light profile. One should note that this uses the \n",
        "highest regularization coefficient of our 3 fits (as determined by maximizing the Bayesian log evidence)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit_flat = fit_data_with_delaunay_magnification_pixelization(\n",
        "    dataset=dataset_source_flat, mask=mask, coefficient=9.2\n",
        ")\n",
        "\n",
        "include = aplt.Include2D(mapper_image_plane_mesh_grid=True, mask=True)\n",
        "\n",
        "fit_plotter = aplt.FitImagingPlotter(fit=fit_flat, include_2d=include)\n",
        "fit_plotter.subplot_fit()\n",
        "fit_plotter.subplot_of_planes(plane_index=1)\n",
        "\n",
        "\n",
        "print(fit_flat.log_evidence)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The fit was *excellent*. There were effectively no residuals in the fit, and the source has been reconstructed using \n",
        "lots of pixels! Nice!\n",
        "\n",
        "Now, lets fit the next source, which is more compact."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit_compact = fit_data_with_delaunay_magnification_pixelization(\n",
        "    dataset=dataset_source_compact, mask=mask, coefficient=3.3\n",
        ")\n",
        "\n",
        "fit_plotter = aplt.FitImagingPlotter(fit=fit_compact, include_2d=include)\n",
        "fit_plotter.subplot_fit()\n",
        "fit_plotter.subplot_of_planes(plane_index=1)\n",
        "\n",
        "print(fit_compact.log_evidence)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Oh no! The fit does not look so good! Sure, we reconstruct *most* of the lensed source's structure, but there are two \n",
        "clear `blobs` in the residual-map where we are failing to reconstruct the central regions of the source galaxy.\n",
        "\n",
        "Take a second to think about why this might be. Is it the pixelization or how the regularization is applying smoothing?\n",
        "\n",
        "Finally, lets fit the very compact source. Given that the results for the compact source didn`t look so good, you`d \n",
        "be right in assuming this is just going to make things even worse. Again, think about why this might be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit_super_compact = fit_data_with_delaunay_magnification_pixelization(\n",
        "    dataset=dataset_source_super_compact, mask=mask, coefficient=3.1\n",
        ")\n",
        "\n",
        "fit_plotter = aplt.FitImagingPlotter(fit=fit_super_compact, include_2d=include)\n",
        "fit_plotter.subplot_fit()\n",
        "fit_plotter.subplot_of_planes(plane_index=1)\n",
        "\n",
        "print(fit_super_compact.log_evidence)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Discussion__\n",
        "\n",
        "Okay, so what did we learn? The more compact our source, the worse the fit. This happens even though we are using the \n",
        "*correct* lens mass model, telling us that something is going fundamentally wrong with our source reconstruction and \n",
        "inversion. Both the pixelization and regularization are to blame!\n",
        "\n",
        "*Pixelization*:\n",
        "\n",
        "The problem is the same one we found when we compared the `Rectangular` and `DelaunayMagnification` meshs in \n",
        "chapter 4. We are simply not dedicating enough source-pixels to the central regions of the source reconstruction, \n",
        "e.g. where it`s brightest. As the source becomes more compact, the source reconstruction no longer has enough \n",
        "resolution to resolve its fine-detailed central structure, causing the fit to the image to degrade.\n",
        "\n",
        "Think about it, as we made our sources more compact we go from reconstructing them using ~100 source pixels, to ~20 \n",
        "source pixels to ~ 10 source pixels. This is why we advocated not using the `Rectangular` meshin the previous \n",
        "chapter!\n",
        "\n",
        "It turns out that adapting to the magnification wasn`t the best idea all along. As we simulated more compact sources \n",
        "the magnification (which is determined via the mass model) did not change. So, we foolishly reconstructed each source\n",
        "using fewer and fewer pixels, leading to a worse and worse fit! Furthermore, these source's happened to be located in \n",
        "the highest magnification regions of the source plane! If the source's were further away from the centre of the \n",
        "caustic, the pixelization would use *even less* pixels to reconstruct it. That is NOT what we want!\n",
        "\n",
        "**Regularization**:\n",
        "\n",
        "Regularization also causes problems. When using a `ConstantSplit` regularization scheme, we regularize the source by \n",
        "adding up the difference in fluxes between all source-pixels multiplied by one single value, the regularization\n",
        "coefficient. This means that, every single source pixel receives the same `level` of regularization, regardless of \n",
        "whether it is reconstructing the bright central regions of the source or its faint exterior regions. \n",
        "\n",
        "To visualize this, we are going to plot the `regularization_weights`. The `FitImagingPlotter` does not have a\n",
        "method that is able to plot this attribute of the `Inversion`. However, the `FitImagingPlotter` has its own \n",
        "`InversionPlotter` which we can use to make this plot. The benefit of using this is that it inherits from the\n",
        "`FitImagingPlotter` properties like the caustics, so they appear on the figure (this would not happen if we manually \n",
        "set up an `InversionPlotter` as we did in previous tutorials."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "inversion_plotter = fit_plotter.inversion_plotter_of_plane(plane_index=1)\n",
        "inversion_plotter.figures_2d_of_pixelization(\n",
        "    pixelization_index=0, regularization_weights=True\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, all pixels are regularized with our input regularization_coefficient value of 3.6.\n",
        "\n",
        "Is this the best way to regularize the source? Well, it isn't. But why not? Its \n",
        "because different regions of the source demand different levels of regularization:\n",
        "\n",
        " 1) In the source's central regions its flux gradient is steepest; the change in flux between two source pixels is \n",
        " much larger than in the exterior regions where the gradient is flatter (or there is no source flux at all). To \n",
        " reconstruct the detailed structure of the source's cuspy inner regions, the regularization coefficient needs to \n",
        " be much lower to avoid over-smoothing.\n",
        "\n",
        " 2) On the flip side, the source reconstruction wants to assume a high regularization coefficient further out \n",
        " because the source's flux gradient is flat (or there is no source signal at all). Higher regularization coefficients \n",
        " will increase the Bayesian evidence because by smoothing more source-pixels it makes the solution `simpler`, given \n",
        " that correlating the flux in these source pixels the solution effectively uses fewer source-pixels (e.g. degrees of \n",
        " freedom).\n",
        "\n",
        "So, herein lies the pitfall of a constant regularization scheme. Some parts of the reconstructed source demand a \n",
        "low regularization coefficient whereas other parts want a high value. Unfortunately, we end up with an intermediate \n",
        "regularization coefficient that over-smooths the source's central regions whilst failing to fully correlate exterior \n",
        "pixels. Thus, by using an adaptive regularization scheme, new solutions that further increase the Bayesian evidence \n",
        "become accessible.\n",
        "\n",
        "**Noise Map**:\n",
        "\n",
        "Before we wrap up this tutorial, I want us to also consider the role of our noise-map and get you thinking about \n",
        "why we might want to scale its variances. Lets look at the super-compact fit again;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit_plotter.subplot_fit()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, whats the problem? Look closely at the `chi-squared image`. Here, you'll note that a small subset of our data \n",
        "have extremely large chi-squared values. This means our non-linear search (which is trying minimize chi-squared) is \n",
        "going to seek solutions which primarily only reduce these chi-squared values. For the image above a small subset of \n",
        "the data (e.g. < 5% of pixels) contributes to the majority of the log likelihood (e.g. > 95% of the overall chi-squared). \n",
        "This is *not* what we want, as instead of using the entire surface brightness profile of the lensed source galaxy to \n",
        "fit our lens model, we end up using only a small subset of its brightest pixels.\n",
        "\n",
        "In the context of the Bayesian log evidence things become even more problematic. The Bayesian log evidence is trying to \n",
        "achieve a well-defined solution; a solution that provides a reduced chi-squared of 1. This solution is poorly defined \n",
        "when the chi-squared image looks like the one above. When a subset of pixels have chi-squareds > 300, the only way \n",
        "to achieve a reduced chi-squared 1 is to reduce the chi-squareds of other pixels to 0, e.g. by over-fitting their \n",
        "noise. Thus, we quickly end up in a regime where the choice of regularization_coefficient is ill defined.\n",
        "\n",
        "With that, we have motivated hyper-mode. To put it simply, if we don't adapt our pixelization, regularization and \n",
        "noise-map, we will get solutions which reconstruct the source poorly, regularize the source sub-optimally and \n",
        "over-fit a small sub-set of image pixels. Clearly, we want adaptive pixelizations, regularization and noise-maps, which \n",
        "what we'll cover in this chapter!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}