{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Features: Pixelization\n",
        "======================\n",
        "\n",
        "A pixelization reconstructs the source galaxy\u2019s light on a grid of pixels, which is regularized using a prior that\n",
        "enforces a degree of smoothness in the solution.\n",
        "\n",
        "This script fits a source galaxy model that uses a pixelization to reconstruct the source\u2019s light. It employs a\n",
        "rectangular mesh with a constant regularization scheme, which together form the simplest pixelization and\n",
        "regularization choices available. Despite their simplicity, these choices provide fast and accurate solutions.\n",
        "\n",
        "For simplicity, the lens galaxy\u2019s light is omitted from both the simulated data and the model. For interferometer\n",
        "datasets, the lens light is rarely present and this is the common scenario.\n",
        "\n",
        "You may wish to first read the pixelization/fit.py example, which demonstrates how a pixelized source reconstruction\n",
        "is applied to a single dataset.\n",
        "\n",
        "pixelizations are covered in detail in Chapter 4 of the HowToLens lecture series.\n",
        "\n",
        "__Run Time Overview__\n",
        "\n",
        "Throughout the workspace, it has been emphasised that pixelized source reconstructions are computed using GPU or CPU\n",
        "via JAX, where the linear algebra fully exploits sparsity in a way which minimizes VRAM use. This example uses\n",
        "this functionality, and therefore is suitable for datasets with a low number of visibilities (e.g. < 10000) or\n",
        "many visibilities (E.g. tens of millions).\n",
        "\n",
        "This example fits the dataset with 273 visibilities used throughout the workspace, so the modeling runs in under 10\n",
        "minutes. Fitting a higher resolution dataset will only take an hour to a few hours.\n",
        "\n",
        "If your dataset contains many visibilities (e.g. millions), setting up the matrices for pixelized source reconstruction\n",
        "which speed up the linear algebra may take tens of minutes, or hours. Once you are comfortable with the API introduced\n",
        "in this example, the `feature/pixelization/many_visibilities_preparation` explains how this initial setup can be\n",
        "performed before lens modeling and saved to hard disk for fast loading before the model fit.\n",
        "\n",
        "__Contents__\n",
        "\n",
        "**Advantages & Disadvantages:** Benefits and drawbacks of using an MGE.\n",
        "**Positive Only Solver:** How a positive solution to the reconstructed source pixel fluxes can be ensured, but is often disabled for interferometer data.\n",
        "**Dataset & Mask:** Standard set up of imaging dataset that is fitted.\n",
        "**Pixelization:** How to create a pixelization, including a description of its inputs.\n",
        "**Model:** Composing a model using a pixelization and how it changes the number of free parameters.\n",
        "**Search & Analysis:** Standard set up of non-linear search and analysis.\n",
        "**Positions Likelihood:** Removing unphysical pixelized source solutions using a likelihood penalty using the lensed multiple images.\n",
        "**VRAM:** Profiling of pixelization VRAM use and discussion of how it compares to standard light profiles.\n",
        "**Run Time:** Profiling of pixelization run times and discussion of how they compare to standard light profiles.\n",
        "**Model-Fit:** Performs the model fit using standard API.\n",
        "**Result:** Pixelization results and visualizaiton.\n",
        "**Chaining:** How the advanced modeling feature, non-linear search chaining, can significantly improve lens modeling with pixelizaitons.\n",
        "**Result (Advanced):** API for various pixelization outputs (magnifications, mappings) which requires some polishing.\n",
        "**Simulate (Advanced):** Simulating a strong lens dataset with the inferred pixelized source.\n",
        "\n",
        "__Advantages__\n",
        "\n",
        "Many strongly lensed source galaxies exhibit complex, asymmetric, and irregular morphologies. Such structures\n",
        "cannot be well approximated by analytic light profiles such as a S\u00e9rsic profile, or even combinations of multiple\n",
        "S\u00e9rsic components. pixelizations are therefore required to accurately reconstruct this irregular source-plane light.\n",
        "\n",
        "Even alternative basis-function approaches, such as shapelets or multi-Gaussian expansions, struggle to accurately\n",
        "reconstruct sources with highly complex morphologies or multiple distinct source galaxies.\n",
        "\n",
        "Pixelized source models are also essential for robustly constraining detailed components of the lens mass\n",
        "distribution (e.g. the mass density slope or the presence of dark matter substructure). By fitting all of the lensed\n",
        "source light, they reduce degeneracies between the source and lens mass model.\n",
        "\n",
        "Finally, many science applications aim to study the highly magnified source galaxy itself, in order to learn about\n",
        "distant and intrinsically faint galaxies. pixelizations reconstruct the unlensed source emission, enabling detailed\n",
        "studies of the source-plane structure.\n",
        "\n",
        "For CCD imaging, a disadvantage of pixelized source reconstructions is they are the most computationally expensive\n",
        "modeling approach. However, for interferometer datasets, the way that JAX and GPUs can exploit the sparsity in the\n",
        "linear algebra means pixelized source reconstructions are both significantly faster than other approaches (E.g.\n",
        "light profiles) and can scale to millions of visibilities.\n",
        "\n",
        "__Disadvantages__\n",
        "\n",
        "Lens modeling with pixelizations is conceptually more complex. There are additional failure modes, such as\n",
        "solutions where the source is reconstructed in a highly demagnified configuration due to an unphysical lens mass\n",
        "model (e.g. too little or too much mass). These issues are discussed in detail later in the workspace.\n",
        "\n",
        "As a result, learning to successfully fit lens models with pixelizations typically requires more time and experience\n",
        "than the simpler modeling approaches introduced elsewhere in the workspace.\n",
        "\n",
        "__Positive Only Solver__\n",
        "\n",
        "Many codes which use linear algebra typically rely on a linear algabra solver which allows for positive and negative\n",
        "values of the solution (e.g. `np.linalg.solve`), because they are computationally fast.\n",
        "\n",
        "This could be problematic, as it means that negative surface brightnesses values can be computed to represent a galaxy's\n",
        "light, which is clearly unphysical. For a pixelizaiton, this often produces negative source pixels which over-fit\n",
        "the data, producing unphysical solutions.\n",
        "\n",
        "For CCD imaging datsets pixelized source reconstructions use a positive-only solver, meaning that every source-pixel\n",
        "is only allowed to reconstruct positive flux values. This ensures that the source reconstruction is physical and\n",
        "that we don't reconstruct negative flux values that don't exist in the real source galaxy (a common systematic\n",
        "solution in lens analysis).\n",
        "\n",
        "However, for interferometer datasets this positive-only solver is often disabled, because negative pixel values\n",
        "can be observed from the measurement process. All interferometer examples therefore disable the positive only solver,\n",
        "but you may want to consider if using the positive-only solver is appropriate for your dataset.\n",
        "\n",
        "__Model__\n",
        "\n",
        "This script fits an `Imaging` dataset of a 'galaxy-scale' strong lens with a model where:\n",
        "\n",
        " - The lens galaxy's light is omitted (and is not present in the simulated data).\n",
        " - The lens galaxy's total mass distribution is an `Isothermal` and `ExternalShear`.\n",
        " - The source galaxy's surface-brightness is reconstructed using a `RectangularMagnification` mesh\n",
        "   and `Constant` regularization scheme.\n",
        "\n",
        "__Start Here Notebook__\n",
        "\n",
        "If any code in this script is unclear, refer to the `modeling/start_here.ipynb` notebook.\n",
        "\n",
        "__High Resolution Dataset__\n",
        "\n",
        "A high-resolution `uv_wavelengths` file for ALMA is available in a separate repository that hosts large files which\n",
        "are too big to include in the main `autolens_workspace` repository:\n",
        "\n",
        "https://github.com/Jammy2211/autolens_workspace_large_files\n",
        "\n",
        "After downloading the file, place it in the directory:\n",
        "\n",
        "`autolens_workspace/dataset/interferometer/alma`\n",
        "\n",
        "You can then perform modeling using this high-resolution dataset by uncommenting the relevant line of code\n",
        "below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from autoconf import jax_wrapper  # Sets JAX environment before other imports\n",
        "\n",
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import autofit as af\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Mask__\n",
        "\n",
        "Define the \u2018real_space_mask\u2019 which defines the grid the image the strong lens is evaluated using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mask_radius = 3.5\n",
        "\n",
        "real_space_mask = al.Mask2D.circular(\n",
        "    shape_native=(256, 256),\n",
        "    pixel_scales=0.1,\n",
        "    radius=mask_radius,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "Load and plot the strong lens `Interferometer` dataset `simple` from .fits files, which we will fit \n",
        "with the lens model.\n",
        "\n",
        "This includes the method used to Fourier transform the real-space image of the strong lens to the uv-plane and compare \n",
        "directly to the visiblities. We use a non-uniform fast Fourier transform, which is the most efficient method for \n",
        "interferometer datasets containing ~1-10 million visibilities.\n",
        "\n",
        "If you want to use the high resolution ALMA dataset, uncomment the relevant lines of code below after downloading\n",
        "the data from the repository described in the \"High Resolution Dataset\" section above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"simple\"\n",
        "dataset_path = Path(\"dataset\") / \"interferometer\" / dataset_name\n",
        "\n",
        "# dataset_name = \"alma\"\n",
        "\n",
        "# if dataset_name == \"alma\":\n",
        "#\n",
        "#     real_space_mask = al.Mask2D.circular(\n",
        "#         shape_native=(800, 800),\n",
        "#         pixel_scales=0.01,\n",
        "#         radius=mask_radius,\n",
        "#     )\n",
        "\n",
        "dataset = al.Interferometer.from_fits(\n",
        "    data_path=dataset_path / \"data.fits\",\n",
        "    noise_map_path=dataset_path / \"noise_map.fits\",\n",
        "    uv_wavelengths_path=dataset_path / \"uv_wavelengths.fits\",\n",
        "    real_space_mask=real_space_mask,\n",
        "    transformer_class=al.TransformerDFT,\n",
        ")\n",
        "\n",
        "dataset_plotter = aplt.InterferometerPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()\n",
        "dataset_plotter.subplot_dirty_images()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__W_Tilde__\n",
        "\n",
        "Pixelized source modeling requires heavy linear algebra operations. These calculations are greatly accelerated\n",
        "using an alternative mathematical approach called the **w_tilde formalism**.\n",
        "\n",
        "You do not need to understand the full details of the method, but the key point is:\n",
        "\n",
        "- `w_tilde` exploits the **sparsity** of the matrices used in pixelized source reconstruction.\n",
        "- This leads to a **significant speed-up on GPU or CPU**, using JAX to perform the linear algebra calculations.\n",
        "\n",
        "To enable this feature, we call `apply_w_tilde()` on the dataset. This computes and stores a `w_tilde_preload` matrix,\n",
        "which reused in all subsequent pixelized source fits.\n",
        "\n",
        "On GPU via JAX, this computation is fast even for large datasets with many visibilities, with profiling\n",
        "of high resolution datasets with over 1 million visibilities showing that computation takes under 20 seconds. For\n",
        "10s or 100s of millions of visibilities computation on a GPU may stretch to minutes, but this is still very fast.\n",
        "\n",
        "On CPU, for datasets with over 100000 visibilities and many pixels in their real-space mask, this computation\n",
        "can take 10 minutes or hours (for the small dataset loaded above its miliseconds). The `show_progress` input outputs \n",
        "a progress bar to the terminal so you can monitor the computation, which is useful when it is slow.\n",
        "\n",
        "When computing it is slow, it is recommend you compute it once, save it to hard-disk, and load it\n",
        "before modeling. The example `pixelization/many_visibilities_preparation.py` illustrates how to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset = dataset.apply_w_tilde(use_jax=True, show_progress=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Settings__\n",
        "\n",
        "As discussed above, disable the default position only linear algebra solver so the source\n",
        "reconstruction can have negative pixel values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "settings_inversion = al.SettingsInversion(use_positive_only_solver=False)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Over Sampling__\n",
        "\n",
        "If you are familiar with using imaging data, you may have seen that a numerical technique called over sampling is used, \n",
        "which evaluates light profiles on a higher resolution grid than the image data to ensure the calculation is accurate.\n",
        "\n",
        "Interferometer does not observe galaxies in a way where over sampling is necessary, therefore all interferometer\n",
        "calculations are performed without over sampling.\n",
        "\n",
        "__JAX & Preloads__\n",
        "\n",
        "In JAX, calculations must use static shaped arrays with known and fixed indexes. For certain calculations in the\n",
        "pixelization, this information has to be passed in before the pixelization is performed. Below, we do this for 3\n",
        "inputs:\n",
        "\n",
        "- `total_linear_light_profiles`: The number of linear light profiles in the model. This is 0 because we are not\n",
        "  fitting any linear light profiles to the data, primarily because the lens light is omitted.\n",
        "\n",
        "- `total_mapper_pixels`: The number of source pixels in the rectangular pixelization mesh. This is required to set up \n",
        "  the arrays that perform the linear algebra of the pixelization.\n",
        "\n",
        "- `source_pixel_zeroed_indices`: The indices of source pixels on its edge, which when the source is reconstructed \n",
        "  are forced to values of zero, a technique tests have shown are required to give accruate lens models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mesh_shape = (20, 20)\n",
        "total_mapper_pixels = mesh_shape[0] * mesh_shape[1]\n",
        "\n",
        "total_linear_light_profiles = 0\n",
        "\n",
        "preloads = al.Preloads(\n",
        "    mapper_indices=al.mapper_indices_from(\n",
        "        total_linear_light_profiles=total_linear_light_profiles,\n",
        "        total_mapper_pixels=total_mapper_pixels,\n",
        "    ),\n",
        "    source_pixel_zeroed_indices=al.util.mesh.rectangular_edge_pixel_list_from(\n",
        "        total_linear_light_profiles=total_linear_light_profiles,\n",
        "        shape_native=mesh_shape,\n",
        "    ),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "We compose our lens model using `Model` objects, which represent the galaxies we fit to our data.  In this \n",
        "example fits a lens model where:\n",
        "\n",
        " - The lens galaxy's total mass distribution is an `Isothermal` and `ExternalShear` [7 parameters].\n",
        "\n",
        " - The source-galaxy's light uses a 20 x 20 `RectangularMagnification` mesh [0 parameters].\n",
        "\n",
        " - This pixelization is regularized using a `Constant` scheme which smooths every source pixel equally [1 parameter]. \n",
        "\n",
        "The number of free parameters and therefore the dimensionality of non-linear parameter space is N=6. \n",
        "\n",
        "It is worth noting the pixelization fits the source using significantly fewer parameters (1 parameter for \n",
        "regularization) than fitting the source using light profiles or an MGE (4+ parameters). \n",
        "\n",
        "The lens model therefore includes a mesh and regularization scheme, which are used together to create the \n",
        "pixelization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Lens:\n",
        "\n",
        "mass = af.Model(al.mp.PowerLaw)\n",
        "shear = af.Model(al.mp.ExternalShear)\n",
        "\n",
        "lens = af.Model(al.Galaxy, redshift=0.5, mass=mass, shear=shear)\n",
        "\n",
        "# Source:\n",
        "mesh = af.Model(al.mesh.RectangularMagnification, shape=mesh_shape)\n",
        "regularization = af.Model(al.reg.Constant)\n",
        "\n",
        "pixelization = af.Model(al.Pixelization, mesh=mesh, regularization=regularization)\n",
        "\n",
        "source = af.Model(al.Galaxy, redshift=1.0, pixelization=pixelization)\n",
        "\n",
        "# Overall Lens Model:\n",
        "\n",
        "model = af.Collection(galaxies=af.Collection(lens=lens, source=source))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `info` attribute shows the model in a readable format (if this does not display clearly on your screen refer to\n",
        "`start_here.ipynb` for a description of how to fix this).\n",
        "\n",
        "This confirms that the source galaxy's has a mesh and regularization scheme, which are combined into a pixelization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "The model is fitted to the data using the nested sampling algorithm Nautilus (see `start.here.py` for a \n",
        "full description)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.Nautilus(\n",
        "    path_prefix=Path(\"interferometer\"),\n",
        "    name=\"pixelization\",\n",
        "    unique_tag=dataset_name,\n",
        "    n_live=100,\n",
        "    n_batch=20,  # GPU lens model fits are batched and run simultaneously, see VRAM section below.\n",
        "    iterations_per_quick_update=50000,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Position Likelihood__\n",
        "\n",
        "We add a penalty term ot the likelihood function, which penalizes models where the brightest multiple images of\n",
        "the lensed source galaxy do not trace close to one another in the source plane. This removes \"demagnified source\n",
        "solutions\" from the source pixelization, which one is likely to infer without this penalty.\n",
        "\n",
        "A comprehensive description of why we do this is given at the following readthedocs page. I strongly recommend you \n",
        "read this page in full if you are not familiar with the positions likelihood penalty and demagnified source \n",
        "reconstructions:\n",
        "\n",
        " https://pyautolens.readthedocs.io/en/latest/general/demagnified_solutions.html\n",
        "\n",
        "__Brief Description__\n",
        "\n",
        "Unlike other example scripts, we also pass the `AnalysisImaging` object below a `PositionsLH` object, which\n",
        "includes the positions we loaded above, alongside a `threshold`.\n",
        "\n",
        "This is because `Inversion`'s suffer a bias whereby they fit unphysical lens models where the source galaxy is \n",
        "reconstructed as a demagnified version of the lensed source. \n",
        "\n",
        "To prevent these solutions biasing the model-fit we specify a `position_threshold` of 0.5\", which requires that a \n",
        "mass model traces the four (y,x) coordinates specified by our positions (that correspond to the brightest regions of the \n",
        "lensed source) within 0.5\" of one another in the source-plane. If this criteria is not met, a large penalty term is\n",
        "added to likelihood that massively reduces the overall likelihood. This penalty is larger if the ``positions``\n",
        "trace further from one another.\n",
        "\n",
        "This ensures the unphysical solutions that bias a pixelization have a lower likelihood that the physical solutions\n",
        "we desire. Furthermore, the penalty term reduces as the image-plane multiple image positions trace closer in the \n",
        "source-plane, ensuring Nautilus converges towards an accurate mass model. It does this very fast, as \n",
        "ray-tracing just a few multiple image positions is computationally cheap. \n",
        "\n",
        "The threshold of 0.3\" is large. For an accurate lens model we would anticipate the positions trace within < 0.01\" of\n",
        "one another. The high threshold ensures only the initial mass models at the start of the fit are penalized.\n",
        "\n",
        "Position thresholding is described in more detail in the \n",
        "script `autolens_workspace/*/guides/modeling/customize`\n",
        "\n",
        "The arc-second positions of the multiply imaged lensed source galaxy were drawn onto the\n",
        "image via the GUI described in the file `autolens_workspace/*/imaging/data_preparation/gui/positions.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "positions = al.Grid2DIrregular(\n",
        "    al.from_json(file_path=Path(dataset_path, \"positions.json\"))\n",
        ")\n",
        "\n",
        "positions_likelihood = al.PositionsLH(positions=positions, threshold=0.3)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "Create the `AnalysisInterferometer` object defining how the via Nautilus the model is fitted to the data. \n",
        "\n",
        "The `positions_likelihood_list` is passed to the analysis, which applies the likelihood penalty described above\n",
        "for everyone lens mass model.\n",
        "\n",
        "The `preloads` are passed to the analysis, which contain the static array information JAX needs to perform\n",
        "the pixelization calculations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = al.AnalysisInterferometer(\n",
        "    dataset=dataset,\n",
        "    positions_likelihood_list=[positions_likelihood],\n",
        "    preloads=preloads,\n",
        "    settings_inversion=settings_inversion,\n",
        "    use_jax=True,  # JAX will use GPUs for acceleration if available, else JAX will use multithreaded CPUs.\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__VRAM__\n",
        "\n",
        "The `modeling` example explains how VRAM is used during GPU-based fitting and how to print the estimated VRAM \n",
        "required by a model.\n",
        "\n",
        "pixelizations use a lot less VRAM than light profile-only models, provided the w-tilde sparsity-exploiting\n",
        "formalism is used (as it is above). In this mode, datasets with tens of millions of visibilities and real space\n",
        "masks with pixel scales below 0.05\" can be stored in just GB's of VRAM, which is remarkable given how much\n",
        "data they contain.\n",
        "\n",
        "In w-tilde mode, the **amount of VRAM used is independent of the number of visibilities in the dataset**. \n",
        "This is because the w-tilde method compresses all the visibility information into the `w_tilde_preload` matrix, \n",
        "whose size depends only on the number of pixels in the real-space mask. VRAM use is therefore mostly driven by\n",
        "how many pixels are in the real space mask.\n",
        "\n",
        "VRAM does scale with batch size though, and for high resoluiton datasets may require you to reduce from the value of \n",
        "20 set above if your GPU does not have too much VRAM (e.g. < 4GB)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis.print_vram_use(model=model, batch_size=search.batch_size)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Run Time__\n",
        "\n",
        "The run time of a pixelization are fast provided that the GPU VRAM exceeds the amount of memory required to perform\n",
        "a likelihood evaluation.\n",
        "\n",
        "The **run times of a pixelization are independent of the number of visibilities in the dataset**. This is again \n",
        "because the w-tilde method compresses all the visibility information into the `curvature_preload` matrix,  whose size \n",
        "depends only on the number of pixels in the real-space mask.\n",
        "\n",
        "Therefore, like VRAM, the main driver of trun time is the number of pixels in the real-space mask,\n",
        "not the number of visibilities in the dataset. The calculation also runs the same speed irrespective of whether\n",
        "the real space mask is circular, or irregularly shaped, therefore using a circlular mask is recommended as it is\n",
        "simpler to set up.\n",
        "\n",
        "Assuming the use of a 20 x 20 mesh grid above means this is the case, the run times of this model-fit on a GPU\n",
        "should take under 10 minutes. Increasing the batch size will speed up the fit, provided VRAM allows it.\n",
        "\n",
        "__Model-Fit__\n",
        "\n",
        "We begin the model-fit by passing the model and analysis object to the non-linear search (checkout the output folder\n",
        "for on-the-fly visualization and results)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "The search returns a result object, which whose `info` attribute shows the result in a readable format (if this\n",
        "does not display clearly on your screen refer to `start_here.ipynb` for a description of how to fix this):\n",
        "\n",
        "This confirms that the source galaxy's has a mesh and regularization scheme, which are combined into a pixelization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We plot the maximum likelihood fit, tracer images and posteriors inferred via Nautilus.\n",
        "\n",
        "The end of this example provides a detailed description of all result options for a pixelization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.max_log_likelihood_instance)\n",
        "\n",
        "tracer_plotter = aplt.TracerPlotter(\n",
        "    tracer=result.max_log_likelihood_tracer, grid=result.grids.lp\n",
        ")\n",
        "tracer_plotter.subplot_tracer()\n",
        "\n",
        "fit_plotter = aplt.FitImagingPlotter(fit=result.max_log_likelihood_fit)\n",
        "fit_plotter.subplot_fit()\n",
        "\n",
        "plotter = aplt.NestPlotter(samples=result.samples)\n",
        "plotter.corner_anesthetic()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The example `pixelization/fit` provides a full description of the different calculations that can be performed\n",
        "with the result of a pixelization model-fit.\n",
        "\n",
        "__Result Use__\n",
        "\n",
        "There are many things you can do with the result of a pixelixaiton, including analysing the reconstructing source, \n",
        "magnification calculations of the source and much more.\n",
        "\n",
        "These are documented in the `fit.py` example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "inversion = result.max_log_likelihood_fit.inversion"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Wrap Up__\n",
        "\n",
        "pixelizations are the most complex but also most powerful way to model a source galaxy.\n",
        "\n",
        "Whether you need to use them or not depends on the science you are doing. If you are only interested in measuring a\n",
        "simple quantity like the Einstein radius of a lens, you can get away with using light profiles like a Sersic, MGE or \n",
        "shapelets to model the source. Low resolution data also means that using a pixelization is not necessary, as the\n",
        "complex structure of the source galaxy is not resolved anyway.\n",
        "\n",
        "However, fitting complex mass models (e.g. a power-law, stellar / dark model or dark matter substructure) requires \n",
        "this level of complexity in the source model. Furthermore, if you are interested in studying the properties of the\n",
        "source itself, you won't find a better way to do this than using a pixelization.\n",
        "\n",
        "__Chaining__\n",
        "\n",
        "Modeling using a pixelization can be more efficient, robust and automated using the non-linear chaining feature to \n",
        "compose a pipeline which begins by fitting a simpler model using a parametric source.\n",
        "\n",
        "More information on chaining is provided in the `autolens_workspace/notebooks/guides/modeling/chaining` folder,\n",
        "chapter 3 of the **HowToLens** lectures.\n",
        "\n",
        "__HowToLens__\n",
        "\n",
        "A full description of how pixelizations work, which comes down to a lot of linear algebra, Bayesian statistics and\n",
        "2D geometry, is provided in chapter 4 of the **HowToLens** lectures.\n",
        "\n",
        "__Future Ideas / Contributions__\n",
        "\n",
        "Here are a list of things I would like to add to this tutorial but haven't found the time. If you are interested\n",
        "in having a go at adding them contact me on SLACK! :)\n",
        "\n",
        "- More magnification calculations.\n",
        "- Source gradient calculations.\n",
        "- A calculation which shows differential lensing effects (e.g. magnification across the source plane)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}