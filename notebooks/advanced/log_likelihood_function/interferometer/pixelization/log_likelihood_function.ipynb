{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Log Likelihood Function: Pixelization__\n",
        "\n",
        "This script provides a step-by-step guide of the **PyAutoLens** `log_likelihood_function` which is used to fit\n",
        "`Interferometer` data with an inversion (specifically a `Delaunay` mesh and `Constant` regularization scheme`).\n",
        "\n",
        "This script has the following aims:\n",
        "\n",
        " - To provide a resource that authors can include in papers using **PyAutoLens**, so that readers can understand the\n",
        " likelihood function (including references to the previous literature from which it is defined) without having to\n",
        " write large quantities of text and equations.\n",
        "\n",
        " - To make inversions in **PyAutoLens** less of a \"black-box\" to users.\n",
        "\n",
        "Accompanying this script is the `contributor_guide.py` which provides URL's to every part of the source-code that\n",
        "is illustrated in this guide. This gives contributors a sequential run through of what source-code functions, modules and\n",
        "packages are called when the likelihood is evaluated.\n",
        "\n",
        "__Prerequisites__\n",
        "\n",
        "The likelihood function of pixelizations is the most complicated likelihood function.\n",
        "\n",
        "It is advised you read through the following two simpler likelihood functions first, which break down a number of the\n",
        "concepts used in this script:\n",
        "\n",
        " - `interferometer/light_profile/log_likelihood_function.py` the likelihood function for a parametric light profile.\n",
        " - `imaging/linear_light_profile/log_likelihood_function.py` the likelihood function for a linear light profile, which\n",
        " introduces the linear algebra used for a pixelization but with a simpler use case.\n",
        "\n",
        "This script repeats all text and code examples in the above likelihood function examples. It therefore can be used to\n",
        "learn about the linear light profile likelihood function without reading other likelihood scripts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Mask__\n",
        "\n",
        "We define the \u2018real_space_mask\u2019 which defines the grid the image the galaxy is evaluated using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "real_space_mask = al.Mask2D.circular(\n",
        "    shape_native=(80, 80), pixel_scales=0.05, radius=4.0\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "Load and plot the galaxy `Interferometer` dataset `simple` from .fits files, which we will fit \n",
        "with the model.\n",
        "\n",
        "This includes the method used to Fourier transform the real-space image of the galaxy to the uv-plane and compare \n",
        "directly to the visibilities. We use a non-uniform fast Fourier transform, which is the most efficient method for \n",
        "interferometer datasets containing ~1-10 million visibilities. We will discuss how the calculation of the likelihood\n",
        "function changes for different methods of Fourier transforming in this guide."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"simple\"\n",
        "dataset_path = Path(\"dataset\") / \"interferometer\" / dataset_name\n",
        "\n",
        "dataset = al.Interferometer.from_fits(\n",
        "    data_path=dataset_path / \"data.fits\",\n",
        "    noise_map_path=dataset_path / \"noise_map.fits\",\n",
        "    uv_wavelengths_path=Path(dataset_path, \"uv_wavelengths.fits\"),\n",
        "    real_space_mask=real_space_mask,\n",
        "    transformer_class=al.TransformerDFT,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This guide uses in-built visualization tools for plotting. \n",
        "\n",
        "For example, using the `InterferometerPlotter` the dataset we perform a likelihood evaluation on is plotted.\n",
        "\n",
        "The `subplot_dataset` displays the visibilities in the uv-plane, which are the raw data of the interferometer\n",
        "dataset. These are what will ultimately be directly fitted in the Fourier space.\n",
        "\n",
        "The `subplot_dirty_images` displays the dirty images of the dataset, which are the reconstructed images of visibilities\n",
        "using an inverse Fourier transform to convert these to real-space. These dirty images are not the images we fit, but\n",
        "visualization of the dirty images are often used in radio interferometry to show the data in a way that is more\n",
        "interpretable to the human eye."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_plotter = aplt.InterferometerPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()\n",
        "dataset_plotter.subplot_dirty_images()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Over Sampling__\n",
        "\n",
        "Over sampling evaluates a light profile using multiple samples of its intensity per image-pixel.\n",
        "\n",
        "For simplicity, in previous likelihood function examples we disabled over sampling by setting `sub_size=1`. \n",
        "\n",
        "A full description of over sampling and how to use it is given in `autogalaxy_workspace/*/guides/over_sampling.py`.\n",
        "\n",
        "Over sampling is used for the same purpose in a pixelization, whereby it uses multiple samples of a pixel to\n",
        "perform the reconstruction via the pixelization. It uses an independent over sampling factor to the light profile\n",
        "over sampling factor, called `over_sample_size_pixelization`.\n",
        "\n",
        "However, for interferometer datasets, over sampling is not used in the pixelization (or for light profiles)\n",
        "therefore it is implicitly set to 1 and can be ignored hereafter.\n",
        "\n",
        "The notebook `log_likelihood_function/imaging/pixelization/with_over_sampling.ipynb` describes how the likelihood\n",
        "function of a pixelization changes when over sampling is used.\n",
        "\n",
        "__Masked Image Grid__\n",
        "\n",
        "To perform galaxy calculations we define a 2D image-plane grid of (y,x) coordinates.\n",
        "\n",
        "For light profiles these are given by `dataset.lp`, which is a uniform grid of (y,x) Cartesian coordinates\n",
        "which have had the 3.0\" circular mask applied.\n",
        "\n",
        "A pixelization uses a separate grid of (y,x) coordinates, called `dataset.grids.pixelization`, which is\n",
        "identical to the light profile grid but may of had a different over-sampling scale applied (but in this example\n",
        "does not).\n",
        "\n",
        "Each (y,x) coordinate coordinates to the centre of each image-pixel in the dataset, meaning that when this grid is\n",
        "used to construct a pixelization there is a straight forward mapping between the image data and pixelization pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grid_plotter = aplt.Grid2DPlotter(grid=dataset.grids.pixelization)\n",
        "grid_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Lens Galaxy__\n",
        "\n",
        "We set up a lens galaxy with the lens light and mass, which we will use to demonstrate a pixelized source\n",
        "reconstruction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mass = al.mp.Isothermal(\n",
        "    centre=(0.0, 0.0),\n",
        "    einstein_radius=1.6,\n",
        "    ell_comps=al.convert.ell_comps_from(axis_ratio=0.9, angle=45.0),\n",
        ")\n",
        "\n",
        "shear = al.mp.ExternalShear(gamma_1=0.05, gamma_2=0.05)\n",
        "\n",
        "lens_galaxy = al.Galaxy(redshift=0.5, mass=mass, shear=shear)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Source Galaxy Pixelization and Regularization__\n",
        "\n",
        "We combine the pixelization into a single `Galaxy` object.\n",
        "\n",
        "The galaxy includes the delaunay mesh and constant regularization scheme, which will ultimately be used\n",
        "to reconstruct its star forming clumps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pixelization = al.Pixelization(\n",
        "    image_mesh=al.image_mesh.Overlay(shape=(30, 30)),\n",
        "    mesh=al.mesh.Delaunay(),\n",
        "    regularization=al.reg.Constant(coefficient=1.0),\n",
        ")\n",
        "\n",
        "source_galaxy = al.Galaxy(redshift=1.0, pixelization=pixelization)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Source Pixel Centre Calculation__\n",
        "\n",
        "In order to reconstruct the source galaxy using a Delaunay mesh, we need to determine the centres of the Delaunay \n",
        "source pixels.\n",
        "\n",
        "The image-mesh `Overlay` object computes the source-pixel centres in the image-plane (which are ray-traced to the \n",
        "source-plane below). The source pixelization therefore adapts to the lens model magnification, because more\n",
        "source pixels will congregate in higher magnification regions.\n",
        "\n",
        "This calculation is performed by overlaying a uniform regular grid with an `pixelization_shape_2d` over the image\n",
        "mask and retaining all pixels that fall within the mask. This uses a `Grid2DSparse` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "image_plane_mesh_grid = pixelization.image_mesh.image_plane_mesh_grid_from(\n",
        "    mask=dataset.mask,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting this grid shows a sparse grid of (y,x) coordinates within the mask, which will form our source pixel centres."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "visuals = aplt.Visuals2D(grid=image_plane_mesh_grid)\n",
        "dataset_plotter = aplt.InterferometerPlotter(dataset=dataset, visuals_2d=visuals)\n",
        "dataset_plotter.figures_2d(dirty_image=True)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Ray Tracing__\n",
        "\n",
        "To perform lensing calculations we ray-trace every 2d (y,x) coordinate $\\theta$ from the image-plane to its (y,x) \n",
        "source-plane coordinate $\\beta$ using the summed deflection angles $\\alpha$ of the mass profiles:\n",
        "\n",
        " $\\beta = \\theta - \\alpha(\\theta)$\n",
        "\n",
        "The likelihood function of a pixelized source reconstruction ray-traces two grids from the image-plane to the source-plane:\n",
        "\n",
        " 1) A 2D grid of (y,x) coordinates aligned with the imaging data's image-pixels.\n",
        "\n",
        " 2) The sparse 2D grid of (y,x) coordinates above which form the centres of the Delaunay pixels.\n",
        "\n",
        "The function below computes the 2D deflection angles of the tracer's lens galaxies and subtracts them from the \n",
        "image-plane 2D (y,x) coordinates $\\theta$ of each grid, thus ray-tracing their coordinates to the source plane to \n",
        "compute their $\\beta$ values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tracer = al.Tracer(galaxies=[lens_galaxy, source_galaxy])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The source code gets quite complex when handling grids for a pixelization, but it is all handled in\n",
        "the `TracerToInversion` objects.\n",
        "\n",
        "The plots at the bottom of this cell show the traced grids used by the source pixelization, showing\n",
        "how the Delaunay mesh and traced image pixels are constructed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tracer_to_inversion = al.TracerToInversion(tracer=tracer, dataset=dataset)\n",
        "\n",
        "# A list of every grid (e.g. image-plane, source-plane) however we only need the source plane grid with index -1.\n",
        "traced_grid_pixelization = tracer.traced_grid_2d_list_from(\n",
        "    grid=dataset.grids.pixelization\n",
        ")[-1]\n",
        "\n",
        "# This functions a bit weird - it returns a list of lists of ndarrays. Best not to worry about it for now!\n",
        "traced_mesh_grid = tracer_to_inversion.traced_mesh_grid_pg_list[-1][-1]\n",
        "\n",
        "mat_plot = aplt.MatPlot2D(axis=aplt.Axis(extent=[-1.5, 1.5, -1.5, 1.5]))\n",
        "\n",
        "grid_plotter = aplt.Grid2DPlotter(grid=traced_grid_pixelization, mat_plot_2d=mat_plot)\n",
        "grid_plotter.figure_2d()\n",
        "\n",
        "grid_plotter = aplt.Grid2DPlotter(grid=traced_mesh_grid, mat_plot_2d=mat_plot)\n",
        "grid_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Border Relocation__\n",
        "\n",
        "Coordinates that are ray-traced near the mass profile centres are heavily demagnified and may trace to far outskirts of\n",
        "the source-plane. \n",
        "\n",
        "We relocate these pixels (for both grids above) to the edge of the source-plane border (defined via the border of the \n",
        "image-plane mask). This is detailed in **HowToLens chapter 4 tutorial 5** and figure 2 of https://arxiv.org/abs/1708.07377."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from autoarray.inversion.pixelization.border_relocator import BorderRelocator\n",
        "\n",
        "border_relocator = BorderRelocator(mask=dataset.mask, sub_size=1)\n",
        "\n",
        "relocated_grid = border_relocator.relocated_grid_from(grid=traced_grid_pixelization)\n",
        "\n",
        "relocated_mesh_grid = border_relocator.relocated_mesh_grid_from(\n",
        "    grid=traced_mesh_grid, mesh_grid=traced_mesh_grid\n",
        ")\n",
        "\n",
        "mat_plot = aplt.MatPlot2D(axis=aplt.Axis(extent=[-1.5, 1.5, -1.5, 1.5]))\n",
        "\n",
        "grid_plotter = aplt.Grid2DPlotter(grid=relocated_grid, mat_plot_2d=mat_plot)\n",
        "grid_plotter.figure_2d()\n",
        "\n",
        "grid_plotter = aplt.Grid2DPlotter(grid=relocated_mesh_grid, mat_plot_2d=mat_plot)\n",
        "grid_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Delaunay Mesh__\n",
        "\n",
        "The relocated pixelization grid is used to create the `Pixelization`'s Delaunay mesh using the `scipy.spatial` library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grid_delaunay = al.Mesh2DDelaunay(\n",
        "    values=relocated_mesh_grid,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting the Delaunay mesh shows that the source-plane and been discretized into a grid of irregular Delaunay pixels.\n",
        "\n",
        "(To plot the Delaunay mesh, we have to convert it to a `Mapper` object, which is described in the next likelihood step).\n",
        "\n",
        "Below, we plot the Delaunay mesh without the traced image-grid pixels (for clarity) and with them as black dots in order\n",
        "to show how each set of image-pixels fall within a Delaunay pixel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mapper_grids = al.MapperGrids(\n",
        "    mask=real_space_mask,\n",
        "    source_plane_data_grid=relocated_grid,\n",
        "    source_plane_mesh_grid=grid_delaunay,\n",
        "    image_plane_mesh_grid=image_plane_mesh_grid,\n",
        ")\n",
        "\n",
        "mapper = al.Mapper(\n",
        "    mapper_grids=mapper_grids,\n",
        "    regularization=None,\n",
        ")\n",
        "\n",
        "mapper_plotter = aplt.MapperPlotter(mapper=mapper)\n",
        "mapper_plotter.figure_2d(interpolate_to_uniform=False)\n",
        "\n",
        "visuals = aplt.Visuals2D(\n",
        "    grid=mapper_grids.source_plane_data_grid,\n",
        ")\n",
        "mapper_plotter = aplt.MapperPlotter(mapper=mapper, visuals_2d=visuals)\n",
        "mapper_plotter.figure_2d(interpolate_to_uniform=False)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Image-Source Mapping__\n",
        "\n",
        "We now combine grids computed above to create a `Mapper`, which describes how every image-plane pixel maps to\n",
        "every source-plane Delaunay pixel. \n",
        "\n",
        "There are two steps in this calculation, which we show individually below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mapper = al.Mapper(\n",
        "    mapper_grids=mapper_grids,\n",
        "    regularization=None,\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Image-Source Mapping__\n",
        "\n",
        "We now combine grids computed above to create a `Mapper`, which describes how every image-plane pixel maps to\n",
        "every source-plane Delaunay pixel. \n",
        "\n",
        "There are two steps in this calculation, which we show individually below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mapper = al.Mapper(\n",
        "    mapper_grids=mapper_grids,\n",
        "    regularization=None,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Mapper` contains:\n",
        "\n",
        " 1) `source_plane_data_grid`: the traced grid of (y,x) image-pixel coordinate centres (`relocated_grid`).\n",
        " 2) `source_plane_mesh_grid`: The Delaunay mesh of traced (y,x) source-pixel coordinates (`grid_delaunay`).\n",
        "\n",
        "We have therefore discretized the source-plane into a Delaunay mesh, and can pair every traced image-pixel coordinate\n",
        "with the corresponding Delaunay source pixel it lands in.\n",
        "\n",
        "This pairing is contained in the ndarray `pix_indexes_for_sub_slim_index` which maps every image-pixel index to \n",
        "every source-pixel index.\n",
        "\n",
        "In the API, the `pix_indexes` refers to the source pixel indexes (e.g. source pixel 0, 1, 2 etc.) and `sub_slim_index` \n",
        "refers to the index of an image pixel (e.g. image-pixel 0, 1, 2 etc.). \n",
        "\n",
        "For example, printing the first ten entries of `pix_indexes_for_sub_slim_index` shows the first ten source-pixel\n",
        "indexes these image sub-pixels map too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pix_indexes_for_sub_slim_index = mapper.pix_indexes_for_sub_slim_index\n",
        "\n",
        "print(pix_indexes_for_sub_slim_index[0:9])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This array can be used to visualize how an input list of image-pixel indexes map to the source-plane.\n",
        "\n",
        "It also shows that image-pixel indexing begins from the top-left and goes rightwards and downwards, accounting for \n",
        "all image-pixels which are not masked."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "visuals = aplt.Visuals2D(indexes=[list(range(2050, 2090))])\n",
        "\n",
        "mapper_plotter = aplt.MapperPlotter(\n",
        "    mapper=mapper,\n",
        "    visuals_2d=visuals,\n",
        ")\n",
        "mapper_plotter.subplot_image_and_mapper(\n",
        "    image=dataset.dirty_image, interpolate_to_uniform=False\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The reverse mappings of source-pixels to image-pixels can also be used.\n",
        "\n",
        "If we choose the right source-pixel index, we can see that multiple imaging occur whereby image-pixels in different\n",
        "regions of the image-plane are grouped into the same source-pixel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "visuals = aplt.Visuals2D(pix_indexes=[[200]])\n",
        "mapper_plotter = aplt.MapperPlotter(\n",
        "    mapper=mapper,\n",
        "    visuals_2d=visuals,\n",
        ")\n",
        "\n",
        "mapper_plotter.subplot_image_and_mapper(\n",
        "    image=dataset.dirty_image, interpolate_to_uniform=False\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Mapping Matrix__\n",
        "\n",
        "The `mapping_matrix` represents the image-pixel to source-pixel mappings above in a 2D matrix. \n",
        "\n",
        "It has dimensions `(total_image_pixels, total_source_pixels)`.\n",
        "\n",
        "(A number of inputs are not used for the `Delaunay` pixelization and are expanded upon in the `features.ipynb`\n",
        "log likelihood guide notebook)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "mapping_matrix = al.util.mapper.mapping_matrix_from(\n",
        "    pix_indexes_for_sub_slim_index=pix_indexes_for_sub_slim_index,\n",
        "    pix_size_for_sub_slim_index=mapper.pix_sizes_for_sub_slim_index,  # unused for Delaunay\n",
        "    pix_weights_for_sub_slim_index=mapper.pix_weights_for_sub_slim_index,  # unused for Delaunay\n",
        "    pixels=mapper.pixels,\n",
        "    total_mask_pixels=mapper.source_plane_data_grid.mask.pixels_in_mask,\n",
        "    slim_index_for_sub_slim_index=mapper.slim_index_for_sub_slim_index,\n",
        "    sub_fraction=np.array(mapper.over_sampler.sub_fraction),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A 2D plot of the `mapping_matrix` shows of all image-source pixel mappings.\n",
        "\n",
        "No row of pixels has more than one non-zero entry. It is not possible for two image pixels to map to the same source \n",
        "pixel (meaning that there are no correlated pixels in the mapping matrix)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.imshow(mapping_matrix, aspect=(mapping_matrix.shape[1] / mapping_matrix.shape[0]))\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each column of the `mapping_matrix` can therefore be used to show all image-pixels it maps too. \n",
        "\n",
        "For example, above, we plotted all image-pixels of source-pixel 200 (as well as 202 and 204). We can extract all\n",
        "image-pixel indexes of source pixels 200 using the `mapping_matrix` and use them to plot the image of this\n",
        "source-pixel (which corresponds to only values of zeros or ones)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "indexes_source_pix_200 = np.nonzero(mapping_matrix[:, 200])\n",
        "\n",
        "print(indexes_source_pix_200[0])\n",
        "\n",
        "array_2d = al.Array2D(values=mapping_matrix[:, 200], mask=dataset.mask)\n",
        "\n",
        "array_2d_plotter = aplt.Array2DPlotter(array=array_2d)\n",
        "array_2d_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Transformed Mapping Matrix ($f$)__\n",
        "\n",
        "Each pixelization pixel can therefore be thought of as an image (where all entries of this image are zeros and ones). \n",
        "\n",
        "However, for interferometer datasets we want to fit the visibilities in the uv-plane, not the image-plane. Therefore,\n",
        "each image in the `mapping_matrix` must be transformed to the uv-plane via a Fourier transform, such that each\n",
        "column in the `transformed_mapping_matrix` represents the visibilities in the uv-plane of each pixelization pixel.\n",
        "\n",
        "This operation changes the dimensions of the mapping matrix, meaning the `transformed_mapping_matrix` has\n",
        "dimensions `(total_image_pixels, total_visibilities)`. \n",
        "\n",
        "If the number of visibilities is large (e.g. 10^6) this matrix becomes extremely large and computationally expensive to \n",
        "store memory, meaning the `w_tilde` likelihood function, described in \n",
        "the `/log_likelihood_function/interferometer/`w_tilde.ipynb` notebook must be used instead.\n",
        "\n",
        "The `transformed_mapping_matrix` is also complex, storing all entries of the visibilities after the NUFFT as real\n",
        "and complex values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "transformed_mapping_matrix = dataset.transformer.transform_mapping_matrix(\n",
        "    mapping_matrix=mapping_matrix\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A 2D plot of the `transformed_mapping_matrix` shows all visibility-source pixel mappings.\n",
        "\n",
        "Note how, unlike for the `mapping_matrix`, every row of image-pixels fully consists of non-zero entries. This\n",
        "means the matrix is fully dense, making it even more difficult to store in memory for large datasets.\n",
        "\n",
        "Below, we plot the real and imaginary components of the `transformed_mapping_matrix` separately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.imshow(\n",
        "    transformed_mapping_matrix.real,\n",
        "    aspect=(transformed_mapping_matrix.shape[1] / transformed_mapping_matrix.shape[0]),\n",
        ")\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "plt.imshow(\n",
        "    transformed_mapping_matrix.imag,\n",
        "    aspect=(transformed_mapping_matrix.shape[1] / transformed_mapping_matrix.shape[0]),\n",
        ")\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each column of the `transformed_mapping_matrix` shows all visibilities it maps to after the NUFFT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "indexes_pix_200 = np.nonzero(transformed_mapping_matrix[:, 200])\n",
        "\n",
        "print(indexes_pix_200[0])\n",
        "\n",
        "visibilities = al.Visibilities(visibilities=transformed_mapping_matrix[:, 200])\n",
        "\n",
        "grid_plotter = aplt.Grid2DPlotter(grid=visibilities.in_grid)\n",
        "grid_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In Warren & Dye 2003 (https://arxiv.org/abs/astro-ph/0302587) the `transformed_mapping_matrix` is denoted $f_{ij}$\n",
        "where $i$ maps over all $I$ source pixels and $j$ maps over all $J$ visibilities. \n",
        "\n",
        "For example: \n",
        "\n",
        " - $f_{0, 2} = 0.3$ indicates that visibility number $2$ maps to pixelization pixel $0$ with a weight of $0.3$ after the NUFFT.\n",
        "\n",
        "The indexing of the `mapping_matrix` is reversed compared to the notation of WD03 (e.g. visibilities\n",
        "are the first entry of `mapping_matrix` whereas for $f$ they are the second index)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Mapping between visibility 0 and delaunay pixel 2 = {mapping_matrix[0, 2]}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Data Vector (D)__\n",
        "\n",
        "To solve for the delaunay pixel fluxes we now pose the problem as a linear inversion.\n",
        "\n",
        "This requires us to convert the `transformed_mapping_matrix` and our `data` and `noise map` into matrices of certain dimensions. \n",
        "\n",
        "The `data_vector`, $D$, is the first matrix and it has dimensions `(total_delaunay_pixels,)`.\n",
        "\n",
        "In WD03 (https://arxiv.org/abs/astro-ph/0302587) and N15 (https://arxiv.org/abs/1412.7436) the data vector \n",
        "is give by: \n",
        "\n",
        " $\\vec{D}_{i} = \\sum_{\\rm  j=1}^{J}f_{ij}(d_{j})/\\sigma_{j}^2 \\, \\, .$\n",
        "\n",
        "Where:\n",
        "\n",
        " - $d_{\\rm j}$ are the image-pixel data flux values.\n",
        " - $\\sigma{\\rm _j}^2$ are the statistical uncertainties of each image-pixel value.\n",
        "\n",
        "$i$ maps over all $I$ source pixels and $j$ maps over all $J$ image pixels. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data_vector = (\n",
        "    al.util.inversion_interferometer.data_vector_via_transformed_mapping_matrix_from(\n",
        "        transformed_mapping_matrix=transformed_mapping_matrix,\n",
        "        visibilities=np.array(dataset.data),\n",
        "        noise_map=np.array(dataset.noise_map),\n",
        "    )\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$D$ describes which delaunay pixels trace to which visibilities, with associated weights, after the NUFFT. This \n",
        "ensures the reconstruction fully accounts for the NUFFT when fitting the data.\n",
        "\n",
        "We can plot $D$ as a column vector:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.imshow(\n",
        "    data_vector.reshape(data_vector.shape[0], 1), aspect=10.0 / data_vector.shape[0]\n",
        ")\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dimensions of $D$ are the number of source pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Data Vector:\")\n",
        "print(data_vector)\n",
        "print(data_vector.shape)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Curvature Matrix (F)__\n",
        "\n",
        "The `curvature_matrix` $F$ is the second matrix and it has \n",
        "dimensions `(total_delaunay_pixels, total_delaunay_pixels)`.\n",
        "\n",
        "In WD03 / N15 (https://arxiv.org/abs/astro-ph/0302587) the curvature matrix is a 2D matrix given by:\n",
        "\n",
        " ${F}_{ik} = \\sum_{\\rm  j=1}^{J}f_{ij}f_{kj}/\\sigma_{j}^2 \\, \\, .$\n",
        "\n",
        "NOTE: this notation implicitly assumes a summation over $K$, where $k$ runs over all pixelization pixel indexes $K$.\n",
        "\n",
        "Note how summation over $J$ runs over $f$ twice, such that every entry of $F$ is the sum of the multiplication\n",
        "between all values in every two columns of $f$.\n",
        "\n",
        "For example, $F_{0,1}$ is the sum of all visibility values in $f$ of source pixel 0 multiplied by\n",
        "all visibility values of source pixel 1.\n",
        "\n",
        "Visibilities are both real and complex values, and the `curvature_matrix` is computed separately for the real and\n",
        "imaginary components of the visibilities and then summed together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "real_curvature_matrix = al.util.inversion.curvature_matrix_via_mapping_matrix_from(\n",
        "    mapping_matrix=transformed_mapping_matrix.real,\n",
        "    noise_map=dataset.noise_map.real,\n",
        ")\n",
        "\n",
        "imag_curvature_matrix = al.util.inversion.curvature_matrix_via_mapping_matrix_from(\n",
        "    mapping_matrix=transformed_mapping_matrix.imag,\n",
        "    noise_map=dataset.noise_map.imag,\n",
        ")\n",
        "\n",
        "curvature_matrix = np.add(real_curvature_matrix, imag_curvature_matrix)\n",
        "\n",
        "plt.imshow(curvature_matrix)\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For $F_{ik}$ to be non-zero, this requires that the images of delaunay pixels $i$ and $k$ share at least one\n",
        "image-pixel, which for visibilities after the NUFFT is always true for all $i$ and $k$.\n",
        "\n",
        "For example, we can see a non-zero entry for $F_{100,101}$ and plotting their images\n",
        "show overlap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "source_pixel_0 = 0\n",
        "source_pixel_1 = 1\n",
        "\n",
        "print(curvature_matrix[source_pixel_0, source_pixel_1])\n",
        "\n",
        "visibilities = al.Visibilities(\n",
        "    visibilities=transformed_mapping_matrix[:, source_pixel_0],\n",
        ")\n",
        "\n",
        "grid_plotter = aplt.Grid2DPlotter(grid=visibilities.in_grid)\n",
        "grid_plotter.figure_2d()\n",
        "\n",
        "visibilities = al.Visibilities(\n",
        "    visibilities=transformed_mapping_matrix[:, source_pixel_1],\n",
        ")\n",
        "\n",
        "grid_plotter = aplt.Grid2DPlotter(grid=visibilities.in_grid)\n",
        "grid_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following chi-squared is minimized when we perform the inversion and reconstruct the source_galaxy:\n",
        "\n",
        "$\\chi^2 = \\sum_{\\rm  j=1}^{J} \\bigg[ \\frac{(\\sum_{\\rm  i=1}^{I} s_{i} f_{ij}) - d_{j}}{\\sigma_{j}} \\bigg]$\n",
        "\n",
        "Where $s$ is the reconstructed pixel fluxes in all $I$ delaunay pixels.\n",
        "\n",
        "The solution for $s$ is therefore given by (equation 5 WD03):\n",
        "\n",
        " $s = F^{-1} D$\n",
        "\n",
        "We can compute this using NumPy linear algebra:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# Because we are no using regularizartion (see below) it is common for the curvature matrix to be singular and lead\n",
        "# to a LinAlgException. The loop below mitigates this -- you can ignore it as it is not important for understanding\n",
        "# the PyAutoLens likelihood function.\n",
        "\n",
        "for i in range(curvature_matrix.shape[0]):\n",
        "    curvature_matrix[i, i] += 1e-8\n",
        "\n",
        "reconstruction = np.linalg.solve(curvature_matrix, data_vector)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can plot this reconstruction -- it looks like a mess.\n",
        "\n",
        "The pixelization pixels have noisy and unsmooth values, and it is hard to make out if a source galaxy is even being \n",
        "reconstructed. \n",
        "\n",
        "In fact, the linear inversion is (over-)fitting noise in the image data, meaning this system of equations is \n",
        "ill-posed. We need to apply some form of smoothing on the reconstruction to avoid over fitting noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mapper_plotter = aplt.MapperPlotter(mapper=mapper)\n",
        "\n",
        "mapper_plotter.figure_2d(solution_vector=reconstruction, interpolate_to_uniform=False)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Regularization Matrix (H)__\n",
        "\n",
        "Regularization adds a linear regularization term $G_{\\rm L}$ to the $\\chi^2$ we solve for giving us a new merit \n",
        "function $G$ (equation 11 WD03):\n",
        "\n",
        " $G = \\chi^2 + \\lambda \\, G_{\\rm L}$\n",
        "\n",
        "where $\\lambda$ is the `regularization_coefficient` which describes the magnitude of smoothness that is applied. A \n",
        "higher $\\lambda$ will regularize the source more, leading to a smoother source galaxy reconstruction.\n",
        "\n",
        "Different forms for $G_{\\rm L}$ can be defined which regularize the reconstruction in different ways. The \n",
        "`Constant` regularization scheme used in this example applies gradient regularization (equation 14 WD03):\n",
        "\n",
        " $G_{\\rm L} = \\sum_{\\rm  i}^{I} \\sum_{\\rm  n=1}^{N}  [s_{i} - s_{i, v}]$\n",
        "\n",
        "This regularization scheme is easier to express in words -- the summation goes to each delaunay pixelization pixel,\n",
        "determines all delaunay pixels with which it shares a direct vertex (e.g. its neighbors) and penalizes solutions \n",
        "where the difference in reconstructed flux of these two neighboring pixels is large.\n",
        "\n",
        "The summation does this for all delaunay pixels, thus it favours solutions where neighboring delaunay \n",
        "pixels reconstruct similar values to one another (e.g. it favours a smooth source galaxy reconstruction).\n",
        "\n",
        "We now define the `regularization matrix`, $H$, which allows us to include this smoothing when we solve for $s$. $H$\n",
        "has dimensions `(total_delaunay_pixels, total_delaunay_pixels)`.\n",
        "\n",
        "This relates to $G_{\\rm L}$ as (equation 13 WD03):\n",
        "\n",
        " $H_{ik} = \\frac{1}{2} \\frac{\\partial G_{\\rm L}}{\\partial s_{i} \\partial s_{k}}$\n",
        "\n",
        "$H$ has the `regularization_coefficient` $\\lambda$ folded into it such $\\lambda$'s control on the degree of smoothing\n",
        "is accounted for."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "regularization_matrix = al.util.regularization.constant_regularization_matrix_from(\n",
        "    coefficient=source_galaxy.pixelization.regularization.coefficient,\n",
        "    neighbors=mapper.source_plane_mesh_grid.neighbors,\n",
        "    neighbors_sizes=mapper.source_plane_mesh_grid.neighbors.sizes,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can plot the regularization matrix and note that:\n",
        "\n",
        " - non-zero entries indicate that two delaunay pixelization pixels are neighbors and therefore are regularized \n",
        " with one another.\n",
        "\n",
        " - Zeros indicate the two delaunay pixels do not neighbor one another.\n",
        "\n",
        "The majority of entries are zero, because the majority of delaunay pixels are not neighbors with one another."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.imshow(regularization_matrix)\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__F + Lamdba H__\n",
        "\n",
        "$H$ enters the linear algebra system we solve for as follows (WD03 equation (12)):\n",
        "\n",
        " $s = [F + H]^{-1} D$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "curvature_reg_matrix = np.add(curvature_matrix, regularization_matrix)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Galaxy Reconstruction (s)__\n",
        "\n",
        "We can now solve the linear system above using NumPy linear algebra. \n",
        "\n",
        "Note that the for loop used above to prevent a LinAlgException is no longer required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "reconstruction = np.linalg.solve(curvature_reg_matrix, data_vector)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By plotting this source galaxy reconstruction we can see that regularization has lead us to reconstruct a smoother \n",
        "source galaxy, which actually looks like the star forming clumps in the imaging data! \n",
        "\n",
        "This also implies we are not over-fitting the noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mapper_plotter = aplt.MapperPlotter(mapper=mapper)\n",
        "\n",
        "mapper_plotter.figure_2d(solution_vector=reconstruction, interpolate_to_uniform=False)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Visibilities Reconstruction__\n",
        "\n",
        "Using the reconstructed pixel fluxes we can map the reconstruction back to the image plane (via \n",
        "the `blurred mapping_matrix`) and produce a reconstruction of the image data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mapped_reconstructed_visibilities = (\n",
        "    al.util.inversion_interferometer.mapped_reconstructed_visibilities_from(\n",
        "        transformed_mapping_matrix=transformed_mapping_matrix,\n",
        "        reconstruction=reconstruction,\n",
        "    )\n",
        ")\n",
        "\n",
        "mapped_reconstructed_visibilities = al.Visibilities(\n",
        "    visibilities=mapped_reconstructed_visibilities\n",
        ")\n",
        "\n",
        "grid_plotter = aplt.Grid2DPlotter(grid=mapped_reconstructed_visibilities.in_grid)\n",
        "grid_plotter.figure_2d()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Likelihood Function__\n",
        "\n",
        "We now quantify the goodness-of-fit of our pixelization source galaxy reconstruction. \n",
        "\n",
        "We compute the `log_likelihood` of the fit, which is the value returned by the `log_likelihood_function`.\n",
        "\n",
        "The likelihood function for source galaxy modeling consists of five terms:\n",
        "\n",
        " $-2 \\mathrm{ln} \\, \\epsilon = \\chi^2 + s^{T} H s + \\mathrm{ln} \\, \\left[ \\mathrm{det} (F + H) \\right] - { \\mathrm{ln}} \\, \\left[ \\mathrm{det} (H) \\right] + \\sum_{\\rm  j=1}^{J} { \\mathrm{ln}} \\left [2 \\pi (\\sigma_j)^2 \\right]  \\, .$\n",
        "\n",
        "This expression was first derived by Suyu 2006 (https://arxiv.org/abs/astro-ph/0601493) and is given by equation (19).\n",
        "It was derived into **PyAutoLens** notation in Dye 2008 (https://arxiv.org/abs/0804.4002) equation (5).\n",
        "\n",
        "We now explain what each of these terms mean.\n",
        "\n",
        "__Chi Squared__\n",
        "\n",
        "The first term is a $\\chi^2$ statistic, which is defined above in our merit function as and is computed as follows:\n",
        "\n",
        " - `model_data` = `mapped_reconstructed_visibilities`\n",
        " - `residual_map` = (`data` - `model_data`)\n",
        " - `normalized_residual_map` = (`data` - `model_data`) / `noise_map`\n",
        " - `chi_squared_map` = (`normalized_residuals`) ** 2.0 = ((`data` - `model_data`)**2.0)/(`variances`)\n",
        " - `chi_squared` = sum(`chi_squared_map`)\n",
        "\n",
        "The chi-squared therefore quantifies if our fit to the data is accurate or not. \n",
        "\n",
        "High values of chi-squared indicate that there are many image pixels our model did not produce a good fit to the image \n",
        "for, corresponding to a fit with a lower likelihood."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_visibilities = mapped_reconstructed_visibilities\n",
        "\n",
        "residual_map = dataset.data - model_visibilities\n",
        "\n",
        "\n",
        "normalized_residual_map_real = (residual_map.real / dataset.noise_map.real).astype(\n",
        "    \"complex128\"\n",
        ")\n",
        "normalized_residual_map_imag = (residual_map.imag / dataset.noise_map.imag).astype(\n",
        "    \"complex128\"\n",
        ")\n",
        "normalized_residual_map = (\n",
        "    normalized_residual_map_real + 1j * normalized_residual_map_imag\n",
        ")\n",
        "\n",
        "\n",
        "chi_squared_map_real = (residual_map.real / dataset.noise_map.real) ** 2\n",
        "chi_squared_map_imag = (residual_map.imag / dataset.noise_map.imag) ** 2\n",
        "chi_squared_map = chi_squared_map_real + 1j * chi_squared_map_imag\n",
        "\n",
        "\n",
        "chi_squared_real = np.sum(chi_squared_map.real)\n",
        "chi_squared_imag = np.sum(chi_squared_map.imag)\n",
        "chi_squared = chi_squared_real + chi_squared_imag\n",
        "\n",
        "print(chi_squared)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `chi_squared_map` indicates which regions of the image we did and did not fit accurately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "chi_squared_map = al.Visibilities(visibilities=chi_squared_map)\n",
        "\n",
        "grid_plotter = aplt.Grid2DPlotter(grid=chi_squared_map.in_grid)\n",
        "grid_plotter.figure_2d()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Regularization Term__\n",
        "\n",
        "The second term, $s^{T} H s$, corresponds to the $\\lambda $G_{\\rm L}$ regularization term we added to our merit \n",
        "function above.\n",
        "\n",
        "This is the term which sums up the difference in flux of all reconstructed delaunay pixels, and reduces the \n",
        "likelihood of solutions where there are large differences in flux (e.g. the source galaxy is less smooth and more \n",
        "likely to be overfitting noise).\n",
        "\n",
        "We compute it below via matrix multiplication, noting that the `regularization_coefficient`, $\\lambda$, is built into \n",
        "the `regularization_matrix` already."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "regularization_term = np.matmul(\n",
        "    reconstruction.T, np.matmul(regularization_matrix, reconstruction)\n",
        ")\n",
        "\n",
        "print(regularization_term)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Complexity Terms__\n",
        "\n",
        "Up to this point, it is unclear why we chose a value of `regularization_coefficient=1.0`. \n",
        "\n",
        "We cannot rely on the `chi_squared` and `regularization_term` above to optimally choose its value, because increasing \n",
        "the `regularization_coefficient` smooths the solution more and therefore:\n",
        "\n",
        " - Decreases `chi_squared` by fitting the data worse, producing a lower `log_likelihood`.\n",
        "\n",
        " - Increases the `regularization_term` by penalizing the differences between source pixel fluxes more, again reducing\n",
        " the inferred `log_likelihood`.\n",
        "\n",
        "If we set the regularization coefficient based purely on these two terms, we would set a value of 0.0 and be back where\n",
        "we started over-fitting noise!\n",
        "\n",
        "The terms $\\left[ \\mathrm{det} (F + H) \\right]$ and $ - { \\mathrm{ln}} \\, \\left[ \\mathrm{det} (H) \\right]$ address \n",
        "this problem. \n",
        "\n",
        "They quantify how complex the reconstruction is, and penalize solutions where *it is more complex*. Reducing \n",
        "the `regularization_coefficient` makes the source galaxy reconstruction more complex (because a galaxy that is \n",
        "smoothed less uses more flexibility to fit the data better).\n",
        "\n",
        "These two terms therefore counteract the `chi_squared` and `regularization_term`, so as to attribute a higher\n",
        "`log_likelihood` to solutions which fit the data with a more smoothed and less complex source (e.g. one with a higher \n",
        "`regularization_coefficient`).\n",
        "\n",
        "In **HowToGalaxy** -> `chapter 4` -> `tutorial_4_bayesian_regularization` we expand on this further and give a more\n",
        "detailed description of how these different terms impact the `log_likelihood_function`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "log_curvature_reg_matrix_term = np.linalg.slogdet(curvature_reg_matrix)[1]\n",
        "log_regularization_matrix_term = np.linalg.slogdet(regularization_matrix)[1]\n",
        "\n",
        "print(log_curvature_reg_matrix_term)\n",
        "print(log_regularization_matrix_term)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Noise Normalization Term__\n",
        "\n",
        "Our likelihood function assumes the imaging data consists of independent Gaussian noise in every image pixel.\n",
        "\n",
        "The final term ins the likelihood function is therefore a `noise_normalization` term, which consists of the sum\n",
        "of the log of every noise-map value squared. \n",
        "\n",
        "Given the `noise_map` is fixed, this term does not change during the lens modeling process and has no impact on the \n",
        "model we infer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "noise_normalization_real = np.sum(np.log(2 * np.pi * dataset.noise_map.real**2.0))\n",
        "noise_normalization_imag = np.sum(np.log(2 * np.pi * dataset.noise_map.imag**2.0))\n",
        "noise_normalization = noise_normalization_real + noise_normalization_imag"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Calculate The Log Likelihood__\n",
        "\n",
        "We can now, finally, compute the `log_likelihood` of the model, by combining the five terms computed above using\n",
        "the likelihood function defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "log_evidence = float(\n",
        "    -0.5\n",
        "    * (\n",
        "        chi_squared\n",
        "        + regularization_term\n",
        "        + log_curvature_reg_matrix_term\n",
        "        - log_regularization_matrix_term\n",
        "        + noise_normalization\n",
        "    )\n",
        ")\n",
        "\n",
        "print(log_evidence)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Fit__\n",
        "\n",
        "This process to perform a likelihood function evaluation performed via the `FitInterferometer` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tracer = al.Tracer(galaxies=[lens_galaxy, source_galaxy])\n",
        "\n",
        "fit = al.FitInterferometer(\n",
        "    dataset=dataset,\n",
        "    tracer=tracer,\n",
        "    settings_inversion=al.SettingsInversion(\n",
        "        use_w_tilde=False, use_border_relocator=True\n",
        "    ),\n",
        ")\n",
        "fit_log_evidence = fit.log_evidence\n",
        "print(fit_log_evidence)\n",
        "\n",
        "fit_plotter = aplt.FitInterferometerPlotter(fit=fit)\n",
        "fit_plotter.subplot_fit()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Lens Modeling__\n",
        "\n",
        "To fit a lens model to data, the likelihood function illustrated in this tutorial is sampled using a\n",
        "non-linear search algorithm.\n",
        "\n",
        "The default sampler is the nested sampling algorithm `nautilus` (https://github.com/joshspeagle/nautilus)\n",
        "but **PyAutoLens** supports multiple MCMC and optimization algorithms. \n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "We have presented a visual step-by-step guide to the pixelization likelihood function.\n",
        "\n",
        "There are a number of other inputs features which slightly change the behaviour of this likelihood function, which\n",
        "are described in additional notebooks found in this package. In brief, these describe:\n",
        "\n",
        " - **Over Sampling**: Oversampling the image grid into a finer grid of sub-pixels, which are all individually \n",
        " paired fractionally with each delaunay pixel.\n",
        "\n",
        " - **Source-plane Interpolation**: Using bilinear interpolation on the delaunay pixelization to pair each \n",
        " image (sub-)pixel to multiple delaunay pixels with interpolation weights.\n",
        "\n",
        " - **Luminosity Weighted Regularization**: Using an adaptive regularization coefficient which adapts the level of \n",
        " regularization applied to the source galaxy based on its luminosity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}