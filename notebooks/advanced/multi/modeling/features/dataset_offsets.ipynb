{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modeling Features: Dataset Offsets\n",
        "==================================\n",
        "\n",
        "Multi-wavelength datasets often have offsets between their images, which are due to the different telescope pointings\n",
        "during the observations.\n",
        "\n",
        "These offsets are often accounted for during the data reduction process, which aligns the images, however:\n",
        "\n",
        " - Certain data reduction pipelines may not perfectly align the images, and the scientist may be unsure what the\n",
        " true offset between the images are.\n",
        "\n",
        " - Even if the reduction process does align the images, there is still a small uncertainty in the offset due to the\n",
        "   precision of the telescope pointing which for detailed lens models must be accounted for.\n",
        "\n",
        "This script shows how to include an offset in the model, which is two free parameters, the y and x offsets, for every\n",
        "additional dataset after the first dataset. The offset therefore describes the offset of each dataset in the\n",
        "multi-wavelength dataset relative to the first dataset.\n",
        "\n",
        "To apply the offset, the code simply subtracts the offset from the grids aligned to the dataset pixels before performing\n",
        "lensing calculations. This means that the light and mass model centres do not change when the offset is applied, only\n",
        "the coordinates of the image pixels which are input into these profiles to compute the images.\n",
        "\n",
        "__Advantages__\n",
        "\n",
        "If one fits a lens model to one dataset and applies it to other datasets, it is common to see the lens model fit\n",
        "and source reconsturction degrade due to small offsets between the datasets. The same issue persists for simultaneous\n",
        "fits to multiple datasets, even when care has been taken to align the datasets.\n",
        "\n",
        "The advantage is therefore simple, for most multi-wavelength lens modeling, accounting for offsets in this way\n",
        "is the only way to ensure the lens model is accurate and the source reconstruction is reliable.\n",
        "\n",
        "__Disadvantages__\n",
        "\n",
        "Each offset introduces two additional free parameters into the model for each dataset after the first dataset. For\n",
        "4 datasets, this is 6 additional free parameters. This increases the dimensionality of the non-linear parameter space\n",
        "and therefore the computational run-time of the model-fit.\n",
        "\n",
        "__Model__\n",
        "\n",
        "This script fits an `Imaging` dataset of a 'galaxy-scale' strong lens with a model where:\n",
        "\n",
        " - The lens galaxy's light is a linear parametric linear `Sersic` bulge.\n",
        " - The lens galaxy's total mass distribution is an `Isothermal` and `ExternalShear`.\n",
        " - The source galaxy's light is a linear parametric linear `Sersic`.\n",
        "\n",
        "__Start Here Notebook__\n",
        "\n",
        "If any code in this script is unclear, refer to the `modeling/start_here.ipynb` notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import autofit as af\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Colors__\n",
        "\n",
        "The colors of the multi-wavelength image, which in this case are green (g-band) and red (r-band).\n",
        "\n",
        "The strings are used for load each dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "color_list = [\"g\", \"r\"]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Pixel Scales__\n",
        "\n",
        "Every multi-wavelength dataset can have its own unique pixel-scale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pixel_scales_list = [0.08, 0.12]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "Load and plot each multi-wavelength strong lens dataset, using a list of their waveband colors.\n",
        "\n",
        "The plotted images show that the datasets have a small offset between them, half a pixel based on the resolution of\n",
        "the second image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_type = \"multi\"\n",
        "dataset_label = \"imaging\"\n",
        "dataset_name = \"dataset_offsets\"\n",
        "\n",
        "dataset_path = path.join(\"dataset\", dataset_type, dataset_label, dataset_name)\n",
        "\n",
        "dataset_list = [\n",
        "    al.Imaging.from_fits(\n",
        "        data_path=path.join(dataset_path, f\"{color}_data.fits\"),\n",
        "        psf_path=path.join(dataset_path, f\"{color}_psf.fits\"),\n",
        "        noise_map_path=path.join(dataset_path, f\"{color}_noise_map.fits\"),\n",
        "        pixel_scales=pixel_scales,\n",
        "    )\n",
        "    for color, pixel_scales in zip(color_list, pixel_scales_list)\n",
        "]\n",
        "\n",
        "for dataset in dataset_list:\n",
        "    dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "    dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Mask__\n",
        "\n",
        "Define a 3.0\" circular mask, which includes the emission of the lens and source galaxies.\n",
        "\n",
        "For multi-wavelength lens modeling, we use the same mask for every dataset whenever possible. This is not\n",
        "absolutely necessary, but provides a more reliable analysis.\n",
        "\n",
        "The small offset between datasets means that the mask may not contain the exact same area of the image for every\n",
        "dataset. \n",
        "\n",
        "For this dataset's offset of half a pixel (and anything of order a few pixels) this is fine and wont impact the analysis. \n",
        "However, for larger offsets the mask may need to be adjusted to ensure the same image area is masked out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mask_list = [\n",
        "    al.Mask2D.circular(\n",
        "        shape_native=dataset.shape_native, pixel_scales=dataset.pixel_scales, radius=3.0\n",
        "    )\n",
        "    for dataset in dataset_list\n",
        "]\n",
        "\n",
        "dataset_list = [\n",
        "    dataset.apply_mask(mask=mask) for imaging, mask in zip(dataset_list, mask_list)\n",
        "]\n",
        "\n",
        "for dataset in dataset_list:\n",
        "    dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "    dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "We create an `Analysis` object for every dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_list = [al.AnalysisImaging(dataset=dataset) for dataset in dataset_list]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sum the analyses to create an overall analysis object, which sums the `log_likelihood_function` of each dataset\n",
        "and returns the overall likelihood of the model fit to the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = sum(analysis_list)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can parallelize the likelihood function of these analysis classes, whereby each evaluation is performed on a \n",
        "different CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis.n_cores = 1"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "We compose a lens model where:\n",
        "\n",
        " - Parameters which shift the second dataset's image (y_offset_0, x_offset_0) relative to the first dataset's image\n",
        " are included via the `DatasetModel` object [2 parameters].\n",
        "\n",
        " - The lens galaxy's light is a linear parametric `Sersic` [6 parameters].\n",
        "\n",
        " - The lens galaxy's total mass distribution is an `Isothermal` and `ExternalShear` [7 parameters].\n",
        "\n",
        " - The source galaxy's light is a linear parametric `SersicCore` [6 parameters].\n",
        "\n",
        "The number of free parameters and therefore the dimensionality of non-linear parameter space is N=23."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lens = af.Model(\n",
        "    al.Galaxy,\n",
        "    redshift=0.5,\n",
        "    bulge=al.lp_linear.Sersic,\n",
        "    mass=al.mp.Isothermal,\n",
        "    shear=al.mp.ExternalShear,\n",
        ")\n",
        "source = af.Model(al.Galaxy, redshift=1.0, bulge=al.lp_linear.SersicCore)\n",
        "\n",
        "dataset_model = af.Model(al.DatasetModel)\n",
        "\n",
        "model = af.Collection(\n",
        "    dataset_model=dataset_model, galaxies=af.Collection(lens=lens, source=source)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now make the dataset offsets vary across every analysis object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = analysis.with_free_parameters(model.dataset_model.grid_offset)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The default prior on a `DatasetModel`'s offsets are actually not a prior, but fixed values of (0.0, 0.0).\n",
        "\n",
        "We must therefore manually overwrite the prior on the offsets of datasets we wish to be included. \n",
        "\n",
        "We updated the priors on the offsets of the second dataset to be GaussianPrior's with mean 0.0\" and sigma 0.1\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis[1][model.dataset_model.grid_offset.grid_offset_0] = af.UniformPrior(\n",
        "    lower_limit=-0.1, upper_limit=0.1\n",
        ")\n",
        "analysis[1][model.dataset_model.grid_offset.grid_offset_1] = af.UniformPrior(\n",
        "    lower_limit=-0.1, upper_limit=0.1\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.Nautilus(\n",
        "    path_prefix=path.join(\"multi\", \"modeling\"),\n",
        "    name=\"dataset_offsets\",\n",
        "    unique_tag=dataset_name,\n",
        "    n_live=100,\n",
        "    number_of_cores=4,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model-Fit__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_list = search.fit(model=model, analysis=analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "The result object returned by this model-fit is a list of `Result` objects, because we used a combined analysis.\n",
        "Each result corresponds to each analysis, and therefore corresponds to the model-fit at that wavelength."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result_list[0].max_log_likelihood_instance)\n",
        "print(result_list[1].max_log_likelihood_instance)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting each result's tracer shows that the source appears different, owning to its different intensities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for result in result_list:\n",
        "    tracer_plotter = aplt.TracerPlotter(\n",
        "        tracer=result.max_log_likelihood_tracer, grid=result.grids.uniform\n",
        "    )\n",
        "    tracer_plotter.subplot_tracer()\n",
        "\n",
        "    fit_plotter = aplt.FitImagingPlotter(fit=result.max_log_likelihood_fit)\n",
        "    fit_plotter.subplot_fit()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Samples` object still has the dimensions of the overall non-linear search (in this case N=15). \n",
        "\n",
        "Therefore, the samples is identical in every result object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for result in result_list:\n",
        "    plotter = aplt.NestPlotter(samples=result.samples)\n",
        "    plotter.corner_anesthetic()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checkout `autolens_workspace/*/imaging/results` for a full description of analysing results in **PyAutoLens**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}