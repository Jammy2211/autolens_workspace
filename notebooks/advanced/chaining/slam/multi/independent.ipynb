{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SLaM: Multi Wavelength Independent\n",
        "==================================\n",
        "\n",
        "This example shows how to use the SLaM pipeline to fit a lens dataset at one wavelength, and then fit other data of\n",
        "the same lens at different wavelengths with the same mass model fitted to the original dataset.\n",
        "\n",
        "The first dataset fitted is regarded as the \"main\" dataset, meaning it should have the highest resolution and\n",
        "signal-to-noise. This will ensure the mass model is the most accurate.\n",
        "\n",
        "The remaining datasets are then fitted, which may be similar quality to the main dataset or lower resolution and\n",
        "signal-to-noise. These datasets are fitted with the following approach:\n",
        "\n",
        "- The mass model (e.g. SIE +Shear) is fixed to the result of the VIS fit.\n",
        "\n",
        "- The lens light (Multi Gaussian Expansion) has the `intensity` values of the Gaussians updated using linear algebra.\n",
        "  to capture changes in the lens light over wavelength, but it does not update the Gaussian parameters (e.g. `centre`,\n",
        " `elliptical_comps`, `sigma`) themselves due to the lower resolution of the data.\n",
        "\n",
        "- The source reconstruction (Delaunay adaptive mesh) is updated using linear algebra to reconstruct the source, but again fixes\n",
        "  the source pixelization parameters themselves.\n",
        "\n",
        "- Sub-pixel offsets between the datasets are fully modeled as free parameters, because the precision of a lens model\n",
        "can often be less than the requirements on astrometry.\n",
        "\n",
        "The restrictive nature of the lens mass, light and source models mean that much lower quality multi-wavelength data\n",
        "can be fitted provided the first dataset is of high quality. This is key for upcoming surveys such as Euclid, where\n",
        "the VIS instrument will be high resolution but many other wavebands will be lower resolution.\n",
        "\n",
        "The first fit,is identical to the `start_here.py` script, you should therefore familiarize yourself with that script\n",
        "before reading this one.\n",
        "\n",
        "The subsequent fits to the lower resolution data use a reduced and simplified SLaM pipeline with the mass model\n",
        "fixed to the result of the VIS fit.\n",
        "\n",
        "__Preqrequisites__\n",
        "\n",
        "Before reading this script, you should have familiarity with the following key concepts:\n",
        "\n",
        "- **Multi**: The `autolens_workspace/*/advanced/multi` package describes many different ways that multiple datasets\n",
        "  can be modeled in a single analysis, including the example script `one_by_one.ipynb` which fits a primary dataset\n",
        "  and then follows it up with fits to lower resolution datasets.\n",
        "\n",
        "__This Script__\n",
        "\n",
        "Using a SOURCE LP PIPELINE, SOURCE PIX PIPELINE, LIGHT LP PIPELINE and TOTAL MASS PIPELINE this SLaM modeling\n",
        "script  fits `Imaging` dataset  of a strong lens system where in the final model:\n",
        "\n",
        " - The lens galaxy's light is a bulge with Multiple Gaussian Expansion (MGE) light profile.\n",
        " - The lens galaxy's total mass distribution is an `PowerLaw` plus an `ExternalShear`.\n",
        " - The source galaxy's light is a `Pixelization`.\n",
        " - Two extra galaxies are included in the model, each with their light represented as a bulge with MGE light profile\n",
        "   and their mass as a `IsothermalSph` profile.\n",
        "\n",
        "This modeling script uses the SLaM pipelines:\n",
        "\n",
        " `source_lp`\n",
        " `source_pix`\n",
        " `light_lp`\n",
        " `mass_total`\n",
        "\n",
        "__Start Here Notebook__\n",
        "\n",
        "If any code in this script is unclear, refer to the `chaining/start_here.ipynb` notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Everything below is identical to `start_here.py` and thus not commented, as it is the same code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from os import path\n",
        "import autofit as af\n",
        "import autolens as al\n",
        "import autolens.plot as aplt\n",
        "\n",
        "sys.path.insert(0, os.getcwd())\n",
        "import slam"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__ \n",
        "\n",
        "Load, plot and mask the `Imaging` data.\n",
        "\n",
        "We load a dataset with the waveband \"g\", which is the highest resolution data in this multi-wavelength example. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"lens_sersic\"\n",
        "dataset_main_path = path.join(\"dataset\", \"multi\", \"imaging\", dataset_name)\n",
        "\n",
        "dataset_waveband = \"g\"\n",
        "\n",
        "dataset = al.Imaging.from_fits(\n",
        "    data_path=path.join(dataset_main_path, f\"{dataset_waveband}_data.fits\"),\n",
        "    noise_map_path=path.join(dataset_main_path, f\"{dataset_waveband}_noise_map.fits\"),\n",
        "    psf_path=path.join(dataset_main_path, f\"{dataset_waveband}_psf.fits\"),\n",
        "    pixel_scales=0.08,\n",
        ")\n",
        "\n",
        "mask_radius = 3.0\n",
        "\n",
        "mask = al.Mask2D.circular(\n",
        "    shape_native=dataset.shape_native,\n",
        "    pixel_scales=dataset.pixel_scales,\n",
        "    radius=mask_radius,\n",
        ")\n",
        "\n",
        "dataset = dataset.apply_mask(mask=mask)\n",
        "\n",
        "dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "dataset_plotter.subplot_dataset()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Settings AutoFit__\n",
        "\n",
        "The settings of autofit, which controls the output paths, parallelization, database use, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "settings_search = af.SettingsSearch(\n",
        "    path_prefix=path.join(\"slam\", \"multi\", \"independent\"),\n",
        "    unique_tag=f\"{dataset_name}_data_{dataset_waveband}\",\n",
        "    info=None,\n",
        "    number_of_cores=4,\n",
        "    session=None,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Redshifts__\n",
        "\n",
        "The redshifts of the lens and source galaxies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "redshift_lens = 0.5\n",
        "redshift_source = 1.0"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__SOURCE LP PIPELINE__\n",
        "\n",
        "The SOURCE LP PIPELINE is identical to the `start_here.ipynb` example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = al.AnalysisImaging(dataset=dataset)\n",
        "\n",
        "# Lens Light\n",
        "\n",
        "centre_0 = af.GaussianPrior(mean=0.0, sigma=0.1)\n",
        "centre_1 = af.GaussianPrior(mean=0.0, sigma=0.1)\n",
        "\n",
        "total_gaussians = 30\n",
        "gaussian_per_basis = 2\n",
        "\n",
        "log10_sigma_list = np.linspace(-2, np.log10(mask_radius), total_gaussians)\n",
        "\n",
        "bulge_gaussian_list = []\n",
        "\n",
        "for j in range(gaussian_per_basis):\n",
        "    gaussian_list = af.Collection(\n",
        "        af.Model(al.lp_linear.Gaussian) for _ in range(total_gaussians)\n",
        "    )\n",
        "\n",
        "    for i, gaussian in enumerate(gaussian_list):\n",
        "        gaussian.centre.centre_0 = centre_0\n",
        "        gaussian.centre.centre_1 = centre_1\n",
        "        gaussian.ell_comps = gaussian_list[0].ell_comps\n",
        "        gaussian.sigma = 10 ** log10_sigma_list[i]\n",
        "\n",
        "    bulge_gaussian_list += gaussian_list\n",
        "\n",
        "lens_bulge = af.Model(\n",
        "    al.lp_basis.Basis,\n",
        "    profile_list=bulge_gaussian_list,\n",
        ")\n",
        "\n",
        "# Source Light\n",
        "\n",
        "centre_0 = af.GaussianPrior(mean=0.0, sigma=0.3)\n",
        "centre_1 = af.GaussianPrior(mean=0.0, sigma=0.3)\n",
        "\n",
        "total_gaussians = 30\n",
        "gaussian_per_basis = 1\n",
        "\n",
        "log10_sigma_list = np.linspace(-3, np.log10(1.0), total_gaussians)\n",
        "\n",
        "bulge_gaussian_list = []\n",
        "\n",
        "for j in range(gaussian_per_basis):\n",
        "    gaussian_list = af.Collection(\n",
        "        af.Model(al.lp_linear.Gaussian) for _ in range(total_gaussians)\n",
        "    )\n",
        "\n",
        "    for i, gaussian in enumerate(gaussian_list):\n",
        "        gaussian.centre.centre_0 = centre_0\n",
        "        gaussian.centre.centre_1 = centre_1\n",
        "        gaussian.ell_comps = gaussian_list[0].ell_comps\n",
        "        gaussian.sigma = 10 ** log10_sigma_list[i]\n",
        "\n",
        "    bulge_gaussian_list += gaussian_list\n",
        "\n",
        "source_bulge = af.Model(\n",
        "    al.lp_basis.Basis,\n",
        "    profile_list=bulge_gaussian_list,\n",
        ")\n",
        "\n",
        "source_lp_result = slam.source_lp.run(\n",
        "    settings_search=settings_search,\n",
        "    analysis=analysis,\n",
        "    lens_bulge=lens_bulge,\n",
        "    lens_disk=None,\n",
        "    mass=af.Model(al.mp.Isothermal),\n",
        "    shear=af.Model(al.mp.ExternalShear),\n",
        "    source_bulge=source_bulge,\n",
        "    mass_centre=(0.0, 0.0),\n",
        "    redshift_lens=redshift_lens,\n",
        "    redshift_source=redshift_source,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__SOURCE PIX PIPELINE__\n",
        "\n",
        "The SOURCE PIX PIPELINE (and every pipeline that follows) are identical to the `start_here.ipynb` example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = al.AnalysisImaging(\n",
        "    dataset=dataset,\n",
        "    adapt_image_maker=al.AdaptImageMaker(result=source_lp_result),\n",
        "    positions_likelihood=source_lp_result.positions_likelihood_from(\n",
        "        factor=3.0, minimum_threshold=0.2\n",
        "    ),\n",
        ")\n",
        "\n",
        "source_pix_result_1 = slam.source_pix.run_1(\n",
        "    settings_search=settings_search,\n",
        "    analysis=analysis,\n",
        "    source_lp_result=source_lp_result,\n",
        "    mesh_init=al.mesh.Delaunay,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__SOURCE PIX PIPELINE 2 (with lens light)__\n",
        "\n",
        "As above, this pipeline also has the same API as the `start_here.ipynb` example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = al.AnalysisImaging(\n",
        "    dataset=dataset,\n",
        "    adapt_image_maker=al.AdaptImageMaker(result=source_pix_result_1),\n",
        "    settings_inversion=al.SettingsInversion(\n",
        "        image_mesh_min_mesh_pixels_per_pixel=3,\n",
        "        image_mesh_min_mesh_number=5,\n",
        "        image_mesh_adapt_background_percent_threshold=0.1,\n",
        "        image_mesh_adapt_background_percent_check=0.8,\n",
        "    ),\n",
        ")\n",
        "\n",
        "source_pix_result_2 = slam.source_pix.run_2(\n",
        "    settings_search=settings_search,\n",
        "    analysis=analysis,\n",
        "    source_lp_result=source_lp_result,\n",
        "    source_pix_result_1=source_pix_result_1,\n",
        "    image_mesh=al.image_mesh.Hilbert,\n",
        "    mesh=al.mesh.Delaunay,\n",
        "    regularization=al.reg.AdaptiveBrightnessSplit,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__LIGHT LP PIPELINE__ \n",
        "\n",
        "As above, this pipeline also has the same API as the `start_here.ipynb` example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = al.AnalysisImaging(\n",
        "    dataset=dataset, adapt_image_maker=al.AdaptImageMaker(result=source_pix_result_1)\n",
        ")\n",
        "\n",
        "total_gaussians = 30\n",
        "gaussian_per_basis = 2\n",
        "\n",
        "log10_sigma_list = np.linspace(-2, np.log10(mask_radius), total_gaussians)\n",
        "\n",
        "bulge_gaussian_list = []\n",
        "\n",
        "for j in range(gaussian_per_basis):\n",
        "    gaussian_list = af.Collection(\n",
        "        af.Model(al.lp_linear.Gaussian) for _ in range(total_gaussians)\n",
        "    )\n",
        "\n",
        "    for i, gaussian in enumerate(gaussian_list):\n",
        "        gaussian.centre.centre_0 = gaussian_list[0].centre.centre_0\n",
        "        gaussian.centre.centre_1 = gaussian_list[0].centre.centre_1\n",
        "        gaussian.ell_comps = gaussian_list[0].ell_comps\n",
        "        gaussian.sigma = 10 ** log10_sigma_list[i]\n",
        "\n",
        "    bulge_gaussian_list += gaussian_list\n",
        "\n",
        "lens_bulge = af.Model(\n",
        "    al.lp_basis.Basis,\n",
        "    profile_list=bulge_gaussian_list,\n",
        ")\n",
        "\n",
        "light_result = slam.light_lp.run(\n",
        "    settings_search=settings_search,\n",
        "    analysis=analysis,\n",
        "    source_result_for_lens=source_pix_result_1,\n",
        "    source_result_for_source=source_pix_result_2,\n",
        "    lens_bulge=lens_bulge,\n",
        "    lens_disk=None,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__MASS TOTAL PIPELINE__\n",
        "\n",
        "As above, this pipeline also has the same API as the `start_here.ipynb` example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis = al.AnalysisImaging(\n",
        "    dataset=dataset,\n",
        "    adapt_image_maker=al.AdaptImageMaker(result=source_pix_result_1),\n",
        "    positions_likelihood=source_pix_result_2.positions_likelihood_from(\n",
        "        factor=3.0, minimum_threshold=0.2\n",
        "    ),\n",
        ")\n",
        "\n",
        "mass_result = slam.mass_total.run(\n",
        "    settings_search=settings_search,\n",
        "    analysis=analysis,\n",
        "    source_result_for_lens=source_pix_result_1,\n",
        "    source_result_for_source=source_pix_result_2,\n",
        "    light_result=light_result,\n",
        "    mass=af.Model(al.mp.PowerLaw),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Output__\n",
        "\n",
        "The `start_hre.ipynb` example describes how results can be output to hard-disk after the SLaM pipelines have been run.\n",
        "Checkout that script for a complete description of the output of this script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# slam.slam_util.output_model_to_fits(\n",
        "#     output_path=path.join(dataset_path, \"model\"),\n",
        "#     result=mass_result,\n",
        "#     model_lens_light=True,\n",
        "#     model_source_light=True,\n",
        "#     source_reconstruction=True,\n",
        "# )\n",
        "#\n",
        "# slam.slam_util.output_model_results(\n",
        "#     output_path=path.join(dataset_path, \"model\"),\n",
        "#     result=mass_result,\n",
        "#     filename=\"model.results\",\n",
        "# )\n",
        "#\n",
        "# slam.slam_util.output_fit_multi_png(\n",
        "#     output_path=dataset_path,\n",
        "#     result_list=[mass_result],\n",
        "#     filename=\"sie_fit\",\n",
        "# )\n",
        "#\n",
        "# slam.slam_util.output_source_multi_png(\n",
        "#     output_path=dataset_path,\n",
        "#     result_list=[mass_result],\n",
        "#     filename=\"source_reconstruction\",\n",
        "# )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Second Dataset Fits__\n",
        "\n",
        "We now fit the secondary multi-wavelength datasets, which are lower resolution than the main dataset. \n",
        "\n",
        "This uses a for loop to iterate over every waveband of every dataset, load and mask the data and fit it.\n",
        "\n",
        "Each fit uses a fixed mass model, the lens and source light models update via linear algebra and offsets are\n",
        "includded (see full description above).\n",
        "\n",
        "Its the usual API to set up dataset paths, but include its \"main` path which is before the waveband folders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"lens_sersic\"\n",
        "dataset_main_path = path.join(\"dataset\", \"multi\", \"imaging\", dataset_name)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset Wavebands__\n",
        "\n",
        "The following list gives the names of the wavebands we are going to fit. \n",
        "\n",
        "The data for each waveband is loaded from a folder in the dataset folder with that name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_waveband_list = [\"r\"]\n",
        "pixel_scale_list = [0.12]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset Model__\n",
        "\n",
        "For each fit, the (y,x) offset of the secondary data from the primary data is a free parameter. \n",
        "\n",
        "This is achieved by setting up a `DatasetModel` for each waveband, which extends the model with components\n",
        "including the grid offset.\n",
        "\n",
        "This ensures that if the datasets are offset with respect to one another, the model can correct for this,\n",
        "with sub-pixel offsets often being important in lens modeling as the precision of a lens model can often be\n",
        "less than the requirements on astrometry."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_model = af.Model(al.DatasetModel)\n",
        "\n",
        "dataset_model.grid_offset.grid_offset_0 = af.UniformPrior(\n",
        "    lower_limit=-0.2, upper_limit=0.2\n",
        ")\n",
        "dataset_model.grid_offset.grid_offset_1 = af.UniformPrior(\n",
        "    lower_limit=-0.2, upper_limit=0.2\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result Dict__\n",
        "\n",
        "Visualization at the end of the pipeline will output all fits to all wavebands on a single matplotlib subplot.\n",
        "\n",
        "The results of each fit are stored in a dictionary, which is used to pass the results of each fit to the\n",
        "visualization functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "multi_result_dict = {\"g\": mass_result}\n",
        "\n",
        "for dataset_waveband, pixel_scale in zip(dataset_waveband_list, pixel_scale_list):\n",
        "    dataset_path = dataset_main_path\n",
        "\n",
        "    dataset = al.Imaging.from_fits(\n",
        "        data_path=path.join(dataset_main_path, f\"{dataset_waveband}_data.fits\"),\n",
        "        noise_map_path=path.join(\n",
        "            dataset_main_path, f\"{dataset_waveband}_noise_map.fits\"\n",
        "        ),\n",
        "        psf_path=path.join(dataset_main_path, f\"{dataset_waveband}_psf.fits\"),\n",
        "        pixel_scales=pixel_scale,\n",
        "    )\n",
        "\n",
        "    mask = al.Mask2D.circular(\n",
        "        shape_native=dataset.shape_native,\n",
        "        pixel_scales=dataset.pixel_scales,\n",
        "        radius=mask_radius,\n",
        "    )\n",
        "\n",
        "    dataset = dataset.apply_mask(mask=mask)\n",
        "\n",
        "    dataset = dataset.apply_over_sampling(\n",
        "        over_sampling=al.OverSamplingDataset(\n",
        "            uniform=al.OverSamplingUniform.from_radial_bins(\n",
        "                grid=dataset.grid,\n",
        "                sub_size_list=[4, 2, 1],\n",
        "                radial_list=[0.1, 0.3],\n",
        "                centre_list=[(0.0, 0.0)],\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    dataset_plotter = aplt.ImagingPlotter(dataset=dataset)\n",
        "    dataset_plotter.subplot_dataset()\n",
        "\n",
        "    \"\"\"\n",
        "    __Settings AutoFit__\n",
        "    \"\"\"\n",
        "    settings_search = af.SettingsSearch(\n",
        "        path_prefix=path.join(\"slam\", \"multi\", \"independent\"),\n",
        "        unique_tag=f\"{dataset_name}_data_{dataset_waveband}\",\n",
        "        info=None,\n",
        "    )\n",
        "\n",
        "    \"\"\"\n",
        "    __SOURCE LP PIPELINE (with lens light)__\n",
        "\n",
        "    The SOURCE LP PIPELINE (with lens light) uses three searches to initialize a robust model for the \n",
        "    source galaxy's light, which in this example:\n",
        "\n",
        "     - Uses a parametric `Sersic` bulge and `Exponential` disk with centres aligned for the lens\n",
        "     galaxy's light.\n",
        "\n",
        "     - Uses an `Isothermal` model for the lens's total mass distribution with an `ExternalShear`.\n",
        "\n",
        "     __Settings__:\n",
        "\n",
        "     - Mass Centre: Fix the mass profile centre to (0.0, 0.0) (this assumption will be relaxed in the MASS TOTAL PIPELINE).\n",
        "    \"\"\"\n",
        "    analysis = al.AnalysisImaging(\n",
        "        dataset=dataset,\n",
        "    )\n",
        "\n",
        "    centre_0 = af.GaussianPrior(mean=0.0, sigma=0.3)\n",
        "    centre_1 = af.GaussianPrior(mean=0.0, sigma=0.3)\n",
        "\n",
        "    total_gaussians = 20\n",
        "    gaussian_per_basis = 1\n",
        "\n",
        "    log10_sigma_list = np.linspace(-3, np.log10(1.0), total_gaussians)\n",
        "\n",
        "    bulge_gaussian_list = []\n",
        "\n",
        "    for j in range(gaussian_per_basis):\n",
        "        gaussian_list = af.Collection(\n",
        "            af.Model(al.lp_linear.Gaussian) for _ in range(total_gaussians)\n",
        "        )\n",
        "\n",
        "        for i, gaussian in enumerate(gaussian_list):\n",
        "            gaussian.centre.centre_0 = centre_0\n",
        "            gaussian.centre.centre_1 = centre_1\n",
        "            gaussian.ell_comps = gaussian_list[0].ell_comps\n",
        "            gaussian.sigma = 10 ** log10_sigma_list[i]\n",
        "\n",
        "        bulge_gaussian_list += gaussian_list\n",
        "\n",
        "    source_bulge = af.Model(\n",
        "        al.lp_basis.Basis,\n",
        "        profile_list=bulge_gaussian_list,\n",
        "    )\n",
        "\n",
        "    source_lp_result = slam.source_lp.run(\n",
        "        settings_search=settings_search,\n",
        "        analysis=analysis,\n",
        "        lens_bulge=light_result.instance.galaxies.lens.bulge,\n",
        "        lens_disk=None,\n",
        "        lens_point=light_result.instance.galaxies.lens.point,\n",
        "        mass=mass_result.instance.galaxies.lens.mass,\n",
        "        shear=mass_result.instance.galaxies.lens.shear,\n",
        "        source_bulge=source_bulge,\n",
        "        redshift_lens=0.5,\n",
        "        redshift_source=1.0,\n",
        "        dataset_model=dataset_model,\n",
        "    )\n",
        "\n",
        "    \"\"\"\n",
        "    __SOURCE PIX PIPELINE (with lens light)__\n",
        "\n",
        "    The SOURCE PIX PIPELINE (with lens light) uses four searches to initialize a robust model for the `Inversion` \n",
        "    that reconstructs the source galaxy's light. It begins by fitting a `VoronoiMagnification` pixelization with `Constant` \n",
        "    regularization, to set up the model and hyper images, and then:\n",
        "\n",
        "     - Uses a `VoronoiBrightnessImage` pixelization.\n",
        "     - Uses an `AdaptiveBrightness` regularization.\n",
        "     - Carries the lens redshift, source redshift and `ExternalShear` of the SOURCE LP PIPELINE through to the\n",
        "     SOURCE PIX PIPELINE.\n",
        "    \"\"\"\n",
        "    analysis = al.AnalysisImaging(\n",
        "        dataset=dataset,\n",
        "        adapt_image_maker=al.AdaptImageMaker(result=source_lp_result),\n",
        "        raise_inversion_positions_likelihood_exception=False,\n",
        "    )\n",
        "\n",
        "    source_pix_result_1 = slam.source_pix.run_1__mass_fixed(\n",
        "        settings_search=settings_search,\n",
        "        analysis=analysis,\n",
        "        source_lp_result=source_lp_result,\n",
        "        mesh_init=al.mesh.Delaunay,\n",
        "        dataset_model=dataset_model,\n",
        "    )\n",
        "\n",
        "    source_pix_result_1.max_log_likelihood_fit.inversion.cls_list_from(\n",
        "        cls=al.AbstractMapper\n",
        "    )[0].extent_from()\n",
        "\n",
        "    analysis = al.AnalysisImaging(\n",
        "        dataset=dataset,\n",
        "        adapt_image_maker=al.AdaptImageMaker(result=source_pix_result_1),\n",
        "        settings_inversion=al.SettingsInversion(\n",
        "            image_mesh_min_mesh_pixels_per_pixel=3,\n",
        "            image_mesh_min_mesh_number=5,\n",
        "            image_mesh_adapt_background_percent_threshold=0.1,\n",
        "            image_mesh_adapt_background_percent_check=0.8,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    multi_result = slam.source_pix.run_2(\n",
        "        settings_search=settings_search,\n",
        "        analysis=analysis,\n",
        "        source_lp_result=source_lp_result,\n",
        "        source_pix_result_1=source_pix_result_1,\n",
        "        image_mesh=al.image_mesh.Hilbert,\n",
        "        mesh=al.mesh.Delaunay,\n",
        "        regularization=al.reg.AdaptiveBrightnessSplit,\n",
        "        dataset_model=dataset_model,\n",
        "    )\n",
        "\n",
        "    multi_result_dict[dataset_waveband] = multi_result\n",
        "\n",
        "    slam.slam_util.output_model_to_fits(\n",
        "        output_path=path.join(dataset_path, \"model\"),\n",
        "        result=multi_result,\n",
        "        model_lens_light=True,\n",
        "        model_source_light=True,\n",
        "        source_reconstruction=True,\n",
        "    )\n",
        "\n",
        "    slam.slam_util.output_model_results(\n",
        "        output_path=path.join(dataset_path, \"model\"),\n",
        "        result=multi_result,\n",
        "        filename=\"sie_model.results\",\n",
        "    )\n",
        "\n",
        "tag_list = list(multi_result_dict.keys())\n",
        "\n",
        "slam.slam_util.output_fit_multi_png(\n",
        "    output_path=path.join(dataset_main_path),\n",
        "    result_list=[multi_result_dict[dataset_waveband] for dataset_waveband in tag_list],\n",
        "    tag_list=tag_list,\n",
        "    filename=\"8_sie_fit\",\n",
        ")\n",
        "\n",
        "slam.slam_util.output_source_multi_png(\n",
        "    output_path=path.join(dataset_main_path),\n",
        "    result_list=[multi_result_dict[dataset_waveband] for dataset_waveband in tag_list],\n",
        "    tag_list=tag_list,\n",
        "    filename=\"9_source_reconstruction\",\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}