{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# \"\"\"\n",
        "# Feature: Sensitivity Mapping\n",
        "# ============================\n",
        "#\n",
        "# Bayesian model comparison allows us to take a dataset, fit it with multiple models and use the Bayesian evidence to\n",
        "# quantify which model objectively gives the best-fit following the principles of Occam's Razor.\n",
        "#\n",
        "# However, a complex model may not be favoured by model comparison not because it is the 'wrong' model, but simply\n",
        "# because the dataset being fitted is not of a sufficient quality for the more complex model to be favoured. Sensitivity\n",
        "# mapping addresses what quality of data would be needed for the more complex model to be favoured.\n",
        "#\n",
        "# In order to do this, sensitivity mapping involves us writing a function that uses the model(s) to simulate a dataset.\n",
        "# We then use this function to simulate many datasets, for many different models, and fit each dataset using the same\n",
        "# model-fitting procedure we used to perform Bayesian model comparison. This allows us to infer how much of a Bayesian\n",
        "# evidence increase we should expect for datasets of varying quality and / or models with different parameters.\n",
        "#\n",
        "# For strong lensing, this process is crucial for dark matter substructure detection, as discussed in the following paper:\n",
        "#\n",
        "# https://arxiv.org/abs/0903.4752\n",
        "#\n",
        "# In substructure detection, we scan a strong lens dark matter subhalos by fitting a lens models which include a subhalo.\n",
        "# This tells us whether we successfully did detect a subhalo, but it does not tell us where a subhalo has to be located\n",
        "# (in relation to the source light) to be detectable, nor does to what masses of subhalo we could actually have made a\n",
        "# detection.\n",
        "#\n",
        "# To answer these questions, we must perform sensitivity mapping, where we simulate many thousands of datasets,\n",
        "# each of which include a dark matter subhalo at a given (y,x) coordinate at a given mass. We then fit each dataset twice,\n",
        "# once with a lens model which does not include a subhalo and once with a lens model that does. If the Bayesian evidence\n",
        "# of the model which includes a subhalo is higher than that which does not, then it means a subhalo was detectable!\n",
        "# \"\"\"\n",
        "# %matplotlib inline\n",
        "# from pyprojroot import here\n",
        "# workspace_path = str(here())\n",
        "# %cd $workspace_path\n",
        "# print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "#\n",
        "# import numpy as np\n",
        "# from os import path\n",
        "# import autofit as af\n",
        "# import autolens as al\n",
        "# import autolens.plot as aplt\n",
        "#\n",
        "# \"\"\"\n",
        "# __Dataset + Masking__\n",
        "#\n",
        "# Load the `Interferometer` data, define the visibility and real-space masks and plot them.\n",
        "# \"\"\"\n",
        "# real_space_mask = al.Mask2D.circular(\n",
        "#     shape_native=(151, 151), pixel_scales=0.05, radius=3.0\n",
        "# )\n",
        "#\n",
        "# dataset_name = \"simple\"\n",
        "# dataset_path = path.join(\"dataset\", \"interferometer\", dataset_name)\n",
        "#\n",
        "# dataset = al.Interferometer.from_fits(\n",
        "#     data_path=path.join(dataset_path, \"data.fits\"),\n",
        "#     noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "#     uv_wavelengths_path=path.join(dataset_path, \"uv_wavelengths.fits\"),\n",
        "#     real_space_mask=real_space_mask,\n",
        "# )\n",
        "# dataset = dataset.apply_settings(\n",
        "#     settings=al.SettingsInterferometer(transformer_class=al.TransformerDFT)\n",
        "# )\n",
        "#\n",
        "# dataset_plotter = aplt.InterferometerPlotter(dataset=dataset)\n",
        "# dataset_plotter.subplot_dataset()\n",
        "#\n",
        "#\n",
        "# \"\"\"\n",
        "# We are performing sensitivity mapping to determine where a subhalo is detectable. This will require us to simulate\n",
        "# many realizations of our dataset with a lens model, called the `simulation_instance`. To get this model, we therefore\n",
        "# fit the data before performing sensitivity mapping so that we can set the `simulation_instance` as the maximum\n",
        "# likelihood model.\n",
        "#\n",
        "# We perform this fit using the lens model we will use to perform sensitivity mapping, which we call the `base_model`.\n",
        "# \"\"\"\n",
        "# base_model = af.Collection(\n",
        "#     galaxies=af.Collection(\n",
        "#         lens=af.Model(al.Galaxy, redshift=0.5, mass=al.mp.Isothermal),\n",
        "#         source=af.Model(al.Galaxy, redshift=1.0, bulge=al.lp_linear.SersicCore),\n",
        "#     )\n",
        "# )\n",
        "#\n",
        "# search_base = af.Nautilus(\n",
        "#     path_prefix=path.join(\"interferometer\", \"misc\"),\n",
        "#     name=\"sensitivity_mapping_base\",\n",
        "#     unique_tag=dataset_name,\n",
        "#     n_live=100,\n",
        "# )\n",
        "#\n",
        "# analysis = al.AnalysisInterferometer(dataset=dataset)\n",
        "#\n",
        "# result = search_base.fit(model=base_model, analysis=analysis)\n",
        "#\n",
        "# \"\"\"\n",
        "# We now define the `base_model` that we use to perform sensitivity mapping. This is the lens model that is fitted to\n",
        "# every simulated strong lens without a subhalo, giving us the Bayesian evidence which we compare to the model which\n",
        "# includes one!).\n",
        "#\n",
        "# For this model, we can use the `base_model` above, however we will use the result of fitting this model to the dataset\n",
        "# before sensitivity mapping. This ensures the priors associated with each parameter are initialized so as to speed up\n",
        "# each non-linear search performed during sensitivity mapping.\n",
        "# \"\"\"\n",
        "# base_model = result.model\n",
        "#\n",
        "# \"\"\"\n",
        "# We now define the `perturb_model`, which is the model component whose parameters we iterate over to perform\n",
        "# sensitivity mapping. In this case, this model is a `NFWMCRLudlowSph` model and we will iterate over its\n",
        "# `centre` and `mass_at_200`. We set it up as a `Model` so it has an associated redshift and can be directly\n",
        "# passed to the tracer in the simulate function below.\n",
        "#\n",
        "# Many instances of the `perturb_model` are created and used to simulate the many strong lens datasets that we fit.\n",
        "# However, it is only included in half of the model-fits; corresponding to the lens models which include a dark matter\n",
        "# subhalo and whose Bayesian evidence we compare to the simpler model-fits consisting of just the `base_model` to\n",
        "# determine if the subhalo was detectable.\n",
        "#\n",
        "# By fitting both models to every simulated lens, we therefore infer the Bayesian evidence of every model to every\n",
        "# dataset. Sensitivity mapping therefore maps out for what values of `centre` and `mass_at_200` in the dark matter\n",
        "# subhalo the model-fit including a subhalo provide higher values of Bayesian evidence than the simpler model-fit (and\n",
        "# therefore when it is detectable!).\n",
        "# \"\"\"\n",
        "# perturb_model = af.Model(al.Galaxy, redshift=0.5, mass=al.mp.NFWMCRLudlowSph)\n",
        "#\n",
        "# \"\"\"\n",
        "# Sensitivity mapping is typically performed over a large range of parameters. However, to make this demonstration quick\n",
        "# and clear we are going to fix the `centre` of the subhalo to a value near the Einstein ring of (1.6, 0.0). We will\n",
        "# iterate over just two `mass_at_200` values corresponding to subhalos of mass 1e6 and 1e13, of which only the latter\n",
        "# will be shown to be detectable.\n",
        "# \"\"\"\n",
        "# perturb_model.mass.centre.centre_0 = 1.6\n",
        "# perturb_model.mass.centre.centre_1 = 0.0\n",
        "# perturb_model.mass.redshift_object = 0.5\n",
        "# perturb_model.mass.redshift_source = 1.0\n",
        "# perturb_model.mass.mass_at_200 = af.LogUniformPrior(lower_limit=1e6, upper_limit=1e13)\n",
        "#\n",
        "# \"\"\"\n",
        "# We are performing sensitivity mapping to determine where a subhalo is detectable. This will require us to\n",
        "# simulate many realizations of our dataset with a lens model, called the `simulation_instance`. This model uses the\n",
        "# result of the fit above.\n",
        "# \"\"\"\n",
        "# simulation_instance = result.instance\n",
        "#\n",
        "# \"\"\"\n",
        "# We now write the `simulate_cls`, which takes the `simulation_instance` of our model (defined above) and uses it to\n",
        "# simulate a dataset which is subsequently fitted.\n",
        "#\n",
        "# Note that when this dataset is simulated, the quantity `instance.perturb` is used in the `simulate_cls`.\n",
        "# This is an instance of the `NFWMCRLudlowSph`, and it is different every time the `simulate_cls` is called\n",
        "# based on the value of sensitivity being computed.\n",
        "#\n",
        "# In this example, this `instance.perturb` corresponds to two different subhalos with values of `mass_at_200` of\n",
        "# 1e6 MSun and 1e13 MSun.\n",
        "# \"\"\"\n",
        "#\n",
        "#\n",
        "# def __call__(instance, simulate_path):\n",
        "#     \"\"\"\n",
        "#     Set up the `Tracer` which is used to simulate the strong lens interferometer, which may include the subhalo in\n",
        "#     addition to the lens and source galaxy.\n",
        "#     \"\"\"\n",
        "#     tracer = al.Tracer(\n",
        "#         galaxies=[\n",
        "#             instance.galaxies.lens,\n",
        "#             instance.perturb,\n",
        "#             instance.galaxies.source,\n",
        "#         ]\n",
        "#     )\n",
        "#\n",
        "#     \"\"\"\n",
        "#     Set up the grid, uv_wavelengths and simulator settings used to simulate interferometer dataset of the strong lens.\n",
        "#     These should be tuned to match the S/N and noise properties of the observed data you are performing sensitivity\n",
        "#     mapping on.\n",
        "#     \"\"\"\n",
        "#     grid = al.Grid2D.uniform(\n",
        "#         shape_native=real_space_mask.shape_native,\n",
        "#         pixel_scales=real_space_mask.pixel_scales,\n",
        "# over_sampling = al.OverSamplingIterate(\n",
        "#    fractional_accuracy=0.9999,\n",
        "#    sub_steps=[2, 4, 8, 16]\n",
        "# )\n",
        "#     )\n",
        "#\n",
        "#     simulator = al.SimulatorInterferometer(\n",
        "#         uv_wavelengths=dataset.uv_wavelengths,\n",
        "#         exposure_time=300.0,\n",
        "#         noise_sigma=0.1,\n",
        "#         transformer_class=al.TransformerNUFFT,\n",
        "#     )\n",
        "#\n",
        "#     simulated_dataset = simulator.via_tracer_from(tracer=tracer, grid=grid)\n",
        "#\n",
        "#     \"\"\"\n",
        "#     The data generated by the simulate function is that which is fitted, so we should apply the mask for the analysis\n",
        "#     here before we return the simulated data.\n",
        "#     \"\"\"\n",
        "#     return al.Interferometer(\n",
        "#         data=simulated_dataset.visibilities,\n",
        "#         noise_map=simulated_dataset.noise_map,\n",
        "#         uv_wavelengths=simulated_dataset.uv_wavelengths,\n",
        "#         real_space_mask=real_space_mask,\n",
        "#     )\n",
        "#\n",
        "#\n",
        "# \"\"\"\n",
        "# Each model-fit performed by sensitivity mapping creates a new instance of an `Analysis` class, which contains the\n",
        "# data simulated by the `simulate_cls` for that model.\n",
        "#\n",
        "# This requires us to write a wrapper around the PyAutoLens `Analysis` class.\n",
        "# \"\"\"\n",
        "#\n",
        "#\n",
        "# class AnalysisInterferometerSensitivity(al.AnalysisInterferometer):\n",
        "#     def __init__(self, dataset):\n",
        "#         super().__init__(dataset=dataset)\n",
        "#\n",
        "#\n",
        "# \"\"\"\n",
        "# We next specify the search used to perform each model fit by the sensitivity mapper.\n",
        "# \"\"\"\n",
        "# search = af.Nautilus(\n",
        "#     path_prefix=path.join(\"interferometer\", \"misc\"),\n",
        "#     name=\"sensitivity_mapping\",\n",
        "#     unique_tag=dataset_name,\n",
        "#     n_live=100,\n",
        "#     force_x1_cpu=True,  # ensures parallelizing over grid search works.\n",
        "# )\n",
        "#\n",
        "# \"\"\"\n",
        "# We can now combine all of the objects created above and perform sensitivity mapping. The inputs to the `Sensitivity`\n",
        "# object below are:\n",
        "#\n",
        "# - `simulation_instance`: This is an instance of the model used to simulate every dataset that is fitted. In this example\n",
        "# it is a lens model that does not include a subhalo, which was inferred by fitting the dataset we perform sensitivity\n",
        "# mapping on.\n",
        "#\n",
        "# - `base_model`: This is the lens model that is fitted to every simulated dataset, which does not include a subhalo. In\n",
        "# this example is composed of an `Isothermal` lens and `Sersic` source.\n",
        "#\n",
        "# - `perturb_model`: This is the extra model component that alongside the `base_model` is fitted to every simulated\n",
        "# dataset. In this example it is a `NFWMCRLudlowSph` dark matter subhalo.\n",
        "#\n",
        "# - `simulate_cls`: This is the function that uses the `simulation_instance` and many instances of the `perturb_model`\n",
        "# to simulate many datasets that are fitted with the `base_model` and `base_model` + `perturb_model`.\n",
        "#\n",
        "# - `analysis_class`: The wrapper `Analysis` class that passes each simulated dataset to the `Analysis` class that fits\n",
        "# the data.\n",
        "#\n",
        "# - `number_of_steps`: The number of steps over which the parameters in the `perturb_model` are iterated. In this\n",
        "# example, `mass_at_200` has a `LogUniformPrior` with lower limit 1e6 and upper limit 1e13, therefore\n",
        "# the `number_of_steps` of 2 will simulate and fit just 2 datasets where the `mass_at_200` is between 1e6 and 1e13.\n",
        "#\n",
        "# - `number_of_cores`: The number of cores over which the sensitivity mapping is performed, enabling parallel processing\n",
        "# if set above 1.\n",
        "# \"\"\"\n",
        "# from autofit.non_linear.grid import sensitivity as s\n",
        "#\n",
        "# sensitivity = s.Sensitivity(\n",
        "#     search=search,\n",
        "#     simulation_instance=simulation_instance,\n",
        "#     base_model=base_model,\n",
        "#     perturb_model=perturb_model,\n",
        "#     simulate_cls=simulate_cls,\n",
        "#     analysis_class=AnalysisInterferometerSensitivity,\n",
        "#     number_of_cores=2,\n",
        "#     number_of_steps=2,\n",
        "# )\n",
        "#\n",
        "# sensitivity_result = sensitivity.run()\n",
        "#\n",
        "# \"\"\"\n",
        "# You should now look at the results of the sensitivity mapping in the folder `output/features/sensitivity_mapping`.\n",
        "#\n",
        "# You will note the following 4 model-fits have been performed:\n",
        "#\n",
        "#  - The `base_model` is fitted to a simulated dataset where a subhalo with `mass_at_200=1e6` is included.\n",
        "#\n",
        "#  - The `base_model` + `perturb_model` is fitted to a simulated dataset where a subhalo with `mass_at_200=1e6`\n",
        "#  is included.\n",
        "#\n",
        "#  - The `base_model` is fitted to a simulated dataset where a subhalo with `mass_at_200=1e13` is included.\n",
        "#\n",
        "#  - The `base_model` + `perturb_model` is fitted to a simulated dataset where a subhalo with `mass_at_200=1e13` is\n",
        "#  included.\n",
        "#\n",
        "# The fit produces a `sensitivity_result`.\n",
        "#\n",
        "# We are still developing the `SensitivityResult` class to provide a data structure that better streamlines the analysis\n",
        "# of results. If you intend to use sensitivity mapping, the best way to interpret the resutls is currently via\n",
        "# **PyAutoFit**'s database and `Aggregator` tools.\n",
        "# \"\"\"\n",
        "# print(sensitivity_result.results[0].result.samples.log_evidence)\n",
        "# print(sensitivity_result.results[1].result.samples.log_evidence)\n",
        "#\n",
        "# \"\"\"\n",
        "# Finish.\n",
        "# \"\"\"\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}