{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 4: Hierarchical Models\n",
        "===============================\n",
        "\n",
        "In the previous tutorials, we fitted a graphical model with the aim of determining an estimate of a shared\n",
        "parameter, the `slope` of three lens mass models. We did this by fitting all datasets simultaneously.\n",
        "When there are shared parameters we wish to estimate, this is a powerful and effective tool, but for many graphical\n",
        "models things are not so straight forward.\n",
        "\n",
        "A common extension to this problem is one where we expect that the shared parameter(s) of the model do not have exactly\n",
        "the same value in every dataset. Instead, our expectation is that the parameter(s) are drawn from a common\n",
        "parent distribution (e.g. a Gaussian distribution). It is the parameters of this parent distribution that we\n",
        "consider shared across the dataset, and these are the parameters we ultimately wish to infer to understand the global\n",
        "behaviour of our model.\n",
        "\n",
        "This is called a hierarchical model, and we will fit such a model In this tutorial. We will again fit a dataset\n",
        "comprising 3 strong lenses. However, the `slope` of each model is no longer the same in each dataset -- they are\n",
        "instead drawn from a shared parent Gaussian distribution with `mean=2.0` and `sigma=0.1`. Using a hierarchical model\n",
        "we will recover these input values of the parent distribution's `mean` and `sigma`, by fitting the dataset of all 3\n",
        "lenses simultaneously.\n",
        "\n",
        "__Sample Simulation__\n",
        "\n",
        "The dataset fitted in this example script is simulated imaging data of a sample of 3 galaxies.\n",
        "\n",
        "This data is not automatically provided with the autogalaxy workspace, and must be first simulated by running the\n",
        "script `autolens_workspace/scripts/simulators/imaging/samples/advanced/mass_power_law.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import autolens as al\n",
        "import autofit as af\n",
        "from os import path"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "For each lens dataset in our sample we set up the correct path and load it by iterating over a for loop. \n",
        "\n",
        "We are loading a different dataset to the previous tutorials, where the lenses only have a single bulge component\n",
        "which each have different Sersic indexes which are drawn from a parent Gaussian distribution with a mean value \n",
        "of 2.0 and sigma of 0.5.\n",
        "\n",
        "This data is not automatically provided with the autogalaxy workspace, and must be first simulated by running the \n",
        "script `autolens_workspace/scripts/simulators/imaging/samples/advanced/mass_power_law.py`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_label = \"samples\"\n",
        "dataset_type = \"imaging\"\n",
        "dataset_sample_name = \"mass_power_law\"\n",
        "\n",
        "dataset_path = path.join(\"dataset\", dataset_type, dataset_label, dataset_sample_name)\n",
        "\n",
        "total_datasets = 3\n",
        "\n",
        "dataset_list = []\n",
        "\n",
        "for dataset_index in range(total_datasets):\n",
        "    dataset_sample_path = path.join(dataset_path, f\"dataset_{dataset_index}\")\n",
        "\n",
        "    dataset_list.append(\n",
        "        al.Imaging.from_fits(\n",
        "            data_path=path.join(dataset_sample_path, \"data.fits\"),\n",
        "            psf_path=path.join(dataset_sample_path, \"psf.fits\"),\n",
        "            noise_map_path=path.join(dataset_sample_path, \"noise_map.fits\"),\n",
        "            pixel_scales=0.1,\n",
        "        )\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Mask__\n",
        "\n",
        "We now mask each lens in our dataset, using the imaging list we created above.\n",
        "\n",
        "We will assume a 3.0\" mask for every lens in the dataset is appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "masked_imaging_list = []\n",
        "\n",
        "for dataset in dataset_list:\n",
        "    mask = al.Mask2D.circular(\n",
        "        shape_native=dataset.shape_native, pixel_scales=dataset.pixel_scales, radius=3.0\n",
        "    )\n",
        "\n",
        "    masked_imaging_list.append(dataset.apply_mask(mask=mask))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Paths__\n",
        "\n",
        "The path the results of all model-fits are output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "path_prefix = path.join(\"imaging\", \"hierarchical\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__\n",
        "\n",
        "For each dataset we now create a corresponding `AnalysisImaging` class, as we are used to doing for `Imaging` data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_list = []\n",
        "\n",
        "for masked_dataset in masked_imaging_list:\n",
        "    analysis = al.AnalysisImaging(dataset=masked_dataset)\n",
        "\n",
        "    analysis_list.append(analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Individual Factors__\n",
        "\n",
        "We first set up a model for each lens, with an `PowerLawSph` mass and `ExponentialSph` bulge, which we will use to \n",
        "fit the hierarchical model.\n",
        "\n",
        "This uses a nearly identical for loop to the previous tutorials, however a shared `slope` is no longer used and \n",
        "each mass model is given its own prior for the `slope`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_list = []\n",
        "\n",
        "for dataset_index in range(total_datasets):\n",
        "    lens = af.Model(al.Galaxy, redshift=0.5, mass=al.mp.PowerLawSph)\n",
        "    lens.mass.centre = (0.0, 0.0)\n",
        "\n",
        "    source = af.Model(al.Galaxy, redshift=1.0, bulge=al.lp_linear.ExponentialCoreSph)\n",
        "\n",
        "    model = af.Collection(galaxies=af.Collection(lens=lens, source=source))\n",
        "\n",
        "    model_list.append(model)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis Factors__\n",
        "\n",
        "Now we have our `Analysis` classes and model components, we can compose our `AnalysisFactor`'s.\n",
        "\n",
        "These are composed in the same way as for the graphical model in the previous tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_factor_list = []\n",
        "\n",
        "for model, analysis in zip(model_list, analysis_list):\n",
        "    analysis_factor = af.AnalysisFactor(prior_model=model, analysis=analysis)\n",
        "\n",
        "    analysis_factor_list.append(analysis_factor)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "We now compose the hierarchical model that we fit, using the individual model components created above.\n",
        "\n",
        "We first create a `HierarchicalFactor`, which represents the parent Gaussian distribution from which we will assume \n",
        "that the `slope` of each individual lens mass model is drawn. \n",
        "\n",
        "For this parent `Gaussian`, we have to place priors on its `mean` and `sigma`, given that they are parameters in our\n",
        "model we are ultimately fitting for."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "hierarchical_factor = af.HierarchicalFactor(\n",
        "    af.GaussianPrior,\n",
        "    mean=af.GaussianPrior(mean=2.0, sigma=1.0, lower_limit=0.0, upper_limit=100.0),\n",
        "    sigma=af.GaussianPrior(mean=0.5, sigma=0.5, lower_limit=0.0, upper_limit=100.0),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now add each of the individual mass `slope` parameters to the `hierarchical_factor`.\n",
        "\n",
        "This composes the hierarchical model whereby the individual `slope` of every light model in our dataset is now \n",
        "assumed to be drawn from a shared parent distribution. It is the `mean` and `sigma` of this distribution we are hoping \n",
        "to estimate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "for model in model_list:\n",
        "    hierarchical_factor.add_drawn_variable(model.galaxies.lens.mass.slope)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Factor Graph__\n",
        "\n",
        "We now create the factor graph for this model, using the list of `AnalysisFactor`'s and the hierarchical factor.\n",
        "\n",
        "Note that in previous tutorials, when we created the `FactorGraphModel` we only passed the list of `AnalysisFactor`'s,\n",
        "which contained the necessary information on the model create the factor graph that was fitted. The `AnalysisFactor`'s\n",
        "were created before we composed the `HierachicalFactor` and we pass it separately when composing the factor graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "factor_graph = af.FactorGraphModel(*analysis_factor_list, hierarchical_factor)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The factor graph model `info` attribute shows that the hierarchical factor's parameters are included in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(factor_graph.global_prior_model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "We can now create a non-linear search and used it to the fit the factor graph, using its `global_prior_model` property."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Nautilus = af.Nautilus(\n",
        "    path_prefix=path.join(\"imaging\", \"hierarchical\"),\n",
        "    name=\"tutorial_4_hierarchical_models\",\n",
        "    n_live=150,\n",
        ")\n",
        "\n",
        "result = Nautilus.fit(model=factor_graph.global_prior_model, analysis=factor_graph)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Result__\n",
        "\n",
        "The result's `info` attribute shows the result, including the hierarchical factor's parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(result.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now inspect the inferred value of hierarchical factor's mean and sigma.\n",
        "\n",
        "We see that they are consistent with the input values of `mean=2.0` and `sigma=0.2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples = result.samples\n",
        "\n",
        "mean = samples.median_pdf(as_instance=False)[-2]\n",
        "\n",
        "u1_error = samples.values_at_upper_sigma(sigma=1.0)[-2]\n",
        "l1_error = samples.values_at_lower_sigma(sigma=1.0)[-2]\n",
        "\n",
        "u3_error = samples.values_at_upper_sigma(sigma=3.0)[-2]\n",
        "l3_error = samples.values_at_lower_sigma(sigma=3.0)[-2]\n",
        "\n",
        "print(\n",
        "    \"Inferred value of the mean of the parent hierarchical distribution for the mass model slopes: \\n\"\n",
        ")\n",
        "print(f\"{mean} ({l1_error} {u1_error}) [1.0 sigma confidence intervals]\")\n",
        "print(f\"{mean} ({l3_error} {u3_error}) [3.0 sigma confidence intervals]\")\n",
        "\n",
        "scatter = samples.median_pdf(as_instance=False)[-2]\n",
        "\n",
        "u1_error = samples.values_at_upper_sigma(sigma=1.0)[-1]\n",
        "l1_error = samples.values_at_lower_sigma(sigma=1.0)[-1]\n",
        "\n",
        "u3_error = samples.values_at_upper_sigma(sigma=3.0)[-1]\n",
        "l3_error = samples.values_at_lower_sigma(sigma=3.0)[-1]\n",
        "\n",
        "print(\n",
        "    \"Inferred value of the scatter (the sigma value of the Gassuain) of the parent hierarchical distribution for the mass model slopes: \\n\"\n",
        ")\n",
        "print(f\"{scatter} ({l1_error} {u1_error}) [1.0 sigma confidence intervals]\")\n",
        "print(f\"{scatter} ({l3_error} {u3_error}) [3.0 sigma confidence intervals]\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Benefits of Graphical Model__\n",
        "\n",
        "In the optional tutorial `tutorial_optional_hierarchical_individual` we compare the results inferred in this script\n",
        "via a graphical model to a simpler approach which fits each dataset one-by-one and infers the hierarchical parent\n",
        "distribution's parameters afterwards.\n",
        "\n",
        "The graphical model provides a more accurate and precise estimate of the parent distribution's parameters. This is \n",
        "because the fit to each dataset informs the hierarchical distribution's parameters, which in turn improves\n",
        "constraints on the other datasets. In a hierarchical fit, we describe this as \"the datasets talking to one another\". \n",
        "\n",
        "For example, by itself, dataset_0 may give weak constraints on the slope spanning the range 1.3 -> 2.7 at 1 sigma \n",
        "confidence. Now, consider if simultaneously all of the other datasets provide strong constraints on the \n",
        "hierarchical's distribution's parameters, such that its `mean = 2.0 +- 0.1` and `sigma = 0.1 +- 0.05` at 1 sigma \n",
        "confidence. \n",
        "\n",
        "This will significantly change our inferred parameters for dataset 0, as the other datasets inform us\n",
        "that solutions where the slope is well below approximately 30 are less likely, because they are inconsistent with\n",
        "the parent hierarchical distribution's parameters!\n",
        "\n",
        "For complex graphical models with many hierarchical factors, this phenomena of the \"datasets talking to one another\" \n",
        "can be crucial in breaking degeneracies between parameters and maximally extracting information from extremely large\n",
        "datasets.\n",
        "\n",
        "__Wrap Up__\n",
        "\n",
        "By composing and fitting hierarchical models in the graphical modeling framework we can fit for global trends\n",
        "within large datasets. The tools applied in this tutorial and the previous tutorial can be easily extended to \n",
        "compose complex graphical models, with multiple shared parameters and hierarchical factors.\n",
        "\n",
        "However, there is a clear challenge scaling the graphical modeling framework up in this way: model complexity. As the \n",
        "model becomes more complex, an inadequate sampling of parameter space will lead one to infer local maxima. Furthermore,\n",
        "one will soon hit computational limits on how many datasets can feasibly be fitted simultaneously, both in terms of\n",
        "CPU time and memory limitations. \n",
        "\n",
        "Therefore, the next tutorial introduces expectation propagation, a framework that inspects the factor graph of a \n",
        "graphical model and partitions the model-fit into many separate fits on each graph node. When a fit is complete, \n",
        "it passes the information learned about the model to neighboring nodes. \n",
        "\n",
        "Therefore, graphs comprising hundreds of model components (and tens of thousands of parameters) can be fitted as \n",
        "many bite-sized model fits, where the model fitted at each node consists of just tens of parameters. This makes \n",
        "graphical models scalable to largest datasets and most complex models!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}