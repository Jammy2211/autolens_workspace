{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 6: Science Case\n",
        "========================\n",
        "\n",
        "This tutorial shows using a graphical model and EP for a realistic science case, fitting a sample of time-delay lensed\n",
        "quasars using a graphical model, where time delays allow us include the Cosmological parameter the Hubble constant H0\n",
        "as a shared free parameter in an graphical model.\n",
        "\n",
        "In this example we fit via a graphical model and Expectation Propagation (EP).\n",
        "\n",
        "__Sample Simulation__\n",
        "\n",
        "The dataset fitted in this example script is simulated imaging data of a sample of 3 galaxies.\n",
        "\n",
        "This data is not automatically provided with the autogalaxy workspace, and must be first simulated by running the\n",
        "script `autolens_workspace/scripts/simulators/imaging/samples/advanced/hubble_constant_time_delays.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from pathlib import Path\n",
        "import autolens as al\n",
        "import autofit as af"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Initialization__\n",
        "\n",
        "The following steps repeat all the initial steps performed in tutorial 2 and 3:\n",
        "\n",
        "This data is not automatically provided with the autogalaxy workspace, and must be first simulated by running the \n",
        "script `autolens_workspace/scripts/simulators/imaging/samples/advanced/mass_power_law.py`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_label = \"samples\"\n",
        "dataset_type = \"point_source\"\n",
        "dataset_sample_name = \"hubble_constant_time_delays\"\n",
        "\n",
        "dataset_path = Path(\"dataset\") / dataset_type / dataset_label / dataset_sample_name\n",
        "\n",
        "total_datasets = 3\n",
        "\n",
        "dataset_list = []\n",
        "\n",
        "for dataset_index in range(total_datasets):\n",
        "    dataset_sample_path = dataset_path / f\"dataset_{dataset_index}\"\n",
        "\n",
        "    dataset = al.from_json(\n",
        "        file_path=dataset_sample_path / \"point_dataset_with_time_delays.json\",\n",
        "    )\n",
        "\n",
        "    dataset_list.append(dataset)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Point Solver__\n",
        "\n",
        "We set up the `PointSolver`, which is used to compute the multiple images of the point source in the image-plane.\n",
        "\n",
        "There are no special settings or inputs for the fitting of time_delays, therefore the `PointSolver` is set up in the same way\n",
        "as in the `modeling/start_here.ipynb` notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grid = al.Grid2D.uniform(\n",
        "    shape_native=(100, 100),\n",
        "    pixel_scales=0.2,  # <- The pixel-scale describes the conversion from pixel units to arc-seconds.\n",
        ")\n",
        "\n",
        "solver = al.PointSolver.for_grid(\n",
        "    grid=grid, pixel_scale_precision=0.001, magnification_threshold=0.1\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Paths__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "path_prefix = Path(\"point_source\") / \"hierarchical\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "We compose our model using `Model` objects, which represent the lenses we fit to our data.\n",
        "\n",
        "This graphical model creates a non-linear parameter space that has parameters for every lens mass and source galaxy point\n",
        "source in our sample. In this example, there are 3 lenses each with their own model, therefore:\n",
        "\n",
        " - The lens galaxy's total mass distribution is an `Isothermal` [5 parameters].\n",
        " \n",
        " - The source galaxy's light is a point `Point` [2 parameters].\n",
        "\n",
        " - There is a single cosmological shared free parameter, `H0` [1 parameter]\n",
        "\n",
        " - There are 3 strong lenses in our graphical model [(3 x 7) + 1 = 22 parameters]. \n",
        "\n",
        "The overall dimensionality of each parameter space fitted separately via EP is therefore N=7.\n",
        "\n",
        "In total, the graph has N = 3 x 7 + 1 = 2 free parameters, albeit EP knows the `H0` is shared and fits it \n",
        "using EP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cosmology = af.Model(al.cosmo.FlatwCDMWrap)\n",
        "\n",
        "cosmology.H0 = af.UniformPrior(lower_limit=0.0, upper_limit=150.0)\n",
        "\n",
        "model_list = []\n",
        "\n",
        "for model_index in range(total_datasets):\n",
        "\n",
        "    # Lens:\n",
        "\n",
        "    mass = af.Model(al.mp.Isothermal)\n",
        "    mass.centre.centre_0 = 0.0\n",
        "    mass.centre.centre_1 = 0.0\n",
        "\n",
        "    lens = af.Model(al.Galaxy, redshift=0.5, mass=mass)\n",
        "\n",
        "    # Source:\n",
        "\n",
        "    point_0 = af.Model(al.ps.Point)\n",
        "\n",
        "    source = af.Model(al.Galaxy, redshift=1.0, point_0=point_0)\n",
        "\n",
        "    # Overall Lens Model:\n",
        "\n",
        "    model = af.Collection(\n",
        "        galaxies=af.Collection(lens=lens, source=source),\n",
        "        cosmology=cosmology,\n",
        "    )\n",
        "\n",
        "    model_list.append(model)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "analysis_list = []\n",
        "\n",
        "for dataset in dataset_list:\n",
        "\n",
        "    analysis = al.AnalysisPoint(dataset=dataset, solver=solver)\n",
        "\n",
        "    analysis_list.append(analysis)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Analysis Factors__\n",
        "\n",
        "Now we have our `Analysis` classes and graphical model, we can compose our `AnalysisFactor`'s, just like we did in the\n",
        "previous tutorial.\n",
        "\n",
        "However, unlike the previous tutorial, each `AnalysisFactor` is now assigned its own `search`. This is because the EP \n",
        "framework performs a model-fit to each node on the factor graph (e.g. each `AnalysisFactor`). Therefore, each node \n",
        "requires its own non-linear search. \n",
        "\n",
        "For complex graphs consisting of many  nodes, one could easily use different searches for different nodes on the factor \n",
        "graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "search = af.Nautilus(\n",
        "    path_prefix=Path(\"point_source\") / \"hierarchical\",\n",
        "    name=\"tutorial_6_science_case_graphical\",\n",
        "    n_live=150,\n",
        ")\n",
        "\n",
        "analysis_factor_list = []\n",
        "dataset_index = 0\n",
        "\n",
        "for model, analysis in zip(model_list, analysis_list):\n",
        "    dataset_name = f\"dataset_{dataset_index}\"\n",
        "    dataset_index += 1\n",
        "\n",
        "    analysis_factor = af.AnalysisFactor(\n",
        "        prior_model=model, analysis=analysis, optimiser=search, name=dataset_name\n",
        "    )\n",
        "\n",
        "    analysis_factor_list.append(analysis_factor)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We again combine our `AnalysisFactors` into one, to compose the factor graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "factor_graph = af.FactorGraphModel(*analysis_factor_list)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The factor graph model `info` attribute shows the complex model we are fitting, including both cosmological parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(factor_graph.global_prior_model.info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search__\n",
        "\n",
        "We can now use the search to fit factor graph, using its `global_prior_model` property."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result = search.fit(model=factor_graph.global_prior_model, analysis=factor_graph)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Expectation Propagation__\n",
        "\n",
        "We now perform the fit using EP as we did in tutorial 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "paths = af.DirectoryPaths(\n",
        "    name=Path(\"point_source\") / \"hierarchical\" / \"tutorial_6_science_case_ep\"\n",
        ")\n",
        "\n",
        "search = af.Nautilus(\n",
        "    path_prefix=Path(\"point_source\") / \"hierarchical\",\n",
        "    name=\"tutorial_6_science_case_ep\",\n",
        "    n_live=150,\n",
        ")\n",
        "\n",
        "analysis_factor_list = []\n",
        "\n",
        "dataset_index = 0\n",
        "\n",
        "for model, analysis in zip(model_list, analysis_list):\n",
        "    dataset_name = f\"dataset_{dataset_index}\"\n",
        "    dataset_index += 1\n",
        "\n",
        "    analysis_factor = af.AnalysisFactor(\n",
        "        prior_model=model, analysis=analysis, optimiser=search, name=dataset_name\n",
        "    )\n",
        "\n",
        "    analysis_factor_list.append(analysis_factor)\n",
        "\n",
        "factor_graph = af.FactorGraphModel(*analysis_factor_list)\n",
        "\n",
        "laplace = af.LaplaceOptimiser()\n",
        "\n",
        "factor_graph_result = factor_graph.optimise(\n",
        "    optimiser=laplace, paths=paths, ep_history=af.EPHistory(kl_tol=0.05), max_steps=5\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Output__\n",
        "\n",
        "The results of the factor graph, using the EP framework and message passing, are contained in the folder \n",
        "`output/graphical/imaging/tutorial_6_science_case`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "print(factor_graph_result)\n",
        "\n",
        "print(factor_graph_result.updated_ep_mean_field.mean_field)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Output__\n",
        "\n",
        "The MeanField object representing the posterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(factor_graph_result.updated_ep_mean_field.mean_field)\n",
        "print()\n",
        "\n",
        "print(factor_graph_result.updated_ep_mean_field.mean_field.variables)\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The logpdf of the posterior at the point specified by the dictionary values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# factor_graph_result.updated_ep_mean_field.mean_field(values=None)\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A dictionary of the mean with variables as keys."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(factor_graph_result.updated_ep_mean_field.mean_field.mean)\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A dictionary of the variance with variables as keys."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(factor_graph_result.updated_ep_mean_field.mean_field.variance)\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A dictionary of the s.d./variance**0.5 with variables as keys."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(factor_graph_result.updated_ep_mean_field.mean_field.scale)\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "self.updated_ep_mean_field.mean_field[v: Variable] gives the Message/approximation of the posterior for an \n",
        "individual variable of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# factor_graph_result.updated_ep_mean_field.mean_field[\"help\"]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}