{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial Optional: Search Chaining\n",
        "==================================\n",
        "\n",
        "The graphical model examples compose a fit individual models to large datasets.\n",
        "\n",
        "For complex models, one may need to combine graphical models with search chaining in order to ensure that models\n",
        "are initialized in a robust manner, ensuring automated modeling.\n",
        "\n",
        "This example script shows how models can be fitted via chaining and output /loaded from to .json files in order\n",
        "to combine search chaining with graphical models.\n",
        "\n",
        "__Sample Simulation__\n",
        "\n",
        "The dataset fitted in this example script is simulated imaging data of a sample of 3 galaxies.\n",
        "\n",
        "This data is not automatically provided with the autogalaxy workspace, and must be first simulated by running the\n",
        "script `autolens_workspace/scripts/simulators/imaging/samples/simple__no_lens_light.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "from pyprojroot import here\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "import json\n",
        "from os import path\n",
        "import os\n",
        "import autofit as af\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Dataset__\n",
        "\n",
        "For each dataset in our sample we set up the correct path and load it by iterating over a for loop. \n",
        "\n",
        "This data is not automatically provided with the autogalaxy workspace, and must be first simulated by running the \n",
        "script `autolens_workspace/scripts/simulators/imaging/samples/simple__no_lens_light.py`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_label = \"samples\"\n",
        "dataset_type = \"imaging\"\n",
        "dataset_sample_name = \"simple__no_lens_light\"\n",
        "\n",
        "dataset_path = path.join(\"dataset\", dataset_type, dataset_label, dataset_sample_name)\n",
        "\n",
        "total_datasets = 3\n",
        "\n",
        "dataset_list = []\n",
        "\n",
        "for dataset_index in range(total_datasets):\n",
        "    dataset_sample_path = path.join(dataset_path, f\"dataset_{dataset_index}\")\n",
        "\n",
        "    dataset_list.append(\n",
        "        al.Imaging.from_fits(\n",
        "            data_path=path.join(dataset_sample_path, \"data.fits\"),\n",
        "            psf_path=path.join(dataset_sample_path, \"psf.fits\"),\n",
        "            noise_map_path=path.join(dataset_sample_path, \"noise_map.fits\"),\n",
        "            pixel_scales=0.1,\n",
        "        )\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Mask__\n",
        "\n",
        "We now mask each lens in our dataset, using the imaging list we created above.\n",
        "\n",
        "We will assume a 3.0\" mask for every lens in the dataset is appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "masked_imaging_list = []\n",
        "\n",
        "for dataset in dataset_list:\n",
        "    mask = al.Mask2D.circular(\n",
        "        shape_native=dataset.shape_native, pixel_scales=dataset.pixel_scales, radius=3.0\n",
        "    )\n",
        "\n",
        "    masked_imaging_list.append(dataset.apply_mask(mask=mask))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Paths__\n",
        "\n",
        "The path the results of all model-fits are output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "path_prefix = path.join(\"imaging\", \"hierarchical\", \"tutorial_optional_search_chaining\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model__\n",
        "\n",
        "We compose our model using `Model` objects, which represent the lenses we fit to our data. In this \n",
        "example we fit a model where:\n",
        "\n",
        " - The lens's bulge is a linear parametric `Sersic` bulge with its centre fixed to the input \n",
        " value of (0.0, 0.0) [4 parameters]. \n",
        "\n",
        "The number of free parameters and therefore the dimensionality of non-linear parameter space is N=5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lens = af.Model(al.Galaxy, redshift=0.5, mass=al.mp.PowerLawSph)\n",
        "lens.mass.centre = (0.0, 0.0)\n",
        "\n",
        "source = af.Model(al.Galaxy, redshift=1.0, bulge=al.lp_linear.ExponentialCoreSph)\n",
        "\n",
        "model = af.Collection(galaxies=af.Collection(lens=lens, source=source))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search + Analysis + Model-Fit__\n",
        "\n",
        "For each dataset we now create a non-linear search, analysis and perform the model-fit using this model.\n",
        "\n",
        "Results are output to a unique folder named using the `dataset_index`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_1_list = []\n",
        "\n",
        "for dataset_index, masked_dataset in enumerate(masked_imaging_list):\n",
        "    dataset_name_with_index = f\"dataset_{dataset_index}\"\n",
        "    path_prefix_with_index = path.join(path_prefix, dataset_name_with_index)\n",
        "\n",
        "    search_1 = af.Nautilus(\n",
        "        path_prefix=path_prefix,\n",
        "        name=\"search[1]__simple__no_lens_light\",\n",
        "        unique_tag=dataset_name_with_index,\n",
        "        n_live=100,\n",
        "    )\n",
        "\n",
        "    analysis_1 = al.AnalysisImaging(dataset=masked_dataset)\n",
        "\n",
        "    result_1 = search_1.fit(model=model, analysis=analysis_1)\n",
        "    result_1_list.append(result_1)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model (Search 2)__\n",
        "\n",
        "We use the results of search 1 to create the lens models fitted in search 2, where:\n",
        "\n",
        " - The lens's bulge is again a linear parametric `Sersic` bulge [6 parameters: priors passed from search 1]. \n",
        "\n",
        "The number of free parameters and therefore the dimensionality of non-linear parameter space is N=7.\n",
        "\n",
        "Prior passing via search chaining is now specific to each result, thus this operates on a list via for loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_2_list = []\n",
        "\n",
        "for result_1 in result_1_list:\n",
        "    source = result_1.model.galaxies.source\n",
        "\n",
        "    mass = af.Model(al.mp.PowerLawSph)\n",
        "    mass.take_attributes(result_1.model.galaxies.lens.mass)\n",
        "\n",
        "    lens = af.Model(al.Galaxy, redshift=0.5, mass=mass)\n",
        "\n",
        "    model_2 = af.Collection(galaxies=af.Collection(lens=lens, source=source))\n",
        "    model_2_list.append(model_2)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Search + Analysis + Model-Fit (Search 2)__\n",
        "\n",
        "We now create the non-linear search, analysis and perform the model-fit using this model.\n",
        "\n",
        "You may wish to inspect the `model.info` file of the search 2 model-fit to ensure the priors were passed correctly, as \n",
        "well as the checkout the results to ensure an accurate power-law mass model is inferred."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_2_list = []\n",
        "\n",
        "for dataset_index, masked_dataset in enumerate(masked_imaging_list):\n",
        "    dataset_name_with_index = f\"dataset_{dataset_index}\"\n",
        "    path_prefix_with_index = path.join(path_prefix, dataset_name_with_index)\n",
        "\n",
        "    search_2 = af.Nautilus(\n",
        "        path_prefix=path_prefix,\n",
        "        name=\"search[2]__mass_sph_pl__source_sersic\",\n",
        "        unique_tag=dataset_name_with_index,\n",
        "        n_live=100,\n",
        "    )\n",
        "\n",
        "    analysis_2 = al.AnalysisImaging(dataset=masked_dataset)\n",
        "\n",
        "    result_2 = search_2.fit(model=model_2, analysis=analysis_2)\n",
        "    result_2_list.append(result_2)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Output__\n",
        "\n",
        "The model can also be output to a .`json` file and loaded in another Python script.\n",
        "\n",
        "This is not necessary for combining search chaining and graphical models, but can help make scripts shorter if the\n",
        "search chaining takes up a lot of lines of Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_path = path.join(\"imaging\", \"hierarchical\", \"models\", \"initial\")\n",
        "\n",
        "for dataset_index, model in enumerate(model_2_list):\n",
        "    model_dataset_path = path.join(model_path, f\"dataset_{dataset_index}\")\n",
        "\n",
        "    os.makedirs(model_dataset_path, exist_ok=True)\n",
        "\n",
        "    model_file = path.join(model_dataset_path, \"model.json\")\n",
        "\n",
        "    with open(model_file, \"w\") as f:\n",
        "        json.dump(model.dict(), f, indent=4)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Model Loading__\n",
        "\n",
        "We can load the model above as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_path = path.join(\"imaging\", \"hierarchical\", \"models\", \"initial\")\n",
        "\n",
        "model_list = []\n",
        "\n",
        "for dataset_index in range(total_datasets):\n",
        "    model_file = path.join(model_path, f\"dataset_{dataset_index}\", \"model.json\")\n",
        "\n",
        "    model = af.Collection.from_json(file=model_file)\n",
        "\n",
        "    model_list.append(model)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}