{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 6: Alternative Searches\n",
        "================================\n",
        "\n",
        "Up to now, we've always used the non-linear search Dynesty and not considered the input parameters that control its\n",
        "sampling. In this tutorial, we'll consider how we can change these setting to balance finding the global maxima\n",
        "solution with fast run time, as well as other types of non-linear searches we can use to perform lens modeling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from autoconf import conf\n",
        "import autolens as al\n",
        "import autolens.plot as aplt\n",
        "import autofit as af\n",
        "from pyprojroot import here\n",
        "\n",
        "workspace_path = str(here())\n",
        "print(\"Workspace Path: \", workspace_path)\n",
        "\n",
        "conf.instance = conf.Config(\n",
        "    config_path=f\"{workspace_path}/howtolens/config\",\n",
        "    output_path=f\"{workspace_path}/howtolens/output\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll use new strong lensing data, where:\n",
        "\n",
        " - The lens galaxy's _LightProfile_ is an _EllipticalSersic_.\n",
        " - The lens galaxy's _MassProfile_ is an _EllipticalIsothermal_.\n",
        " - The source galaxy's _LightProfile_ is an _EllipticalSersic_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from autolens_workspace.howtolens.simulators.chapter_2 import (\n",
        "    lens_sersic_sie__source_sersic,\n",
        ")\n",
        "\n",
        "dataset_type = \"chapter_2\"\n",
        "dataset_name = \"lens_sersic_sie__source_sersic\"\n",
        "dataset_path = f\"{workspace_path}/howtolens/dataset/{dataset_type}/{dataset_name}\"\n",
        "\n",
        "imaging = al.Imaging.from_fits(\n",
        "    image_path=f\"{dataset_path}/image.fits\",\n",
        "    noise_map_path=f\"{dataset_path}/noise_map.fits\",\n",
        "    psf_path=f\"{dataset_path}/psf.fits\",\n",
        "    pixel_scales=0.1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll create and use a smaller 2.0\" _Mask_ again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mask = al.Mask.circular(\n",
        "    shape_2d=imaging.shape_2d, pixel_scales=imaging.pixel_scales, radius=2.6\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When plotted, the lens light's is clearly visible in the centre of the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "aplt.Imaging.subplot_imaging(imaging=imaging, mask=mask)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Like in the previous tutorial, we use a_PhaseSettingsImaging_ object to specify our model-fitting procedure uses a \n",
        "regular _Grid_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "settings = al.PhaseSettingsImaging(grid_class=al.Grid, sub_size=2)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Nested Sampling__\n",
        "\n",
        "Lets first perform the model-fit using Dynesty, but first discuss in a bit more detail how Dynesty works.\n",
        "\n",
        "Dynesty is a 'nested sampling' algorithm. As we described in tutorial 1, it throws down a set of 'live points' in \n",
        "parameter space, where each live point corresponds to a lens model with a given set of parameters. These points are\n",
        "intially distributed according to our priors, hence why tuning our priors allows us to sample parameter space faster.\n",
        "\n",
        "The following settings control how fast a nested sampling algorithm samples non-linear parameter space:\n",
        "\n",
        "n_live_points:\n",
        "\n",
        " The number of live points which sample parameter space. More points provide a more thorough sampling of parameter \n",
        " space, increasing the probability that we locate the global maxima solution at the expense if taking longer to \n",
        " convergence on this solution. Ideally, we would use as few live pooints as possible to locate the global maxima\n",
        " as quickly as possible.\n",
        "\n",
        "evidence_tolerance : float\n",
        "\n",
        " Dynesty stops sampling when it estimates that it has converged on the global maxima solution in parameter space and\n",
        " that continuing sampling will not increase the log likelihoods of the current live points more than this evidence\n",
        " tolerance value. For example, an evidence tolerance of 1.0 roughly require that all live points have log likelihood\n",
        " values within 1.0 of one another. Thus, the higher the evidence_tolerance the sooner Dynesty will stop running. \n",
        "    \n",
        " A high tolerance will make the errors estimated on every parameter unreliable, and the tolerance must be kept below\n",
        " 0.8 if you want reliable error estimates. However, when linking phases, we typically *do not care* about the errors \n",
        " in the first phase, therefore setting a high evidence tolerance can be an effective means to make Dynesty converge\n",
        " faster. \n",
        "\n",
        "Lets perform two fits, where:\n",
        "\n",
        " - One has many live points, a low sampling efficiency and evidence tolerance, causing the non-linear search to\n",
        " take a long time to run (in fact, on my laptop, this run takes > 500000 iterations which translates to > 6 \n",
        " hours. So, I've commented the run function out to not waste your time, but feel free to uncomment it and run\n",
        " the phase to see this for yourself!).\n",
        "      \n",
        " - One has few live points, a high sampling efficiency and evidence tolerance, causing the non-linear search to\n",
        " converge and end quicker."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "phase_slow = al.PhaseImaging(\n",
        "    phase_name=\"phase_t6_slow\",\n",
        "    settings=settings,\n",
        "    galaxies=dict(\n",
        "        lens=al.GalaxyModel(\n",
        "            redshift=0.5, light=al.lp.EllipticalSersic, mass=al.mp.EllipticalIsothermal\n",
        "        ),\n",
        "        source=al.GalaxyModel(redshift=1.0, light=al.lp.EllipticalSersic),\n",
        "    ),\n",
        "    search=af.DynestyStatic(n_live_points=150, evidence_tolerance=0.8),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    \"Dynesty has begun running - checkout the workspace/output\"\n",
        "    \" folder for live output of the results, images and lens model.\"\n",
        "    \" This Jupyter notebook cell with progress once Dynesty has completed - this could take some time!\"\n",
        ")\n",
        "\n",
        "# result_slow = phase_slow.run(dataset=imaging, mask=mask)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets check that we get a good model and fit to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# aplt.FitImaging.subplot_fit_imaging(fit=result_slow.max_log_likelihood_fit)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the result to tell us how many iterations Dynesty took to convergence on the solution.\n",
        "\"\"\"\n",
        "print(\"Total Dynesty Iterations (If you skip running the phase, this is ~ 500000):\")\n",
        "# print(result_slow.samples.total_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets run the phase with fast setting, so we can compare the total number of iterations required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "phase_fast = al.PhaseImaging(\n",
        "    phase_name=\"phase_t6_fast\",\n",
        "    settings=settings,\n",
        "    galaxies=dict(\n",
        "        lens=al.GalaxyModel(\n",
        "            redshift=0.5, light=al.lp.EllipticalSersic, mass=al.mp.EllipticalIsothermal\n",
        "        ),\n",
        "        source=al.GalaxyModel(redshift=1.0, light=al.lp.EllipticalSersic),\n",
        "    ),\n",
        "    search=af.DynestyStatic(n_live_points=30),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\n",
        "    \"Dynesty has begun running - checkout the workspace/output\"\n",
        "    \" folder for live output of the results, images and lens model.\"\n",
        "    \" This Jupyter notebook cell with progress once Dynesty has completed - this could take some time!\"\n",
        ")\n",
        "\n",
        "# result_fast = phase_fast.run(dataset=imaging, mask=mask)\n",
        "\n",
        "print(\"Dynesty has finished run - you may now continue the notebook.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets check that this search, despite its faster sampling settings, still gives us the global maxima solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# aplt.FitImaging.subplot_fit_imaging(fit=result_fast.max_log_likelihood_fit)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And now lets confirm it uses significantly fewer iterations.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Total Dynesty Iterations:\")\n",
        "print(\"Slow settings: ~500000\")\n",
        "# print(result_slow.samples.total_samples)\n",
        "# print(\"Fast settings: \", result_fast.samples.total_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Optimizers__\n",
        "\n",
        "Nested sampling algorithms like Dynesty provides the errors on all of the model parameters, by fully mapping out all \n",
        "of the high likelihood regions of parameter space. This provides knowledge on the complete *range* of models that do \n",
        "and do not provide high likelihood fits to the data, but takes many extra iterations to perform. If we require precise \n",
        "error estimates (perhaps this is our final lens model fit before we publish the results in a paper), these extra\n",
        "iterations are acceptable. \n",
        "\n",
        "However, we often don't care about the errors. For example, in the previous tutorial when linking phases, the only \n",
        "result we used from the fit performed in the first phase was the maximum log likelihood model, omitting the errors\n",
        "entirely! Its seems wasteful to use a nested sampling algorithm like Dynesty to map out the entirity of parameter\n",
        "space when we don't use this information! \n",
        "\n",
        "There are a class of non-linear searches called 'optimizers', which seek to optimize just one thing, the log \n",
        "likelihood. They want to find the model that maximizes the log likelihood, with no regard for the errors, thus not \n",
        "wasting time mapping out in intricate detail every facet of parameter space. Lets see how much faster we can find a \n",
        "good fit to the lens data using an optimizer.\n",
        "\n",
        "We'll use the 'Particle Swarm Optimizer' PySwarms. Conceptually this works quite similar to Dynesty, it has a set of \n",
        "points in parameter space (called 'particles') and it uses their likelihoods to determine where it thinks the higher\n",
        "likelihood regions of parameter space are. \n",
        "\n",
        "Unlike Dynesty, this algorithm requires us to specify how many iterations it should perform to find the global \n",
        "maxima solutions. Here, an iteration is the number of samples performed by every particle, so the total number of\n",
        "iterations is n_particles * iters. Lets try a total of ? iterations, a factor ? less than our Dynesty runs above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "phase_pso = al.PhaseImaging(\n",
        "    phase_name=\"phase_t7_pso\",\n",
        "    settings=settings,\n",
        "    galaxies=dict(\n",
        "        lens=al.GalaxyModel(\n",
        "            redshift=0.5, light=al.lp.EllipticalSersic, mass=al.mp.EllipticalIsothermal\n",
        "        ),\n",
        "        source=al.GalaxyModel(redshift=1.0, light=al.lp.EllipticalSersic),\n",
        "    ),\n",
        "    search=af.PySwarmsLocal(n_particles=50, iters=1000),\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Dynesty has begun running - checkout the workspace/output\"\n",
        "    \" folder for live output of the results, images and lens model.\"\n",
        "    \" This Jupyter notebook cell with progress once Dynesty has completed - this could take some time!\"\n",
        ")\n",
        "\n",
        "# result_pso = phase_pso.run(dataset=imaging, mask=mask)\n",
        "\n",
        "print(\"PySwarms has finished run - you may now continue the notebook.\")\n",
        "\n",
        "# aplt.FitImaging.subplot_fit_imaging(fit=result_pso.max_log_likelihood_fit)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It worked, and was much faster than Dynesty!\n",
        "\n",
        "So, when should we use Dynesty and when should we use PySwarms? Its simple:\n",
        "\n",
        " - If we don't care about errors and want to get the global maxima solution as quickly as possible, we should use\n",
        "      PySwarms.\n",
        "      \n",
        " - If we want a model with robust and precise errors, we should use Dynesty.\n",
        "    \n",
        "There is one exception however, for complex models whose priors have not be well tuned or initialized by a previous \n",
        "phase, PySwarms has a tendancy to locate a local maxima. Dynesty's slower but more complete sampling of parameter space \n",
        "will often find the global maxima when PySwarms doesn't. So, if you're not happy with the results PySwarms is giving, \n",
        "it may be shrewd to bite-the-button on run-time and use Dynesty to get your initial lens model fit.\n",
        "\n",
        "In the next chapter, when we introduce pipelines, you'll note that are our general strategy to lens modeling is to\n",
        "initialize the model-fit with Dynesty, perform intermediate phases that refine the model with PySwarms and then\n",
        "end with Dynesty for robust errors. Here, we choose our non-linear searches based on what result we want!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__MCMC__\n",
        "\n",
        "For users familiar with Markov Chain Monte Carlo (MCMC) non-linear samplers, PyAutoFit supports the non-linear\n",
        "search *Emcee* (af.Emcee). We have found this to be less effective at lens modeling than Dynesty and PySwarms,\n",
        "but it is sill pretty successful. I've included an example run of Emcee below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "phase_mcmc = al.PhaseImaging(\n",
        "    phase_name=\"phase_t7_mcmc\",\n",
        "    settings=settings,\n",
        "    galaxies=dict(\n",
        "        lens=al.GalaxyModel(\n",
        "            redshift=0.5, light=al.lp.EllipticalSersic, mass=al.mp.EllipticalIsothermal\n",
        "        ),\n",
        "        source=al.GalaxyModel(redshift=1.0, light=al.lp.EllipticalSersic),\n",
        "    ),\n",
        "    search=af.Emcee(nwalkers=50, nsteps=1000),\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Emcee has begun running - checkout the workspace/output\"\n",
        "    \" folder for live output of the results, images and lens model.\"\n",
        "    \" This Jupyter notebook cell with progress once Dynesty has completed - this could take some time!\"\n",
        ")\n",
        "\n",
        "# result_mcmc = phase_mcmc.run(dataset=imaging, mask=mask)\n",
        "\n",
        "print(\"Emcee has finished run - you may now continue the notebook.\")\n",
        "\n",
        "# aplt.FitImaging.subplot_fit_imaging(fit=result_mcmc.max_log_likelihood_fit)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}