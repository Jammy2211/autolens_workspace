{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 1: Fit Problems\n",
        "========================\n",
        "\n",
        "To begin, make sure you have read the 'introduction' file carefully, as a clear understanding of how the Bayesian\n",
        "log_evidence works is key to understanding this chapter!\n",
        "\n",
        "In the previous chapter we investigated two _Pixelization_'s; Rectangular and VoronoiMagnification. We learnt that the\n",
        "latter was better than the former, because it dedicated more source-pixels to the regions of the source-plane where we\n",
        "had more data, e.g, the high-magnification regions. Therefore, we could fit the data using fewer source pixels,\n",
        "which improved computational efficiency and the Bayesian log evidence.\n",
        "\n",
        "So far, we've used just one _Regularization_scheme; Constant. As the name suggests, this _Regularization_scheme applies\n",
        "just one regularization_coefficient when regularizing source-pixels with one another. In case you've forgot, here is\n",
        "a refresher of regularization, from chapter 4:\n",
        "\n",
        "-------------------------------------------- \n",
        "\n",
        "When our _Inversion_ reconstructs a source, it doesn't *just* compute the set of fluxes that best-fit the image. It\n",
        "also 'regularizes' this solution, going to every pixel on our rectangular _Grid_ and comparing its reconstructed flux\n",
        "with its 4 neighboring pixels. If the difference in flux is large the solution is penalized, reducing its log likelihood.\n",
        "You can think of this as us applying a prior that our source galaxy solution is 'smooth'.\n",
        "\n",
        "This adds a 'penalty term' to the log likelihood of an _Inversion_ which is the summed difference between the reconstructed\n",
        "fluxes of every source-pixel pair multiplied by the regularization_coefficient. By setting the regularization\n",
        "coefficient to zero, we set this penalty term to zero, meaning that _Regularization_is omitted.\n",
        "\n",
        "Why do we need to regularize our solution? Well, we just saw why - if we don't apply this smoothing, we 'over-fit'\n",
        "the image. More specifically, we over-fit the noise in the image, which is what the large flux values located at the\n",
        "exteriors of the source reconstruction are doing. Think about it, if your sole aim is to maximize the log likelihood, the\n",
        "best way to do this is to fit *everything* accurately, including the noise.\n",
        "\n",
        "----------------------------------------------\n",
        "\n",
        "So, when using a Constant _Regularization_scheme, we regularize the source by adding up the difference in fluxes\n",
        "between all source-pixels multiplied by one single value of a regularization_coefficient. This means that every\n",
        "single source pixel receives the same 'level' of regularization, regardless of whether it is reconstructing the\n",
        "bright central regions of the source or its faint exterior regions.\n",
        "\n",
        "\n",
        "In this tutorial, we'll learn that our magnification-based _Pixelization_and constant _Regularization_schemes are far\n",
        "from optimal. To understand why, we'll inspect fits to three strong lenses, simulated using the same _MassProfile_ but\n",
        "with different sources whose _LightProfile_'s become gradually more compact. For all 3 fits, we'll use the same\n",
        "source-plane resolution and a regularization_coefficient that maximize the Bayesian log evidence. Thus, these are the\n",
        "'best' source reconstructions we can hope to achieve when adapting to the magnification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll use 3 sources whose effective radius and Sersic index are changed such that each is more compact that the last."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "source_galaxy_flat = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    light=al.lp.EllipticalSersic(\n",
        "        centre=(0.0, 0.0),\n",
        "        elliptical_comps=(0.0, 0.15),\n",
        "        intensity=0.2,\n",
        "        effective_radius=0.5,\n",
        "        sersic_index=1.0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "source_galaxy_compact = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    light=al.lp.EllipticalSersic(\n",
        "        centre=(0.0, 0.0),\n",
        "        elliptical_comps=(0.0, 0.15),\n",
        "        intensity=0.2,\n",
        "        effective_radius=0.2,\n",
        "        sersic_index=2.5,\n",
        "    ),\n",
        ")\n",
        "\n",
        "source_galaxy_super_compact = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    light=al.lp.EllipticalSersic(\n",
        "        centre=(0.0, 0.0),\n",
        "        elliptical_comps=(0.0, 0.15),\n",
        "        intensity=0.2,\n",
        "        effective_radius=0.1,\n",
        "        sersic_index=4.0,\n",
        "    ),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The function below uses each source galaxy to simulate _Imaging_ data. It performs the usual tasks we are used to \n",
        "seeing (make the PSF, _Galaxy_'s , _Tracer_, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def simulate_for_source_galaxy(source_galaxy):\n",
        "\n",
        "    grid = al.Grid.uniform(shape_2d=(150, 150), pixel_scales=0.05, sub_size=2)\n",
        "\n",
        "    psf = al.Kernel.from_gaussian(shape_2d=(11, 11), sigma=0.05, pixel_scales=0.05)\n",
        "\n",
        "    lens_galaxy = al.Galaxy(\n",
        "        redshift=0.5,\n",
        "        mass=al.mp.EllipticalIsothermal(\n",
        "            centre=(0.0, 0.0), elliptical_comps=(0.111111, 0.0), einstein_radius=1.6\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    tracer = al.Tracer.from_galaxies(galaxies=[lens_galaxy, source_galaxy])\n",
        "\n",
        "    simulator = al.SimulatorImaging(\n",
        "        exposure_time_map=al.Array.full(fill_value=300.0, shape_2d=grid.shape_2d),\n",
        "        psf=psf,\n",
        "        background_sky_map=al.Array.full(fill_value=100.0, shape_2d=grid.shape_2d),\n",
        "        add_noise=True,\n",
        "        noise_seed=1,\n",
        "    )\n",
        "\n",
        "    return simulator.from_tracer_and_grid(tracer=tracer, grid=grid)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll use the same 3.0\" mask to fit all three of our sources."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mask = al.Mask.circular(shape_2d=(150, 150), pixel_scales=0.05, sub_size=2, radius=3.0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, lets simulate all 3 of our source's as _Imaging_ data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "imaging_source_flat = simulate_for_source_galaxy(source_galaxy=source_galaxy_flat)\n",
        "\n",
        "imaging_source_compact = simulate_for_source_galaxy(source_galaxy=source_galaxy_compact)\n",
        "\n",
        "imaging_source_super_compact = simulate_for_source_galaxy(\n",
        "    source_galaxy=source_galaxy_super_compact\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll make one more useful function which fits each simulated _Imaging_ with a VoronoiMagniication _Pixelization_ and \n",
        "_Constant_ _Regularization_ scheme.\n",
        "\n",
        "We'll input the regularization_coefficient of each fit, so that for each simulated source we regularize it at an \n",
        "appropriate level. Again, there is nothing new in this function you haven't seen before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def fit_imaging_with_voronoi_magnification_pixelization(\n",
        "    imaging, mask, regularization_coefficient\n",
        "):\n",
        "\n",
        "    masked_imaging = al.MaskedImaging(\n",
        "        imaging=imaging,\n",
        "        mask=mask,\n",
        "        settings=al.SettingsMaskedImaging(grid_class=al.Grid, sub_size=2),\n",
        "    )\n",
        "\n",
        "    lens_galaxy = al.Galaxy(\n",
        "        redshift=0.5,\n",
        "        mass=al.mp.EllipticalIsothermal(\n",
        "            centre=(0.0, 0.0), elliptical_comps=(0.111111, 0.0), einstein_radius=1.6\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    source_galaxy = al.Galaxy(\n",
        "        redshift=1.0,\n",
        "        pixelization=al.pix.VoronoiMagnification(shape=(30, 30)),\n",
        "        regularization=al.reg.Constant(coefficient=regularization_coefficient),\n",
        "    )\n",
        "\n",
        "    tracer = al.Tracer.from_galaxies(galaxies=[lens_galaxy, source_galaxy])\n",
        "\n",
        "    return al.FitImaging(masked_imaging=masked_imaging, tracer=tracer)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets fit our first source with the flattest _LightProfile_. One should note that this uses the highest _Regularization_\n",
        "coefficient of our 3 fits (as determined by maximizing the Bayesian log evidence)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit_flat = fit_imaging_with_voronoi_magnification_pixelization(\n",
        "    imaging=imaging_source_flat, mask=mask, regularization_coefficient=9.2\n",
        ")\n",
        "\n",
        "aplt.FitImaging.subplot_fit_imaging(\n",
        "    fit=fit_flat,\n",
        "    include=aplt.Include(inversion_image_pixelization_grid=True, mask=True),\n",
        ")\n",
        "\n",
        "aplt.Inversion.reconstruction(\n",
        "    inversion=fit_flat.inversion, include=aplt.Include(inversion_pixelization_grid=True)\n",
        ")\n",
        "\n",
        "print(fit_flat.log_evidence)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Okay, so the fit was *excellent*. There were effectively no residuals in the fit, and the source has been \n",
        "reconstructed using lots of pixels! Nice!\n",
        "\n",
        "Now, lets fit the next source, which is more compact."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit_compact = fit_imaging_with_voronoi_magnification_pixelization(\n",
        "    imaging=imaging_source_compact, mask=mask, regularization_coefficient=3.3\n",
        ")\n",
        "\n",
        "aplt.FitImaging.subplot_fit_imaging(\n",
        "    fit=fit_compact,\n",
        "    include=aplt.Include(inversion_image_pixelization_grid=True, mask=True),\n",
        ")\n",
        "\n",
        "aplt.Inversion.reconstruction(\n",
        "    inversion=fit_compact.inversion,\n",
        "    include=aplt.Include(inversion_pixelization_grid=True),\n",
        ")\n",
        "\n",
        "print(fit_compact.log_evidence)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Oh no! The fit doesn't look so good! Sure, we reconstruct *most* of the lensed source's structure, but there are two \n",
        "clear 'blobs' in the residual-map where we are failing to reconstruct the central regions of the source galaxy.\n",
        "\n",
        "Take a second to think about why this might be. Is it the _Pixelization_? The _Regularization_?\n",
        "\n",
        "Okay, so finally, we're going to fit our super compact source. Given that the results for the compact source didn't \n",
        "look so good, you'd be right in assuming this is just going to make things even worse. Again, think about why this \n",
        "might be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit_super_compact = fit_imaging_with_voronoi_magnification_pixelization(\n",
        "    imaging=imaging_source_super_compact, mask=mask, regularization_coefficient=3.1\n",
        ")\n",
        "\n",
        "aplt.FitImaging.subplot_fit_imaging(\n",
        "    fit=fit_super_compact,\n",
        "    include=aplt.Include(inversion_image_pixelization_grid=True, mask=True),\n",
        ")\n",
        "\n",
        "aplt.Inversion.reconstruction(\n",
        "    inversion=fit_super_compact.inversion,\n",
        "    include=aplt.Include(inversion_pixelization_grid=True),\n",
        ")\n",
        "\n",
        "print(fit_super_compact.log_evidence)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Okay, so what did we learn? The more compact our source, the worse the fit. This happens even though we are using the \n",
        "*correct* lens mass model, telling us that something is going fundamentally wrong with our source reconstruction and \n",
        "_Inversion_. As you might of guessed, both our _Pixelization_and _Regularization_ scheme are to blame!\n",
        "\n",
        "__Pixelization__\n",
        "\n",
        "For the _Pixelization_ the problem is the same one we found when comparing the _Rectangular_ and _VoronoiMagnification_\n",
        "_Pixelization_'s. Put simply, we are not dedicating enough source-pixels to the central regions of the source \n",
        "reconstruction, e.g. where it's brightest. As the source becomes more compact, the source reconstruction doesn't \n",
        "have enough resolution to resolve its fine-detailed central structure, causing the fit to the image to degrade.\n",
        "\n",
        "Think about it, as we made our sources more compact we go from reconstructing them using ~100 source pixels, to ~20 \n",
        "source pixels to ~ 10 source pixels. This is why we advocated not using the _Rectangular_ _Pixelization_in the previous \n",
        "chapter!\n",
        "\n",
        "It turns out that adapting to the magnification wasn't the best idea all along. As we simulated more compact sources \n",
        "the magnification (which is determined via the mass model) didn't change. So, we foolishly reconstructed each source\n",
        "using fewer and fewer pixels, leading to a worse and worse fit! Furthermore, these source's happened to be located in \n",
        "the highest magnification regions of the source plane! If the source's were further away from the centre of the \n",
        "caustic, the _VoronoiMagnification_ _Pixelization_ would use *even less* pixels to reconstruct it. That is NOT what we \n",
        "want!\n",
        "\n",
        "__Regularization__\n",
        "\n",
        "Regularization also causes problems. When using a _Constant_ _Regularization_scheme, we regularize the source by \n",
        "adding up the difference in fluxes between all source-pixels multiplied by one single value of a _Regularization_\n",
        "coefficient. This means that, every single source pixel receives the same 'level' of regularization, regardless of \n",
        "whether it is reconstructing the bright central regions of the source or its faint exterior regions. Lets look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "aplt.Inversion.regularization_weights(\n",
        "    inversion=fit_compact.inversion,\n",
        "    include=aplt.Include(inversion_pixelization_grid=True),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, all pixels are regularized with our input regularization_coefficient value of 3.6.\n",
        "\n",
        "Is this the best way to regularize the source? Well, as you've probably guessed, it isn't. But why not? Its \n",
        "because different regions of the source demand different levels of regularization:\n",
        "\n",
        " 1) In the source's central regions its flux gradient is steepest; the change in flux between two source pixels is \n",
        " much larger than in the exterior regions where the gradient is flatter (or there is no source flux at all). To \n",
        " reconstruct the detailed structure of the source's cuspy inner regions, the regularization_coefficient needs to \n",
        " be much lower to avoid over-smoothing.\n",
        "\n",
        " 2) On the flip side, the source reconstruction wants to assume a high regularization_coefficient further out \n",
        " because the source's flux gradient is flat (or there is no source signal at all). Higher \n",
        " regularization_coefficients will increase the Bayesian log evidence because by smoothing more source-pixels it \n",
        " makes the solution 'simpler', given that correlating the flux in these source pixels the solution effectively \n",
        " uses fewer source-pixels (e.g. degrees of freedom).\n",
        "\n",
        "So, herein lies the pitfall of a constant _Regularization_scheme. Some parts of the reconstructed source demand a \n",
        "low regularization_coefficient whereas other parts want a high value. Unfortunately, we end up with an intermediate \n",
        "regularization coefficient that over-smooths the source's central regions whilst failing to fully correlate exterior \n",
        "pixels. Thus, by using an adaptive _Regularization_scheme, new solutions that further increase the Bayesian log \n",
        "evidence become accessible.\n",
        "\n",
        "__Noise Map__\n",
        "\n",
        "Before we wrap up this tutorial, I want us to also consider the role of our noise-map and get you thinking about \n",
        "why we might want to scale its variances. Lets look at the super-compact fit again;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "aplt.FitImaging.subplot_fit_imaging(\n",
        "    fit=fit_super_compact,\n",
        "    include=aplt.Include(inversion_image_pixelization_grid=True, mask=True),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, whats the problem? Look closely at the 'chi-squared image'. Here, you'll note that a small subset of our data \n",
        "have extremely large chi-squared values. This means our non-linear search (which is trying minimize chi-squared) is \n",
        "going to seek solutions which primarily only reduce these chi-squared values. For the image above a small subset of \n",
        "the data (e.g. < 5% of pixels) contributes to the majority of the log likelihood (e.g. > 95% of the overall chi-squared). \n",
        "This is *not* what we want, as instead of using the entire surface brightness profile of the lensed source galaxy to \n",
        "fit our lens model, we end up using only a small subset of its brightest pixels.\n",
        "\n",
        "In the context of the Bayesian log evidence things become even more problematic. The Bayesian log evidence is trying to \n",
        "achieve a well-defined solution; a solution that provides a reduced chi-squared of 1. This solution is poorly defined \n",
        "when the chi-squared image looks like the one above. When a subset of pixels have chi-squareds > 300, the only way \n",
        "to achieve a reduced chi-squared 1 is to reduce the chi-squareds of other pixels to 0, e.g. by over-fitting their \n",
        "noise. Thus, we quickly end up in a regime where the choice of regularization_coefficient is ill defined.\n",
        "\n",
        "With that, we have motivated hyper_galaxy-mode. To put it simply, if we don't adapt our pixelization, regularization\n",
        "and noise-map, we will get solutions which reconstruct the source poorly, regularize the source sub-optimally and \n",
        "over-fit a small sub-set of image pixels. Clearly, we want adaptive _Pixelization_, _Regularization_ and \n",
        "noise-maps, which what we'll cover tutorials 2, 3 and 4!"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}