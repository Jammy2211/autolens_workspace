{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 5: Noise Map Scaling 2\n",
        "===============================\n",
        "\n",
        "Noise-map scaling is important when our mass model lead to an inaccurate source reconstruction . However, it serves an\n",
        "even more important use, when another component of our lens model doesn't fit the data well. Can you think what it is?\n",
        "What could leave significant residuals in our model-fit? What might happen to also be the highest S/N values in our\n",
        "image, meaning these residuals contribute *even more* to the chi-squared distribution?\n",
        "\n",
        "Yep, you guessed it, it's the lens galaxy _LightProfile_ fit and subtraction. Just like our overly simplified mass\n",
        "profile's mean we can't perfectly reconstruct the source's light, the same is true of the Sersic profiles we use to\n",
        "fit the lens galaxy's light. Lets take a look."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import autolens as al\n",
        "import autolens.plot as aplt\n",
        "from pyprojroot import here\n",
        "\n",
        "workspace_path = str(here())\n",
        "print(\"Workspace Path: \", workspace_path)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll use the same strong lensing data as the previous tutorial, where:\n",
        "\n",
        " - The lens galaxy's light is an _EllipticalSersic_.\n",
        " - The lens galaxy's _MassProfile_ is an _EllipticalIsothermal_.\n",
        " - The source galaxy's _LightProfile_ is an _EllipticalSersic_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from howtolens.simulators.chapter_5 import lens_sersic_sie__source_sersic\n",
        "\n",
        "dataset_type = \"chapter_5\"\n",
        "dataset_name = \"lens_sersic_sie__source_sersic\"\n",
        "dataset_path = f\"{workspace_path}/howtolens/dataset/{dataset_type}/{dataset_name}\"\n",
        "\n",
        "imaging = al.Imaging.from_fits(\n",
        "    image_path=f\"{dataset_path}/image.fits\",\n",
        "    noise_map_path=f\"{dataset_path}/noise_map.fits\",\n",
        "    psf_path=f\"{dataset_path}/psf.fits\",\n",
        "    pixel_scales=0.1,\n",
        ")\n",
        "\n",
        "mask = al.Mask.circular(\n",
        "    shape_2d=imaging.shape_2d, pixel_scales=imaging.pixel_scales, sub_size=2, radius=3.0\n",
        ")\n",
        "\n",
        "masked_imaging = al.MaskedImaging(\n",
        "    imaging=imaging,\n",
        "    mask=mask,\n",
        "    settings=al.SettingsMaskedImaging(grid_class=al.Grid, sub_size=2),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, we'll use a convenience function to fit the lens data we simulated above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def fit_masked_imaging_with_lens_and_source_galaxy(\n",
        "    masked_imaging, lens_galaxy, source_galaxy\n",
        "):\n",
        "\n",
        "    tracer = al.Tracer.from_galaxies(galaxies=[lens_galaxy, source_galaxy])\n",
        "\n",
        "    return al.FitImaging(masked_imaging=masked_imaging, tracer=tracer)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, lets use this function to fit the lens data. We'll use a lens model with the correct mass model but an incorrect \n",
        "lens _LightProfile_. The source will use a magnification based grid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lens_galaxy = al.Galaxy(\n",
        "    redshift=0.5,\n",
        "    light=al.lp.EllipticalSersic(\n",
        "        centre=(0.0, 0.0),\n",
        "        elliptical_comps=(0.0, 0.05),\n",
        "        intensity=0.4,\n",
        "        effective_radius=0.8,\n",
        "        sersic_index=3.0,\n",
        "    ),\n",
        "    mass=al.mp.EllipticalIsothermal(\n",
        "        centre=(0.0, 0.0), elliptical_comps=(0.111111, 0.0), einstein_radius=1.6\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n",
        "source_magnification = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    pixelization=al.pix.VoronoiMagnification(shape=(30, 30)),\n",
        "    regularization=al.reg.Constant(coefficient=3.3),\n",
        ")\n",
        "\n",
        "fit = fit_masked_imaging_with_lens_and_source_galaxy(\n",
        "    masked_imaging=masked_imaging,\n",
        "    lens_galaxy=lens_galaxy,\n",
        "    source_galaxy=source_magnification,\n",
        ")\n",
        "\n",
        "print(\"Evidence using baseline variances = \", fit.log_evidence)\n",
        "\n",
        "aplt.FitImaging.subplot_fit_imaging(\n",
        "    fit=fit, include=aplt.Include(inversion_image_pixelization_grid=True, mask=True)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Okay, so its clear that our poor lens light subtraction leaves residuals in the lens galaxy's centre. These pixels \n",
        "are extremely high S/N, so they contribute large chi-squared values. For a real strong lens, we could not fit these \n",
        "residual features using a more complex _LightProfile_. These types of residuals are extremely common and they are \n",
        "caused by nasty, irregular morphological structures in the lens galaxy; nuclear star emission, nuclear rings, bars, etc.\n",
        "\n",
        "This skewed chi-squared distribution will cause all the same problems we discussed in the previous tutorial, like \n",
        "over-fitting. However, for the source-reconstruction and Bayesian log evidence the residuals are even more problematic \n",
        "than before. Why? Because when we compute the Bayesian log evidence for the source-inversion these pixels are included \n",
        "like all the other image pixels. But, __they do not contain the source__. The Bayesian log evidence is going to try \n",
        "improve the fit to these pixels by reducing the level of regularization,  but its __going to fail miserably__, as they \n",
        "map nowhere near the source!\n",
        "\n",
        "This is a fundamental problem when simultaneously modeling the lens galaxy's light and source galaxy. The source \n",
        "inversion has no way to distinguish whether the flux it is reconstructing belongs to the lens or source. This is \n",
        "why contribution maps are so valuable; by creating a contribution map for every galaxy in the image __PyAutoLens__ has a \n",
        "means by which to distinguish which flux belongs to each component in the image! This is further aided by the \n",
        "_Pixelization_'s / regularizations that adapt to the source morphology, as not only are they adapting to where the \n",
        "source __is*__ they adapt to where __it isn't__ (and therefore where the lens galaxy is).\n",
        "\n",
        "Lets now create our hyper-galaxy-images and use them create the contribution maps of our lens and source galaxies. \n",
        "Note below that we now create separate model images for our lens and source galaxies. This allows us to create \n",
        "contribution maps for each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hyper_image = fit.model_image.in_1d\n",
        "\n",
        "hyper_image_lens = fit.model_images_of_planes[0]  # This is the model image of the lens\n",
        "\n",
        "hyper_image_source = fit.model_images_of_planes[\n",
        "    1\n",
        "]  # This is the model image of the source\n",
        "\n",
        "lens_galaxy_hyper = al.Galaxy(\n",
        "    redshift=0.5,\n",
        "    light=al.lp.EllipticalSersic(\n",
        "        centre=(0.0, 0.0),\n",
        "        elliptical_comps=(0.0, 0.05),\n",
        "        intensity=0.4,\n",
        "        effective_radius=0.8,\n",
        "        sersic_index=3.0,\n",
        "    ),\n",
        "    mass=al.mp.EllipticalIsothermal(\n",
        "        centre=(0.0, 0.0), elliptical_comps=(0.111111, 0.0), einstein_radius=1.6\n",
        "    ),\n",
        "    hyper_galaxy=al.HyperGalaxy(\n",
        "        contribution_factor=0.3, noise_factor=4.0, noise_power=1.5\n",
        "    ),\n",
        "    hyper_model_image=hyper_image,\n",
        "    hyper_galaxy_image=hyper_image_lens,  # <- The lens get its own hyper-galaxy image.\n",
        ")\n",
        "\n",
        "source_magnification_hyper = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    pixelization=al.pix.VoronoiMagnification(shape=(30, 30)),\n",
        "    regularization=al.reg.Constant(coefficient=3.3),\n",
        "    hyper_galaxy=al.HyperGalaxy(\n",
        "        contribution_factor=2.0, noise_factor=2.0, noise_power=3.0\n",
        "    ),\n",
        "    hyper_galaxy_image=hyper_image,\n",
        "    hyper_model_image=hyper_image_source,  # <- The source get its own hyper-galaxy image.\n",
        ")\n",
        "\n",
        "fit = fit_masked_imaging_with_lens_and_source_galaxy(\n",
        "    masked_imaging=masked_imaging,\n",
        "    lens_galaxy=lens_galaxy,\n",
        "    source_galaxy=source_magnification,\n",
        ")\n",
        "\n",
        "lens_contribution_map = lens_galaxy_hyper.hyper_galaxy.contribution_map_from_hyper_images(\n",
        "    hyper_model_image=hyper_image, hyper_galaxy_image=hyper_image_lens\n",
        ")\n",
        "\n",
        "aplt.Array(\n",
        "    array=lens_contribution_map,\n",
        "    mask=mask,\n",
        "    plotter=aplt.Plotter(labels=aplt.Labels(title=\"Lens Contribution Map\")),\n",
        ")\n",
        "\n",
        "source_contribution_map = source_magnification_hyper.hyper_galaxy.contribution_map_from_hyper_images(\n",
        "    hyper_model_image=hyper_image, hyper_galaxy_image=hyper_image_source\n",
        ")\n",
        "\n",
        "aplt.Array(\n",
        "    array=source_contribution_map,\n",
        "    mask=mask,\n",
        "    plotter=aplt.Plotter(labels=aplt.Labels(title=\"Source Contribution Map\")),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The contribution maps decomposes the image into its different components. Next, we  use each contribution map to \n",
        "scale different regions of the noise-map. From the fit above it was clear that both the lens and source required the \n",
        "noise to be scaled, but their different chi-squared values ( > 150 and ~ 30) means they require different levels of \n",
        "noise-scaling. Lets see how much our fit improves and Bayesian log evidence increases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit = fit_masked_imaging_with_lens_and_source_galaxy(\n",
        "    masked_imaging=masked_imaging,\n",
        "    lens_galaxy=lens_galaxy_hyper,\n",
        "    source_galaxy=source_magnification_hyper,\n",
        ")\n",
        "\n",
        "aplt.FitImaging.subplot_fit_imaging(\n",
        "    fit=fit, include=aplt.Include(inversion_image_pixelization_grid=True, mask=True)\n",
        ")\n",
        "\n",
        "print(\"Evidence using baseline variances = \", 3356.88)\n",
        "\n",
        "print(\"Evidence using hyper-galaxy hyper variances = \", fit.log_evidence)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great, and with that, we've covered hyper galaxies. You might be wondering, what happens if there are multiple lens \n",
        "galaxies? or multiple source galaxies? Well, as you'd expect, __PyAutoLens__ will make each a hyper-galaxy and \n",
        "therefore scale the noise-map of that individual galaxy in the image. This is what we want, as different parts of \n",
        "the image require different levels of noise-map scaling.\n",
        "\n",
        "Finally, I want to quickly mention two more ways that we change our data during th fitting process. One scales the \n",
        "background noise and one scales the image's background sky. To do this, we use the 'hyper_data' module in __PyAutoLens__."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This module includes all components of the model that scale parts of the data. To scale the background sky in the \n",
        "image we use the HyperImageSky class and input a 'sky_scale'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hyper_image_sky = al.hyper_data.HyperImageSky(sky_scale=1.0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The sky_scale is literally just a constant value we add to every pixel of the observed image before fitting it \n",
        "therefore increasing or decreasing the background sky level in the image .This means we can account for an \n",
        "inaccurate background sky subtraction in our data reduction during __PyAutoLens__ model fitting.\n",
        "\n",
        "We can also scale the background noise in an analogous fashion, using the HyperBackgroundNoise class and the \n",
        "'noise_scale' hyper-galaxy-parameter. This value is added to every pixel in the noise-map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hyper_background_noise = al.hyper_data.HyperBackgroundNoise(noise_scale=1.0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To use these hyper-galaxy-instrument parameters, we pass them to a lens-fit just like we do our _Tracer_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tracer = al.Tracer.from_galaxies(\n",
        "    galaxies=[lens_galaxy_hyper, source_magnification_hyper]\n",
        ")\n",
        "\n",
        "al.FitImaging(\n",
        "    masked_imaging=masked_imaging,\n",
        "    tracer=tracer,\n",
        "    hyper_image_sky=hyper_image_sky,\n",
        "    hyper_background_noise=hyper_background_noise,\n",
        ")\n",
        "\n",
        "aplt.FitImaging.subplot_fit_imaging(\n",
        "    fit=fit, include=aplt.Include(inversion_image_pixelization_grid=True, mask=True)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Is there any reason to scale the background noise other than if the background sky subtraction has a large \n",
        "correction? There is. Lots of pixels in an image do not contain the lensed source but are fitted by the \n",
        "inversion. As we've learnt in this chapter, this isn't problematic when we have our adaptive _Regularization_ scheme \n",
        "because the regularization_coefficient will be increased to large values.\n",
        "\n",
        "However, if you ran a full __PyAutoLens__ analysis in hyper-galaxy-mode (which we cover in the next tutorial), you'd \n",
        "find the method still dedicates a lot of source-pixels to fit these regions of the image, \n",
        "__even though they have no source__. Why is this? Its because although these pixels have no source, they still \n",
        "have a relatively high S/N values (of order 5-10) due to the lens galaxy (e.g. its flux before it is subtracted). The \n",
        "inversion when reconstructing the data 'sees' pixels with a S/N > 1 and therefore wants to fit them with a high \n",
        "resolution.\n",
        "\n",
        "By increasing the background noise these pixels will go to much lower S/N values (<  1). The adaptive _Pixelization_will \n",
        "feel no need to fit them properly and begin to fit these regions of the source-plane with far fewer, much bigger \n",
        "source pixels! This will again give us a net increase in Bayesian log evidence, but more importantly, it will dramatically \n",
        "reduce the number of source pixels we use to fit the data. And what does fewer source-pixels mean? Much, much faster\n",
        "run times. Yay!\n",
        "\n",
        "With that, we have introduced every feature of hyper-galaxy-mode. The only thing left for us to do is to bring it \n",
        "all together and consider how we use all of these features in __PyAutoLens__ pipelines. That is what we'll discuss in the \n",
        "next tutorial, and then you'll be ready to perform your own hyper-galaxy-fits!"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}