{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 2: Brightness Adaption\n",
        "===============================\n",
        "\n",
        "In the previous tutorial we motivated a need to adapt our _Pixelization_to the source's morphology, such that source\n",
        "pixels congregates in the source's brightest regions regardless of where it is located in the source-plane. This\n",
        "raises an interesting question; how do we adapt our source _Pixelization_to the reconstructed source, before we've\n",
        "actually reconstructed the source and therefore know what to adapt it too?\n",
        "\n",
        "To do this, we define a 'hyper-galaxy-image' of the lensed source galaxy. This is a model image of the source\n",
        "computed using a previous lens model fit to the image (e.g. in an earlier phase of a pipeline). This image tells us\n",
        "where in the image our source is located, thus telling us where we need to adapt our source pixelization!\n",
        "\n",
        "So, lets go into the details of how this works. We'll use the same compact source galaxy as the previous tutorial\n",
        "and we'll begin by fitting it with a magnification based _Pixelization_. Why? So we can use its model image to set up\n",
        "the hyper-galaxy-image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import autolens as al\n",
        "import autolens.plot as aplt\n",
        "from pyprojroot import here\n",
        "\n",
        "workspace_path = str(here())\n",
        "print(\"Workspace Path: \", workspace_path)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll use the same strong lensing data as the previous tutorial, where:\n",
        "\n",
        " - The lens galaxy's light is omitted.\n",
        " - The lens galaxy's _MassProfile_ is an _EllipticalIsothermal_.\n",
        " - The source galaxy's _LightProfile_ is an _EllipticalSersic_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from howtolens.simulators.chapter_5 import lens_sie__source_sersic\n",
        "\n",
        "dataset_type = \"chapter_5\"\n",
        "dataset_name = \"lens_sie__source_sersic\"\n",
        "dataset_path = f\"{workspace_path}/howtolens/dataset/{dataset_type}/{dataset_name}\"\n",
        "\n",
        "imaging = al.Imaging.from_fits(\n",
        "    image_path=f\"{dataset_path}/image.fits\",\n",
        "    noise_map_path=f\"{dataset_path}/noise_map.fits\",\n",
        "    psf_path=f\"{dataset_path}/psf.fits\",\n",
        "    pixel_scales=0.1,\n",
        ")\n",
        "\n",
        "mask = al.Mask.circular(\n",
        "    shape_2d=imaging.shape_2d, pixel_scales=imaging.pixel_scales, sub_size=2, radius=3.0\n",
        ")\n",
        "\n",
        "masked_imaging = al.MaskedImaging(\n",
        "    imaging=imaging,\n",
        "    mask=mask,\n",
        "    settings=al.SettingsMaskedImaging(grid_class=al.Grid, sub_size=2),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we're going to fit the image using our magnification based grid. The code below does all the usual steps \n",
        "required to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lens_galaxy = al.Galaxy(\n",
        "    redshift=0.5,\n",
        "    mass=al.mp.EllipticalIsothermal(\n",
        "        centre=(0.0, 0.0), elliptical_comps=(0.111111, 0.0), einstein_radius=1.6\n",
        "    ),\n",
        ")\n",
        "\n",
        "source_galaxy_magnification = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    pixelization=al.pix.VoronoiMagnification(shape=(30, 30)),\n",
        "    regularization=al.reg.Constant(coefficient=3.3),\n",
        ")\n",
        "\n",
        "tracer = al.Tracer.from_galaxies(galaxies=[lens_galaxy, source_galaxy_magnification])\n",
        "\n",
        "fit = al.FitImaging(masked_imaging=masked_imaging, tracer=tracer)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets have a quick look to make sure it has the same residuals we saw in tutorial 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "aplt.FitImaging.subplot_fit_imaging(\n",
        "    fit=fit, include=aplt.Include(inversion_image_pixelization_grid=True, mask=True)\n",
        ")\n",
        "\n",
        "aplt.Inversion.reconstruction(\n",
        "    inversion=fit.inversion, include=aplt.Include(inversion_pixelization_grid=True)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can use this fit to set up our hyper-galaxy-image. This hyper-galaxy-image isn't perfect, as there are \n",
        "residuals in the central regions of the reconstructed source. But it's *okay* and it'll certainly give us enough \n",
        "information on where the lensed source is located to adapt our _Pixelization_.\n",
        "\n",
        "(The 'in_1d_binned' ensures our hyper-image is at the native resolution of our _Imaging_ data, as opposed to a \n",
        "higher resolution sub-grid)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "hyper_image = fit.model_image.in_1d_binned"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets take a look at brightness based adaption in action! Below, we define a source-galaxy using our new \n",
        "_VoronoiBrightnessImage_ _Pixelization_and use this to fit the lens-data. \n",
        "\n",
        "We also attach the hyper_galaxy_image to this galaxy, because the _Pixelization_ uses this hyper_galaxy_image for \n",
        "adaption, the galaxy needs to know what hyper-galaxy-image it uses to adapt its _Pixelization_ too!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "source_galaxy_brightness = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    pixelization=al.pix.VoronoiBrightnessImage(\n",
        "        pixels=500, weight_floor=0.0, weight_power=10.0\n",
        "    ),\n",
        "    regularization=al.reg.Constant(coefficient=0.5),\n",
        "    hyper_galaxy_image=hyper_image,\n",
        "    binned_hyper_galaxy_image=hyper_image,\n",
        ")\n",
        "\n",
        "tracer = al.Tracer.from_galaxies(galaxies=[lens_galaxy, source_galaxy_brightness])\n",
        "\n",
        "fit = al.FitImaging(masked_imaging=masked_imaging, tracer=tracer)\n",
        "\n",
        "aplt.FitImaging.subplot_fit_imaging(\n",
        "    fit=fit, include=aplt.Include(inversion_image_pixelization_grid=True, mask=True)\n",
        ")\n",
        "\n",
        "aplt.Inversion.reconstruction(\n",
        "    inversion=fit.inversion, include=aplt.Include(inversion_pixelization_grid=True)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Would you look at that! Our reconstruction of the image no longer has residuals! By congregating more source \n",
        "pixels in the brightest regions of the source reconstruction we get a better fit. Furthermore, we can check that \n",
        "this provides an increase in Bayesian log evidence, noting that the log evidence of the compact source when using a \n",
        "VoronoiMagnification _Pixelization_was 4216:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Evidence using magnification based _Pixelization_= \", 4216)\n",
        "print(\"Evidence using brightness based _Pixelization_= \", fit.log_evidence)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It increases! By over 200, which, for a Bayesian log evidence, is pretty damn large! By any measure, this \n",
        "_Pixelization_ is a huge success. It turns out that we should have been adapting to the source's brightness all along! \n",
        "In doing so, we will *always* reconstruct the detailed structure of the source's brightest regions with a sufficiently \n",
        "high resolution. Hurrah!\n",
        "\n",
        "So, we are now able to adapt our _Pixelization_to the morphology of our lensed source galaxy. To my knowledge, this \n",
        "is the *best* approach one can take in lens modeling. Its more tricky to implement (as I'll explain next) and \n",
        "introduces extra non-linear parameters. But the pay-off is more than worth it, as we fit our data better and \n",
        "end up using far fewer source pixels to fit the data because we don't 'waste' pixels reconstructing regions of the \n",
        "source-plane where there is no signal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Okay, so how does the hyper_image actually adapt our _Pixelization_to the source's brightness? It uses a 'weighted \n",
        "KMeans clustering algorithm', which is a standard algorithm for partioning data in statistics.\n",
        "\n",
        "In simple terms, this algorithm works as follows:\n",
        "\n",
        " 1) Give the KMeans algorithm a set of weighted data (e.g. determined from the hyper-galaxy image).\n",
        "    \n",
        " 2) For a given number of K-clusters, this algorithm will find a set of (y,x) coordinates that equally partition \n",
        " the weighted data-set. Wherever the data has higher weighting, more clusters congregate and visa versa.\n",
        "    \n",
        " 3) The returned (y,x) 'clusters' then make up our source-pixel centres, where the brightest (e.g. higher weighted \n",
        " regions of the hyper-galaxy-image will have more clusters! Like we did for the magnification based \n",
        " _Pixelization_, we can then trace these coordinates to the source-plane to define our source-pixel _Pixelization_.\n",
        "\n",
        "This is a fairly simplistic description of a KMeans algorithm. Feel free to check out the links below for a more \n",
        "in-depth view:\n",
        "\n",
        " https://en.wikipedia.org/wiki/K-means_clustering\n",
        " https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
        "\n",
        "\n",
        "Okay, so we now have a sense of how our VoronoiBrightnessImage _Pixelization_is computed. Now, lets look at how we \n",
        "create the weighted data the KMeans algorithm uses.\n",
        "\n",
        "This image, called the 'cluster_weight_map' is generated using the 'weight_floor' and 'weight_power' parameters of \n",
        "the VoronoiBrightness _Pixelization_. The cluster weight map is generated following 4 steps:\n",
        "\n",
        " 1) Increase all values of the hyper-galaxy-image that are < 0.02 to 0.02. This is necessary because negative \n",
        " values and zeros break the KMeans clustering algorithm.\n",
        "    \n",
        " 2) Divide all values of this image by its maximum value, such that the hyper-galaxy-image now only contains values \n",
        " between 0.0 and 1.0 (where the values of 1.0 were the maximum values of the hyper-galaxy-image).\n",
        "    \n",
        " 3) Add the weight_floor to all values (a weight_floor of 0.0 therefore does not change the cluster weight map).\n",
        "    \n",
        " 4) Raise all values to the power of weight_power (a weight_power of 1.0 therefore does not change the cluster \n",
        " weight map, whereas a value of 0.0 means all values 1.0 and therefore weighted equally).\n",
        "\n",
        "Lets look at this in action. We'll inspect 3 cluster_weight_maps, using a weight_power of 0.0, 5.0 and 10.0, \n",
        "setting the weight_floor to 0.0 such that it does not change the cluster weight map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "source_weight_power_0 = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    pixelization=al.pix.VoronoiBrightnessImage(\n",
        "        pixels=500, weight_floor=0.0, weight_power=0.0\n",
        "    ),\n",
        "    regularization=al.reg.Constant(coefficient=1.0),\n",
        "    hyper_galaxy_image=hyper_image,\n",
        "    binned_hyper_galaxy_image=hyper_image,\n",
        ")\n",
        "\n",
        "cluster_weight_power_0 = source_weight_power_0.pixelization.weight_map_from_hyper_image(\n",
        "    hyper_image=source_weight_power_0.hyper_galaxy_image\n",
        ")\n",
        "\n",
        "aplt.Array(array=cluster_weight_power_0, mask=mask)\n",
        "\n",
        "source_weight_power_5 = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    pixelization=al.pix.VoronoiBrightnessImage(\n",
        "        pixels=500, weight_floor=0.0, weight_power=5.0\n",
        "    ),\n",
        "    regularization=al.reg.Constant(coefficient=1.0),\n",
        "    hyper_galaxy_image=hyper_image,\n",
        "    binned_hyper_galaxy_image=hyper_image,\n",
        ")\n",
        "\n",
        "cluster_weight_power_5 = source_weight_power_5.pixelization.weight_map_from_hyper_image(\n",
        "    hyper_image=source_weight_power_5.hyper_galaxy_image\n",
        ")\n",
        "\n",
        "aplt.Array(array=cluster_weight_power_5, mask=mask)\n",
        "\n",
        "source_weight_power_10 = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    pixelization=al.pix.VoronoiBrightnessImage(\n",
        "        pixels=500, weight_floor=0.0, weight_power=10.0\n",
        "    ),\n",
        "    regularization=al.reg.Constant(coefficient=1.0),\n",
        "    hyper_galaxy_image=hyper_image,\n",
        "    binned_hyper_galaxy_image=hyper_image,\n",
        ")\n",
        "\n",
        "cluster_weight_power_10 = source_weight_power_10.pixelization.weight_map_from_hyper_image(\n",
        "    hyper_image=source_weight_power_10.hyper_galaxy_image\n",
        ")\n",
        "\n",
        "aplt.Array(array=cluster_weight_power_10, mask=mask)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we increase the weight-power the brightest regions of the hyper-galaxy-image become weighted higher relative \n",
        "to the fainter regions. This means that t e KMeans algorithm will adapt its _Pixelization_to the brightest regions of \n",
        "the source."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tracer = al.Tracer.from_galaxies(galaxies=[lens_galaxy, source_weight_power_0])\n",
        "\n",
        "fit = al.FitImaging(masked_imaging=masked_imaging, tracer=tracer)\n",
        "\n",
        "aplt.Inversion.reconstruction(\n",
        "    inversion=fit.inversion, include=aplt.Include(inversion_pixelization_grid=True)\n",
        ")\n",
        "\n",
        "tracer = al.Tracer.from_galaxies(galaxies=[lens_galaxy, source_weight_power_5])\n",
        "\n",
        "fit = al.FitImaging(masked_imaging=masked_imaging, tracer=tracer)\n",
        "\n",
        "aplt.Inversion.reconstruction(\n",
        "    inversion=fit.inversion, include=aplt.Include(inversion_pixelization_grid=True)\n",
        ")\n",
        "\n",
        "tracer = al.Tracer.from_galaxies(galaxies=[lens_galaxy, source_weight_power_10])\n",
        "\n",
        "fit = al.FitImaging(masked_imaging=masked_imaging, tracer=tracer)\n",
        "\n",
        "aplt.Inversion.reconstruction(\n",
        "    inversion=fit.inversion, include=aplt.Include(inversion_pixelization_grid=True)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, what does the weight_floor do? Increasing the weight-power congregates pixels around the source. However, there \n",
        "is a risk that by congregating too many source pixels in its brightest regions we lose resolution further out, where \n",
        "the source is bright, but not its brightest!\n",
        "\n",
        "The noise-floor allows these regions to maintain a higher weighting whilst the noise_power increases. This means that \n",
        "the _Pixelization_can fully adapt to the source's brightest and faintest regions simultaneously.\n",
        "\n",
        "Lets look at once example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "source_weight_floor = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    pixelization=al.pix.VoronoiBrightnessImage(\n",
        "        pixels=500, weight_floor=0.5, weight_power=10.0\n",
        "    ),\n",
        "    regularization=al.reg.Constant(coefficient=1.0),\n",
        "    hyper_galaxy_image=hyper_image,\n",
        ")\n",
        "\n",
        "cluster_weight_floor = source_weight_floor.pixelization.weight_map_from_hyper_image(\n",
        "    hyper_image=source_weight_floor.hyper_galaxy_image\n",
        ")\n",
        "\n",
        "aplt.Array(array=cluster_weight_floor, mask=mask)\n",
        "\n",
        "tracer = al.Tracer.from_galaxies(galaxies=[lens_galaxy, source_weight_floor])\n",
        "\n",
        "fit = al.FitImaging(masked_imaging=masked_imaging, tracer=tracer)\n",
        "\n",
        "aplt.Inversion.reconstruction(\n",
        "    inversion=fit.inversion, include=aplt.Include(inversion_pixelization_grid=True)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To end, lets think about the Bayesian log evidence which goes to significantly higher values than a magnification-based \n",
        "grid. At this point, it might be worth reminding yourself how the Bayesian log evidence works by going back to the \n",
        "'introduction' text file.\n",
        "\n",
        "So, why do you think why adapting to the source's brightness increases the log evidence?\n",
        "\n",
        "It is because by adapting to the source's morphology we can now access solutions that fit the data really well \n",
        "(e.g. to the Gaussian noise-limit) but use significantly fewer source-pixels than other al. For instance, a typical \n",
        "magnification based _Grid_ uses resolutions of 40 x 40, or 1600 pixels. In contrast, a morphology based _Grid_ typically \n",
        "uses just 300-800 pixels (depending on the source itself). Clearly, the easiest way to make our source solution simpler \n",
        "is to use fewer pixels overall!\n",
        "\n",
        "This provides a second benefit. If the best solutions in our fit want to use the fewest source-pixels possible and \n",
        "__PyAutoLens__ can now access those solutions, this means that hyper-galaxy-mode will run much faster than the magnification \n",
        "based grid! Put simply, fewer source-pixels means lower computational overheads. YAY!\n",
        "\n",
        "Tutorial 2 done, next up, adaptive regularization!"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}