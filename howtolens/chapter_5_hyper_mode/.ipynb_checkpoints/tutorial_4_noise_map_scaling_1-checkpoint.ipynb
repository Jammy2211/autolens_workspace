{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Noise-Map Scaling 1__\n",
    "\n",
    "In tutorial 1, we discussed how when our inversion didn't fit a compact source well we had skewed and undesirable chi-squared distribution. A small subset of the lensed source's brightest pixels were fitted poorly, contributing to the majority of our chi-squared signal. In terms of lens modeling, this meant that we would over-fit these regions of the image. We would prefer that our lens model provides a global fit to the entire lensed source galaxy.\n",
    "\n",
    "With our adaptive pixelization and regularization we are now able to fit the data to the noise-limit and remove this skewed chi-squared distribution. So, why do we need to introduce noise-map scaling? Well, we achieve a good fit when our lens's mass model is accurate (in the previous tutorials we used the *correct* lens mass model). But, what if our  lens mass model isn't accurate? Well, we'll have residuals which will cause the same problem as before; a skewed  chi-squared distribution and an inability to fit the data to the noise level.\n",
    "\n",
    "So, lets simulate an image and fit it with a slightly incorrect mass model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import autolens as al\n",
    "import autolens.plot as aplt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the usual simulate function, using the compact source of the previous tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate():\n",
    "\n",
    "    psf = al.kernel.from_gaussian(shape_2d=(11, 11), sigma=0.05, pixel_scales=0.05)\n",
    "\n",
    "    lens_galaxy = al.Galaxy(\n",
    "        redshift=0.5,\n",
    "        mass=al.mp.EllipticalIsothermal(\n",
    "            centre=(0.0, 0.0), axis_ratio=0.8, phi=45.0, einstein_radius=1.6\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    source_galaxy = al.Galaxy(\n",
    "        redshift=1.0,\n",
    "        light=al.lp.EllipticalSersic(\n",
    "            centre=(0.0, 0.0),\n",
    "            axis_ratio=0.7,\n",
    "            phi=135.0,\n",
    "            intensity=0.2,\n",
    "            effective_radius=0.2,\n",
    "            sersic_index=2.5,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    tracer = al.Tracer.from_galaxies(galaxies=[lens_galaxy, source_galaxy])\n",
    "\n",
    "    simulator = al.simulator.imaging(\n",
    "        shape_2d=(150, 150),\n",
    "        pixel_scales=0.05,\n",
    "        exposure_time=300.0,\n",
    "        sub_size=2,\n",
    "        psf=psf,\n",
    "        background_level=1.0,\n",
    "        add_noise=True,\n",
    "        noise_seed=1,\n",
    "    )\n",
    "\n",
    "    return simulator.from_tracer(tracer=tracer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets simulate the data, draw a 3.0\" mask and set up the lens data that we'll fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imaging = simulate()\n",
    "\n",
    "mask = al.mask.circular(\n",
    "    shape_2d=(150, 150), \n",
    "    pixel_scales=0.05, \n",
    "    sub_size=2, \n",
    "    radius=3.0)\n",
    "\n",
    "masked_imaging = al.masked.imaging(\n",
    "    imaging=imaging, mask=mask, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to fit the image using our magnification based grid. To perform the fit, we'll use a convenience function to fit the lens data we simulated above.\n",
    "\n",
    "In this fitting function, we have changed the lens galaxy's einstein radius to 1.55 from the 'true' simulated value of 1.6. Thus, we are going to fit the data with an *incorrect* mass model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_masked_imaging_with_source_galaxy(masked_imaging, source_galaxy):\n",
    "\n",
    "    lens_galaxy = al.Galaxy(\n",
    "        redshift=0.5,\n",
    "        mass=al.mp.EllipticalIsothermal(\n",
    "            centre=(0.0, 0.0), axis_ratio=0.8, phi=45.0, einstein_radius=1.55\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    tracer = al.Tracer.from_galaxies(galaxies=[lens_galaxy, source_galaxy])\n",
    "\n",
    "    return al.fit(masked_dataset=masked_imaging, tracer=tracer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we'll use the same magnification based source to fit this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_magnification = al.Galaxy(\n",
    "    redshift=1.0,\n",
    "    pixelization=al.pix.VoronoiMagnification(shape=(30, 30)),\n",
    "    regularization=al.reg.Constant(coefficient=3.3),\n",
    ")\n",
    "\n",
    "fit = fit_masked_imaging_with_source_galaxy(\n",
    "    masked_imaging=masked_imaging, source_galaxy=source_magnification\n",
    ")\n",
    "\n",
    "aplt.fit_imaging.subplot_fit_imaging(\n",
    "    fit=fit,\n",
    "    include=aplt.Include(inversion_image_pixelization_grid=True, mask=True)\n",
    ")\n",
    "\n",
    "aplt.inversion.reconstruction(\n",
    "    inversion=fit.inversion,\n",
    "    include=aplt.Include(inversion_pixelization_grid=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit isn't great. The main structure of the lensed source is reconstructed, but there are residuals. These residuals are worse than we saw in the previous tutorials (when source's compact central structure was the problem). So, the obvious question is can our adaptive pixelization and regularization schemes address the problem?\n",
    "\n",
    "Lets find out, using this solution as our hyper-galaxy-image. In this case, our hyper-galaxy-image isn't a perfect fit to the data. This shouldn't be too problematic, as the solution still captures the source's overall structure. The pixelization / regularization hyper-galaxy-parameters have enough flexibility in how they use this image to adapt themselves, so the hyper-galaxy-image doesn't *need* to be perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_image = fit.model_image.in_1d_binned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll note that, unlike before, this source galaxy receives two types of hyper-galaxy-images, a 'hyper_galaxy_image' (like before) and a 'hyper_model_image' (which is new). I'll come back to this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_adaptive = al.Galaxy(\n",
    "    redshift=1.0,\n",
    "    pixelization=al.pix.VoronoiBrightnessImage(\n",
    "        pixels=500, weight_floor=0.0, weight_power=5.0\n",
    "    ),\n",
    "    regularization=al.reg.AdaptiveBrightness(\n",
    "        inner_coefficient=0.001, outer_coefficient=0.2, signal_scale=2.0\n",
    "    ),\n",
    "    hyper_galaxy_image=hyper_image,\n",
    "    hyper_model_image=hyper_image,\n",
    ")\n",
    "\n",
    "fit = fit_masked_imaging_with_source_galaxy(\n",
    "    masked_imaging=masked_imaging, source_galaxy=source_adaptive\n",
    ")\n",
    "\n",
    "aplt.fit_imaging.subplot_fit_imaging(\n",
    "    fit=fit,\n",
    "    include=aplt.Include(inversion_image_pixelization_grid=True, mask=True)\n",
    ")\n",
    "\n",
    "aplt.inversion.reconstruction(\n",
    "    inversion=fit.inversion,\n",
    "    include=aplt.Include(inversion_pixelization_grid=True)\n",
    ")\n",
    "\n",
    "print(\"Evidence = \", fit.evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is better, but far from perfect. Furthermore, this solution maximizes the Bayesian evidence, meaning there is no reasonable way to change our source pixelization or regularization to better fit the data. The problem is with our lens's mass model!\n",
    "\n",
    "This poses a major problem for our model-fitting. A small subset of our data has such large chi-squared values the non-linear search is going to seek solutions which reduce only these chi-squared values. For the image above, a small subset of our data (e.g. < 5% of pixels) contributes to the majority of our likelihood (e.g. > 95% of the overall chi-squared). This is *not* what we want, as it means that instead of using the entire surface brightness profile of the lensed source galaxy to constrain our lens model, we end up using only a small subset of its brightest pixels.\n",
    "\n",
    "This is even more problematic when we try and use the Bayesian evidence to objectively quantify the quality of the fit, as it means it cannot obtain a solution that provides a reduced chi-squared of 1.\n",
    "\n",
    "So, you're probably wondering, why can't we just change the mass model to fit the data better? Surely if we actually modeled this image with PyAutoLens it wouldn't go to this solution anyway but instead infer the correct Einstein radius of 1.6? That's true.\n",
    "\n",
    "However, for *real* strong gravitational lenses, there is no such thing as a 'correct mass model'. Real galaxies are not EllipticalIsothermal profiles, or power-laws, or NFW's, or any of the symmetric and smooth analytic profiles we assume to model their mass. For real strong lenses our mass model will pretty much always lead to source-reconstruction residuals, producing these skewed chi-squared distributions. PyAutoLens can't remove them by simply improving the mass model.\n",
    "\n",
    "This is where noise-map scaling comes in. If we have no alternative, the best way to get Gaussian-distribution (e.g. more uniform) chi-squared fit is to increase the variances of image pixels with high chi-squared values. So, that's what we're going to do, by making our source galaxy a 'hyper-galaxy', a galaxy which use's its hyper-galaxy image to increase the noise in pixels where it has a large signal. Let take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_hyper_galaxy = al.Galaxy(\n",
    "    redshift=1.0,\n",
    "    pixelization=al.pix.VoronoiBrightnessImage(\n",
    "        pixels=500, weight_floor=0.0, weight_power=5.0\n",
    "    ),\n",
    "    regularization=al.reg.AdaptiveBrightness(\n",
    "        inner_coefficient=0.001, outer_coefficient=0.2, signal_scale=2.0\n",
    "    ),\n",
    "    hyper_galaxy=al.HyperGalaxy(\n",
    "        contribution_factor=1.0, noise_factor=1.5, noise_power=1.0\n",
    "    ),\n",
    "    hyper_galaxy_image=hyper_image,\n",
    "    hyper_model_image=hyper_image,\n",
    "    binned_hyper_galaxy_image=hyper_image,\n",
    ")\n",
    "\n",
    "fit = fit_masked_imaging_with_source_galaxy(\n",
    "    masked_imaging=masked_imaging, source_galaxy=source_hyper_galaxy\n",
    ")\n",
    "\n",
    "aplt.fit_imaging.subplot_fit_imaging(\n",
    "    fit=fit,\n",
    "    include=aplt.Include(inversion_image_pixelization_grid=True, mask=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the chi-squared distribution looks *alot* better. The chi-squareds have reduced from the 200's to the 50's, because the variances were increased. This is what we want, so lets make sure we see an appropriate increase i  Bayesian evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evidence using baseline variances = \", 8911.66)\n",
    "\n",
    "print(\"Evidence using variances scaling by hyper-galaxy galaxy = \", fit.evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, a huge increase in the 1000's! Clearly, if our model doesn't fit the data well we *need* to increase the noise wherever the fit is poor to ensure that our use of the Bayesian evidence is well defined.\n",
    "\n",
    "__How does the HyperGalaxy that we attached to the source-galaxy above actually scale the noise?__\n",
    "\n",
    "First, it creates a 'contribution_map' from the hyper-galaxy-image of the lensed source galaxy. This uses the 'hyper_model_image', which is the overall model-image of the best-fit lens model. In this tutorial, because our strong lens imaging only has a source galaxy emitting light, the hyper-galaxy-image of the source galaxy is the same as the hyper_model_image. However, In the next tutorial, we'll introduce the lens galaxy's light, such that each hyper-galaxy galaxy image is different to the hyper-galaxy model image!\n",
    "\n",
    "We compute the contribution map as follows:\n",
    "\n",
    "1) Add the 'contribution_factor' hyper-galaxy-parameter value to the 'hyper_model_image'.\n",
    "\n",
    "2) Divide the 'hyper_galaxy_image' by the hyper-galaxy-model image created in step 1).\n",
    "\n",
    "3) Divide the image created in step 2) by its maximum value, such that all pixels range between 0.0 and 1.0.\n",
    "\n",
    "Lets look at a few contribution maps, generated using hyper-galaxy's with different contribution factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_contribution_factor_1 = al.Galaxy(\n",
    "    redshift=1.0,\n",
    "    hyper_galaxy=al.HyperGalaxy(contribution_factor=1.0),\n",
    "    hyper_galaxy_image=hyper_image,\n",
    "    hyper_model_image=hyper_image,\n",
    "    binned_hyper_galaxy_image=hyper_image,\n",
    ")\n",
    "\n",
    "contribution_map = source_contribution_factor_1.hyper_galaxy.contribution_map_from_hyper_images(\n",
    "    hyper_model_image=hyper_image, hyper_galaxy_image=hyper_image\n",
    ")\n",
    "\n",
    "aplt.array(\n",
    "    array=contribution_map,\n",
    "    mask=mask,\n",
    "    plotter=aplt.Plotter(labels=aplt.Labels(title=\"Contribution Map\"))\n",
    ")\n",
    "\n",
    "source_contribution_factor_3 = al.Galaxy(\n",
    "    redshift=1.0,\n",
    "    hyper_galaxy=al.HyperGalaxy(contribution_factor=3.0),\n",
    "    hyper_galaxy_image=hyper_image,\n",
    "    hyper_model_image=hyper_image,\n",
    "    binned_hyper_galaxy_image=hyper_image,\n",
    ")\n",
    "\n",
    "contribution_map = source_contribution_factor_3.hyper_galaxy.contribution_map_from_hyper_images(\n",
    "    hyper_model_image=hyper_image, hyper_galaxy_image=hyper_image\n",
    ")\n",
    "\n",
    "aplt.array(\n",
    "    array=contribution_map,\n",
    "    mask=mask,\n",
    "    plotter=aplt.Plotter(labels=aplt.Labels(title=\"Contribution Map\"))\n",
    ")\n",
    "\n",
    "source_hyper_galaxy = al.Galaxy(\n",
    "    redshift=1.0,\n",
    "    hyper_galaxy=al.HyperGalaxy(contribution_factor=5.0),\n",
    "    hyper_galaxy_image=hyper_image,\n",
    "    hyper_model_image=hyper_image,\n",
    "    binned_hyper_galaxy_image=hyper_image,\n",
    ")\n",
    "\n",
    "contribution_map = source_hyper_galaxy.hyper_galaxy.contribution_map_from_hyper_images(\n",
    "    hyper_model_image=hyper_image, hyper_galaxy_image=hyper_image\n",
    ")\n",
    "\n",
    "aplt.array(\n",
    "    array=contribution_map,\n",
    "    mask=mask,\n",
    "    plotter=aplt.Plotter(labels=aplt.Labels(title=\"Contribution Map\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By increasing the contribution factor we allocate more pixels with higher contributions (e.g. values closer to 1.0) than pixels with lower values. This is all the contribution_factor does; it scales how we allocate contributions to the source galaxy. Now, we're going to use this contribution map to scale the noise-map, as follows:\n",
    "\n",
    "1) Multiply the baseline (e.g. unscaled) noise-map of the image-data by the contribution map made in step 3) above. This means that only noise-map values where the contribution map has large values (e.g. near 1.0) are going to remain in this image, with the majority of values multiplied by contribution map values near 0.0.\n",
    "\n",
    "2) Raise the noise-map generated in step 1) above to the power of the hyper-galaxy-parameter noise_power. Thus, for large values of noise_power, the largest noise-map values will be increased even more, raising their noise the most.\n",
    "\n",
    "3) Multiply the noise-map values generated in step 2) by the hyper-galaxy-parameter noise_factor. Again, this is a means by which PyAutoLens is able to scale the noise-map values.\n",
    "\n",
    "Lets compare two fits, one where a hyper-galaxy-galaxy scales the noise-map, and one where it doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_no_hyper_galaxy = al.Galaxy(\n",
    "    redshift=1.0,\n",
    "    pixelization=al.pix.VoronoiBrightnessImage(\n",
    "        pixels=500, weight_floor=0.0, weight_power=5.0\n",
    "    ),\n",
    "    regularization=al.reg.AdaptiveBrightness(\n",
    "        inner_coefficient=0.001, outer_coefficient=0.2, signal_scale=2.0\n",
    "    ),\n",
    "    hyper_galaxy_image=hyper_image,\n",
    "    binned_hyper_galaxy_image=hyper_image,\n",
    ")\n",
    "\n",
    "fit = fit_masked_imaging_with_source_galaxy(\n",
    "    masked_imaging=masked_imaging, source_galaxy=source_no_hyper_galaxy\n",
    ")\n",
    "\n",
    "aplt.fit_imaging.subplot_fit_imaging(\n",
    "    fit=fit,\n",
    "    include=aplt.Include(inversion_image_pixelization_grid=True, mask=True)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Evidence using baseline variances = \", fit.evidence)\n",
    "\n",
    "source_hyper_galaxy = al.Galaxy(\n",
    "    redshift=1.0,\n",
    "    pixelization=al.pix.VoronoiBrightnessImage(\n",
    "        pixels=500, weight_floor=0.0, weight_power=5.0\n",
    "    ),\n",
    "    regularization=al.reg.AdaptiveBrightness(\n",
    "        inner_coefficient=0.001, outer_coefficient=0.2, signal_scale=2.0\n",
    "    ),\n",
    "    hyper_galaxy=al.HyperGalaxy(\n",
    "        contribution_factor=1.0, noise_factor=1.5, noise_power=1.0\n",
    "    ),\n",
    "    hyper_galaxy_image=hyper_image,\n",
    "    hyper_model_image=hyper_image,\n",
    ")\n",
    "\n",
    "fit = fit_masked_imaging_with_source_galaxy(\n",
    "    masked_imaging=masked_imaging, source_galaxy=source_hyper_galaxy\n",
    ")\n",
    "\n",
    "aplt.fit_imaging.subplot_fit_imaging(\n",
    "    fit=fit,\n",
    "    include=aplt.Include(inversion_image_pixelization_grid=True, mask=True)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Evidence using variances scaling by hyper-galaxy galaxy = \", fit.evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to play around with the noise_factor and noise_power hyper-galaxy-parameters above. It should be fairly clear what they do; they simply change the amount by which the noise is increased.\n",
    "\n",
    "And with that, we've completed the first of two tutorials on noise-map scaling. To end, I want you to have a quick think, is there anything else that you can think of that would mean we need to scale the noise? In this tutorial, it was the inadequacy of our mass-model that lead to significant residuals and a skewed chi-squared distribution. What else might cause residuals? I'll give you a couple below;\n",
    "\n",
    "1) A mismatch between our model of the imaging data's Point Spread Function (PSF) and the true PSF of the telescope optics of the data.\n",
    "\n",
    "2) Unaccounted for effects in our idata-reduction for the image, in particular the presense of correlated signal and noise during the image's instrument reduction.\n",
    "\n",
    "3) A sub-optimal background sky subtraction of the image, which can leave large levels of signal in the outskirts of the image that are not due to the strong lens system itself.\n",
    "\n",
    "Oh, there's on more thing that can cause much worse residuals than all the effects above. That'll be the topic of the next tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
