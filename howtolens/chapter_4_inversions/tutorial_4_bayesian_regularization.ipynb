{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Bayesian Regularization__\n",
        "\n",
        "So, we can use an inversion to reconstruct an image. Furthermore, this reconstruction provides the 'best-fit'\n",
        "solution. And, when we inspect the fit with the fitting module, we see residuals indicative of a good fit.\n",
        "\n",
        "Everything sounds pretty good, doesn't it? You're probably thinking, why are there more tutorials? We can use\n",
        "inversions now, don't ruin it! Well, there is a problem - which I hid from you in the last tutorial, which we'll\n",
        "cover now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets use the same simple source as last time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def simulate():\n",
        "\n",
        "    grid = al.Grid.uniform(shape_2d=(180, 180), pixel_scales=0.05, sub_size=1)\n",
        "\n",
        "    psf = al.Kernel.from_gaussian(shape_2d=(11, 11), sigma=0.05, pixel_scales=0.05)\n",
        "\n",
        "    lens_galaxy = al.Galaxy(\n",
        "        redshift=0.5,\n",
        "        mass=al.mp.EllipticalIsothermal(\n",
        "            centre=(0.0, 0.0), axis_ratio=0.8, phi=135.0, einstein_radius=1.6\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    source_galaxy_0 = al.Galaxy(\n",
        "        redshift=1.0,\n",
        "        light=al.lp.EllipticalSersic(\n",
        "            centre=(0.1, 0.1),\n",
        "            axis_ratio=0.8,\n",
        "            phi=90.0,\n",
        "            intensity=0.2,\n",
        "            effective_radius=0.3,\n",
        "            sersic_index=1.0,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    tracer = al.Tracer.from_galaxies(galaxies=[lens_galaxy, source_galaxy_0])\n",
        "\n",
        "    simulator = al.SimulatorImaging(\n",
        "        exposure_time_map=al.Array.full(fill_value=300.0, shape_2d=grid.shape_2d),\n",
        "        psf=psf,\n",
        "        background_sky_map=al.Array.full(fill_value=0.1, shape_2d=grid.shape_2d),\n",
        "        add_noise=True,\n",
        "    )\n",
        "\n",
        "    return simulator.from_tracer_and_grid(tracer=tracer, grid=grid)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We're going to perform a lot of fits using an inversion this tutorial. This would create a lot of code, so to keep \n",
        "things tidy, I've setup this function which handles it all for us.\n",
        "\n",
        "(You may notice we include an option to 'use_inversion_border, ignore this for now, as we'll be covering borders in \n",
        "the next tutorial)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def perform_fit_with_source_galaxy(source_galaxy):\n",
        "\n",
        "    imaging = simulate()\n",
        "\n",
        "    mask = al.Mask.circular_annular(\n",
        "        shape_2d=imaging.shape_2d,\n",
        "        pixel_scales=imaging.pixel_scales,\n",
        "        sub_size=2,\n",
        "        inner_radius=0.5,\n",
        "        outer_radius=2.2,\n",
        "    )\n",
        "\n",
        "    masked_imaging = al.MaskedImaging(imaging=imaging, mask=mask)\n",
        "\n",
        "    lens_galaxy = al.Galaxy(\n",
        "        redshift=0.5,\n",
        "        mass=al.mp.EllipticalIsothermal(\n",
        "            centre=(0.0, 0.0), axis_ratio=0.8, phi=135.0, einstein_radius=1.6\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    tracer = al.Tracer.from_galaxies(galaxies=[lens_galaxy, source_galaxy])\n",
        "\n",
        "    return al.FitImaging(masked_imaging=masked_imaging, tracer=tracer)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Okay, so lets look at our fit from the previous tutorial in more detail. We'll use a higher resolution 40 x 40 grid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "source_galaxy = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    pixelization=al.pix.Rectangular(shape=(40, 40)),\n",
        "    regularization=al.reg.Constant(coefficient=1.0),\n",
        ")\n",
        "\n",
        "fit = perform_fit_with_source_galaxy(source_galaxy=source_galaxy)\n",
        "\n",
        "aplt.FitImaging.subplot_fit_imaging(fit=fit)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It looks pretty good! However, this is because I sneakily chose a regularization coefficient that gives a good \n",
        "looking solution. If we reduce this regularization coefficient to zero, our source reconstruction goes weird."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "source_galaxy = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    pixelization=al.pix.Rectangular(shape=(40, 40)),\n",
        "    regularization=al.reg.Constant(coefficient=0.0),\n",
        ")\n",
        "\n",
        "no_regularization_fit = perform_fit_with_source_galaxy(source_galaxy=source_galaxy)\n",
        "\n",
        "aplt.FitImaging.subplot_fit_imaging(\n",
        "    fit=no_regularization_fit, include=aplt.Include(mask=True)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, what's happening here? Why does reducing the regularization do this to our source reconstruction?\n",
        "\n",
        "When our inversion reconstructs a source, it doesn't *just* compute the set of fluxes that best-fit the image. It \n",
        "also 'regularizes' this solution, going to every pixel on our rectangular grid and comparing its reconstructed flux \n",
        "with its 4 neighboring pixels. If the difference in flux is large the solution is penalized, reducing its log likelihood. \n",
        "You can think of this as us applying a prior that our source galaxy solution is 'smooth'.\n",
        "\n",
        "This adds a 'penalty term' to the log likelihood of an inversion which is the summed difference between the \n",
        "reconstructed fluxes of every source-pixel pair multiplied by the regularization coefficient. By setting the \n",
        "regularization coefficient to zero, we set this penalty term to zero, meaning that regularization is omitted.\n",
        "\n",
        "Why do we need to regularize our solution? Well, we just saw why - if we don't apply this smoothing, we 'over-fit' \n",
        "the image. More specifically, we over-fit the noise in the image, which is what the large flux values located at\n",
        " the exteriors of the source reconstruction are doing. Think about it, if your sole aim is to maximize the log likelihood, \n",
        " the best way to do this is to fit *everything* accurately, including the noise.\n",
        "\n",
        "If we change the 'normalization' variables of the plotter such that the color-map is restricted to a narrower range of \n",
        "values, we can see that even without regularization we are still reconstructing the actual source galaxy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "aplt.Inversion.reconstruction(\n",
        "    inversion=no_regularization_fit.inversion,\n",
        "    plotter=aplt.Plotter(cmap=aplt.ColorMap(norm_max=0.5, norm_min=-0.5)),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Over-fitting is why regularization is necessary. Solutions like this completely ruin our attempts to model a strong \n",
        "lens. By smoothing our source reconstruction we ensure it doesn't fit the noise in the image. If we set a really high \n",
        "regularization coefficient we completely remove over-fitting at the expense of also fitting the image less accurately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "source_galaxy = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    pixelization=al.pix.Rectangular(shape=(40, 40)),\n",
        "    regularization=al.reg.Constant(coefficient=100.0),\n",
        ")\n",
        "\n",
        "high_regularization_fit = perform_fit_with_source_galaxy(source_galaxy=source_galaxy)\n",
        "\n",
        "aplt.FitImaging.subplot_fit_imaging(\n",
        "    fit=high_regularization_fit, include=aplt.Include(mask=True)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, we now understand regularization and its purpose. But there is one nagging question that remains, how do I choose \n",
        "the regularization coefficient? We can't use our log_likelihood, as decreasing the regularization coefficient will always \n",
        "increase the log likelihood, because it allows the source reconstruction to fit the data better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Likelihood Without Regularization:\")\n",
        "print(no_regularization_fit.log_likelihood_with_regularization)\n",
        "print(\"Likelihood With Normal Regularization:\")\n",
        "print(fit.log_likelihood_with_regularization)\n",
        "print(\"Likelihood With High Regularization:\")\n",
        "print(high_regularization_fit.log_likelihood_with_regularization)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we used the log likelihood we will always choose a coefficient of 0! We need a different goodness-of-fit measure. For \n",
        "this, we invoke the 'Bayesian log evidence', which quantifies the goodness of the fit as follows:\n",
        "\n",
        "- First, it requires that the residuals of the fit are consistent with Gaussian noise (which is the noise expected \n",
        "in imaging). If this Gaussian pattern is not visible in the residuals, it tells us that the noise must have been \n",
        "over-fitted. Thus, the Bayesian log evidence decreases. Obviously, if the image is poorly fitted, the residuals don't \n",
        "appear Gaussian either, but the poor fit will lead to a decrease in Bayesian log evidence decreases all the same!\n",
        "\n",
        "- This leaves us with a large number of solutions which all fit the data equally well (e.g., to the noise level). \n",
        "To determine the best-fit from these solutions the Bayesian log evidence quantifies the complexity of each solution's \n",
        "source reconstruction. If the inversion requires lots of pixels and a low level of regularization to achieve a good \n",
        "fit, the Bayesian log evidence decreases. It penalizes solutions which are complex, which, in a Bayesian sense, are less \n",
        "probable (you may want to look up 'Occam's Razor').\n",
        "\n",
        "If a really complex source reconstruction is paramount to fitting the image accurately than that is probably the \n",
        "correct solution. However, the Bayesian log evidence ensures we only invoke this more complex solution when the data \n",
        "necessitates it.\n",
        "\n",
        "Lets take a look at the Bayesian log evidence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Bayesian Evidence Without Regularization:\")\n",
        "print(no_regularization_fit.log_evidence)\n",
        "print(\"Bayesian Evidence With Normal Regularization:\")\n",
        "print(fit.log_evidence)\n",
        "print(\"Bayesian Evidence With High Regularization:\")\n",
        "print(high_regularization_fit.log_evidence)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great! As expected, the solution that we could see 'by-eye' was the best solution corresponds to the highesl log evidence \n",
        "solution.\n",
        "\n",
        "Before we end, lets consider which aspects of an inversion are linear and which are non-linear.\n",
        "\n",
        "The linear part of the linear inversion solves for the 'best-fit' solution. For a given regularizaton coefficient, \n",
        "this includes the regularization pattern. That is, we linearly reconstruct the combination of source-pixel fluxes that \n",
        "best-fit the image *including* the penalty term due to comparing neighboring source-pixel fluxes.\n",
        "\n",
        "However, determining the regularization coefficient that maximizes the Bayesian log evidence remains a non-linear problem \n",
        "and this becomes part of our non-linear search. The Bayesian log evidence also depends on the source resolution which \n",
        "means the pixel-grid resolution may also be part of our non-linear search. Nevertheless, this is only 3 parameters - \n",
        "there were 30+ when using light profiles to represent the source!\n",
        "\n",
        "Here are a few questions for you to think about.\n",
        "\n",
        "1) We maximize the log evidence by using simpler source reconstructions. Therefore, decreasing the pixel-grid size should \n",
        "provide a higher log_evidence, provided it still has enough resolution to fit the image well (and provided that the \n",
        "regularization coefficient is still an appropriate value). Can you increase the log evidence from the value above by \n",
        "changing these parameters - I've set you up with a code to do so below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "source_galaxy = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    pixelization=al.pix.Rectangular(shape=(40, 40)),\n",
        "    regularization=al.reg.Constant(coefficient=1.0),\n",
        ")\n",
        "\n",
        "fit = perform_fit_with_source_galaxy(source_galaxy=source_galaxy)\n",
        "\n",
        "print(\"Previous Bayesian Evidence:\")\n",
        "print(10395.370224426646)\n",
        "print(\"New Bayesian Evidence:\")\n",
        "print(fit.log_evidence)\n",
        "\n",
        "aplt.FitImaging.subplot_fit_imaging(fit=fit, include=aplt.Include(mask=True))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2) Can you think of any other ways we might increase the log evidence even further? If not - don't worry about - but \n",
        "you'll learn that PyAutoLens actually adapts its source reconstructions to the properties of the image that it is \n",
        "fitting, so as to objectively maximize the log evidence!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}