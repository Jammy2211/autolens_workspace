{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 4: Bayesian Regularization\n",
        "===================================\n",
        "\n",
        "So, we can use an `Inversion` to reconstruct an image. Furthermore, this reconstruction provides the maximum log\n",
        "likelihood solution. And, when we inspect the fit, we see residuals indicative of a good fit.\n",
        "\n",
        "Everything sounds pretty good, doesn`t it? You`re probably thinking, why are there more tutorials? We can use\n",
        "`Inversion`s now, don't ruin it! Well, there is a problem - which I hid from you in the last tutorial, which we'll\n",
        "cover now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from pyprojroot import here\n",
        "\n",
        "workspace_path = str(here())\n",
        "%cd $workspace_path\n",
        "print(f\"Working Directory has been set to `{workspace_path}`\")\n",
        "\n",
        "from os import path\n",
        "import autolens as al\n",
        "import autolens.plot as aplt\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "we'll use the same strong lensing data as the previous tutorial, where:\n",
        "\n",
        " - The lens `Galaxy`'s light is omitted.\n",
        " - The lens `Galaxy`'s total mass distribution is an `EllipticalIsothermal`.\n",
        " - The source `Galaxy`'s `LightProfile` is an `EllipticalSersic`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name = \"mass_sie__source_sersic\"\n",
        "dataset_path = path.join(\"dataset\", \"howtolens\", \"chapter_4\", dataset_name)\n",
        "\n",
        "imaging = al.Imaging.from_fits(\n",
        "    image_path=path.join(dataset_path, \"image.fits\"),\n",
        "    noise_map_path=path.join(dataset_path, \"noise_map.fits\"),\n",
        "    psf_path=path.join(dataset_path, \"psf.fits\"),\n",
        "    pixel_scales=0.1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "we're going to perform a lot of fits using an `Inversion` this tutorial. This would create a lot of code, so to keep \n",
        "things tidy, I've setup this function which handles it all for us.\n",
        "\n",
        "(You may notice we include an option to `use_inversion_border, ignore this for now, as we'll be covering borders in \n",
        "the next tutorial)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def perform_fit_with_source_galaxy(imaging, source_galaxy):\n",
        "\n",
        "    mask = al.Mask2D.circular_annular(\n",
        "        shape_2d=imaging.shape_2d,\n",
        "        pixel_scales=imaging.pixel_scales,\n",
        "        sub_size=2,\n",
        "        inner_radius=0.3,\n",
        "        outer_radius=2.6,\n",
        "    )\n",
        "\n",
        "    masked_imaging = al.MaskedImaging(\n",
        "        imaging=imaging, mask=mask, settings=al.SettingsMaskedImaging(sub_size=2)\n",
        "    )\n",
        "\n",
        "    lens_galaxy = al.Galaxy(\n",
        "        redshift=0.5,\n",
        "        mass=al.mp.EllipticalIsothermal(\n",
        "            centre=(0.0, 0.0), elliptical_comps=(0.1, 0.0), einstein_radius=1.6\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    tracer = al.Tracer.from_galaxies(galaxies=[lens_galaxy, source_galaxy])\n",
        "\n",
        "    return al.FitImaging(masked_imaging=masked_imaging, tracer=tracer)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Okay, so lets look at our fit from the previous tutorial in more detail. we'll use a higher resolution 40 x 40 grid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "source_galaxy = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    pixelization=al.pix.Rectangular(shape=(40, 40)),\n",
        "    regularization=al.reg.Constant(coefficient=1.0),\n",
        ")\n",
        "\n",
        "fit = perform_fit_with_source_galaxy(imaging=imaging, source_galaxy=source_galaxy)\n",
        "\n",
        "aplt.FitImaging.subplot_fit_imaging(fit=fit)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It looks pretty good! However, this is because I sneakily chose a regularization_coefficient that gives a good \n",
        "looking solution. If we reduce this regularization_coefficient to zero, our source reconstruction goes weird."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "source_galaxy = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    pixelization=al.pix.Rectangular(shape=(40, 40)),\n",
        "    regularization=al.reg.Constant(coefficient=0.0),\n",
        ")\n",
        "\n",
        "no_regularization_fit = perform_fit_with_source_galaxy(\n",
        "    imaging=imaging, source_galaxy=source_galaxy\n",
        ")\n",
        "\n",
        "aplt.FitImaging.subplot_fit_imaging(\n",
        "    fit=no_regularization_fit, include=aplt.Include(mask=True)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, what`s happening here? Why does reducing the `Regularization` do this to our source reconstruction?\n",
        "\n",
        "When our `Inversion` reconstructs a source, it doesn`t *just* compute the set of fluxes that best-fit the image. It \n",
        "also `regularizes` this solution, going to every pixel on our rectangular `Grid` and comparing its reconstructed flux \n",
        "with its 4 neighboring pixels. If the difference in flux is large the solution is penalized, reducing its log \n",
        "likelihood. You can think of this as us applying a prior that our source galaxy solution is `smooth`.\n",
        "\n",
        "This adds a `penalty term` to the log likelihood of an `Inversion` which is the summed difference between the \n",
        "reconstructed fluxes of every source-pixel pair multiplied by the regularization_coefficient. By setting the \n",
        "regularization coefficient to zero, we set this penalty term to zero, meaning that `Regularization`.s omitted.\n",
        "\n",
        "Why do we need to regularize our solution? Well, we just saw why - if we don't apply this smoothing, we `over-fit` \n",
        "the image. More specifically, we over-fit the noise in the image, which is what the large flux values located at\n",
        "the exteriors of the source reconstruction are doing. Think about it, if your sole aim is to maximize the log \n",
        "likelihood, the best way to do this is to fit *everything* accurately, including the noise.\n",
        "\n",
        "If we change the `normalization` variables of the `Plotter` such that the color-map is restricted to a narrower range of \n",
        "values, we can see that even without `Regularization`.e are still reconstructing the actual source galaxy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "aplt.Inversion.reconstruction(\n",
        "    inversion=no_regularization_fit.inversion,\n",
        "    plotter=aplt.Plotter(cmap=aplt.ColorMap(norm_max=0.5, norm_min=-0.5)),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Over-fitting is why `Regularization`.s necessary. Solutions like this completely ruin our attempts to model a strong \n",
        "lens. By smoothing our source reconstruction we ensure it doesn`t fit the noise in the image. If we set a really high \n",
        "regularization coefficient we completely remove over-fitting at the expense of also fitting the image less accurately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "source_galaxy = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    pixelization=al.pix.Rectangular(shape=(40, 40)),\n",
        "    regularization=al.reg.Constant(coefficient=100.0),\n",
        ")\n",
        "\n",
        "high_regularization_fit = perform_fit_with_source_galaxy(\n",
        "    imaging=imaging, source_galaxy=source_galaxy\n",
        ")\n",
        "\n",
        "aplt.FitImaging.subplot_fit_imaging(\n",
        "    fit=high_regularization_fit, include=aplt.Include(mask=True)\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, we now understand `Regularization`.nd its purpose. But there is one nagging question that remains, how do I choose \n",
        "the regularization_coefficient? We can`t use our log_likelihood, as decreasing the regularization_coefficient will \n",
        "always increase the log likelihood, because it allows the source reconstruction to fit the data better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Likelihood Without Regularization:\")\n",
        "print(no_regularization_fit.log_likelihood_with_regularization)\n",
        "print(\"Likelihood With Normal Regularization:\")\n",
        "print(fit.log_likelihood_with_regularization)\n",
        "print(\"Likelihood With High Regularization:\")\n",
        "print(high_regularization_fit.log_likelihood_with_regularization)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we used the log likelihood we will always choose a coefficient of 0! We need a different goodness-of-fit measure. \n",
        "For this, we invoke the `Bayesian Evidence`, which quantifies the goodness of the fit as follows:\n",
        "\n",
        " - First, it requires that the residuals of the fit are consistent with Gaussian noise (which is the noise expected \n",
        " in the `Imaging`.. If this Gaussian pattern is not visible in the residuals, the noise must have been over-fitted. \n",
        " Thus, the Bayesian log evidence decreases. Obviously, if the image is poorly fitted, the residuals don't appear \n",
        " Gaussian either, but the poor fit will lead to a decrease in Bayesian log evidence decreases all the same!\n",
        "\n",
        " - This leaves us with a large number of solutions which all fit the data equally well (e.g., to the noise level). \n",
        " To determine the best-fit from these solutions the Bayesian log evidence quantifies the complexity of each \n",
        " solution`s source reconstruction. If the `Inversion` requires lots of pixels and a low level of _Regularization_\n",
        " to achieve a good fit, the Bayesian log evidence decreases. It penalizes solutions which are complex, which, in \n",
        " a Bayesian sense, are less probable (you may want to look up `Occam`s Razor`).\n",
        "\n",
        "If a really complex source reconstruction is paramount to fitting the image accurately than that is probably the \n",
        "correct solution. However, the Bayesian log evidence ensures we only invoke this more complex solution when the data \n",
        "necessitates it.\n",
        "\n",
        "Lets take a look at the Bayesian log evidence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Bayesian Evidence Without Regularization:\")\n",
        "print(no_regularization_fit.log_evidence)\n",
        "print(\"Bayesian Evidence With Normal Regularization:\")\n",
        "print(fit.log_evidence)\n",
        "print(\"Bayesian Evidence With High Regularization:\")\n",
        "print(high_regularization_fit.log_evidence)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Great! As expected, the solution that we could see `by-eye` was the best solution corresponds to the highest log \n",
        "evidence solution.\n",
        "\n",
        "Before we end, lets consider which aspects of an `Inversion` are linear and which are non-linear.\n",
        "\n",
        "The linear part of the linear `Inversion` solves for the `best-fit` solution. For a given regularizaton coefficient, \n",
        "this includes the `Regularization` pattern. That is, we linearly reconstruct the combination of source-pixel fluxes \n",
        "that best-fit the image *including* the penalty term due to comparing neighboring source-pixel fluxes.\n",
        "\n",
        "However, determining the regularization_coefficient that maximizes the Bayesian log evidence remains a non-linear \n",
        "problem and this becomes part of our non-linear search. The Bayesian log evidence also depends on the source resolution \n",
        "which means the pixel-grid resolution may also be part of our non-linear search. Nevertheless, this is only 3 \n",
        "parameters - there were 30+ when using `LightProfile`'s to represent the source!\n",
        "\n",
        "Here are a few questions for you to think about.\n",
        "\n",
        " 1) We maximize the log evidence by using simpler source reconstructions. Therefore, decreasing the pixel-grid \n",
        " size should provide a higher log_evidence, provided it still has enough resolution to fit the image well (and \n",
        " provided that the `Regularization` coefficient is still an appropriate value). Can you increase the log evidence \n",
        " from the value above by changing these parameters - I've set you up with a code to do so below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "source_galaxy = al.Galaxy(\n",
        "    redshift=1.0,\n",
        "    pixelization=al.pix.Rectangular(shape=(40, 40)),\n",
        "    regularization=al.reg.Constant(coefficient=1.0),\n",
        ")\n",
        "\n",
        "fit = perform_fit_with_source_galaxy(imaging=imaging, source_galaxy=source_galaxy)\n",
        "\n",
        "print(\"Previous Bayesian Evidence:\")\n",
        "print(10395.370224426646)\n",
        "print(\"New Bayesian Evidence:\")\n",
        "print(fit.log_evidence)\n",
        "\n",
        "aplt.FitImaging.subplot_fit_imaging(fit=fit, include=aplt.Include(mask=True))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " 2) Can you think of any other ways we might increase the log evidence even further? If not - don't worry. but \n",
        " you'll learn that **PyAutoLens** actually adapts its source reconstructions to the properties of the image that it is \n",
        " fitting, so as to objectively maximize the log evidence!"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}