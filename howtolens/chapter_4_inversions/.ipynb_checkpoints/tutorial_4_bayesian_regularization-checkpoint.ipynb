{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bayesian Regularization__\n",
    "\n",
    "So, we can use an inversion to reconstruct an image. Furthermore, this reconstruction provides the 'best-fit' solution. And, when we inspect the fit with the fitting module, we see residuals indicative of a good fit.\n",
    "\n",
    "Everything sounds pretty good, doesn't it? You're probably thinking, why are there more tutorials? We can use inversions now, don't ruin it! Well, there is a problem - which I hid from you in the last tutorial, which we'll cover now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import autolens as al\n",
    "import autolens.plot as aplt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use the same simple source as last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate():\n",
    "\n",
    "    psf = al.kernel.from_gaussian(shape_2d=(11, 11), sigma=0.05, pixel_scales=0.05)\n",
    "\n",
    "    grid = al.grid.uniform(\n",
    "        shape_2d=(180, 180), pixel_scales=0.05\n",
    "    )\n",
    "\n",
    "    lens_galaxy = al.Galaxy(\n",
    "        redshift=0.5,\n",
    "        mass=al.mp.EllipticalIsothermal(\n",
    "            centre=(0.0, 0.0), axis_ratio=0.8, phi=135.0, einstein_radius=1.6\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    source_galaxy_0 = al.Galaxy(\n",
    "        redshift=1.0,\n",
    "        light=al.lp.EllipticalSersic(\n",
    "            centre=(0.1, 0.1),\n",
    "            axis_ratio=0.8,\n",
    "            phi=90.0,\n",
    "            intensity=0.2,\n",
    "            effective_radius=0.3,\n",
    "            sersic_index=1.0,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    tracer = al.Tracer.from_galaxies(\n",
    "        galaxies=[lens_galaxy, source_galaxy_0],\n",
    "    )\n",
    "\n",
    "    simulator = al.simulator.imaging(\n",
    "        shape_2d=(180, 180),\n",
    "        pixel_scales=0.05,\n",
    "        exposure_time=300.0,\n",
    "        sub_size=1,\n",
    "        psf=psf,\n",
    "        background_level=0.1,\n",
    "        add_noise=True,\n",
    "    )\n",
    "\n",
    "    return simulator.from_tracer(tracer=tracer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to perform a lot of fits using an inversion this tutorial. This would create a lot of code, so to keep things tidy, I've setup this function which handles it all for us.\n",
    "\n",
    "(You may notice we include an option to 'use_inversion_border, ignore this for now, as we'll be covering borders in the next tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_fit_with_source_galaxy(source_galaxy):\n",
    "\n",
    "    imaging = simulate()\n",
    "\n",
    "    mask = al.mask.circular_annular(\n",
    "        shape_2d=imaging.shape_2d,\n",
    "        pixel_scales=imaging.pixel_scales,\n",
    "        sub_size=2,\n",
    "        inner_radius=0.5,\n",
    "        outer_radius=2.2,\n",
    "    )\n",
    "\n",
    "    masked_imaging = al.masked.imaging(imaging=imaging, mask=mask)\n",
    "\n",
    "    lens_galaxy = al.Galaxy(\n",
    "        redshift=0.5,\n",
    "        mass=al.mp.EllipticalIsothermal(\n",
    "            centre=(0.0, 0.0), axis_ratio=0.8, phi=135.0, einstein_radius=1.6\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    tracer = al.Tracer.from_galaxies(galaxies=[lens_galaxy, source_galaxy])\n",
    "\n",
    "    return al.fit(masked_dataset=masked_imaging, tracer=tracer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so lets look at our fit from the previous tutorial in more detail. We'll use a higher resolution 40 x 40 grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_galaxy = al.Galaxy(\n",
    "    redshift=1.0,\n",
    "    pixelization=al.pix.Rectangular(shape=(40, 40)),\n",
    "    regularization=al.reg.Constant(coefficient=1.0),\n",
    ")\n",
    "\n",
    "fit = perform_fit_with_source_galaxy(source_galaxy=source_galaxy)\n",
    "\n",
    "aplt.fit_imaging.subplot_fit_imaging(fit=fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks pretty good! However, this is because I sneakily chose a regularization coefficient that gives a good looking solution. If we reduce this regularization coefficient to zero, our source reconstruction goes weird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_galaxy = al.Galaxy(\n",
    "    redshift=1.0,\n",
    "    pixelization=al.pix.Rectangular(shape=(40, 40)),\n",
    "    regularization=al.reg.Constant(coefficient=0.0),\n",
    ")\n",
    "\n",
    "no_regularization_fit = perform_fit_with_source_galaxy(source_galaxy=source_galaxy)\n",
    "\n",
    "aplt.fit_imaging.subplot_fit_imaging(\n",
    "    fit=no_regularization_fit,\n",
    "    include=aplt.Include(mask=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what's happening here? Why does reducing the regularization do this to our source reconstruction?\n",
    "\n",
    "When our inversion reconstructs a source, it doesn't *just* compute the set of fluxes that best-fit the image. It also 'regularizes' this solution, going to every pixel on our rectangular grid and comparing its reconstructed flux with its 4 neighboring pixels. If the difference in flux is large the solution is penalized, reducing its likelihood. You can think of this as us applying a prior that our source galaxy solution is 'smooth'.\n",
    "\n",
    "This adds a 'penalty term' to the likelihood of an inversion which is the summed difference between the reconstructed fluxes of every source-pixel pair multiplied by the regularization coefficient. By setting the regularization coefficient to zero, we set this penalty term to zero, meaning that regularization is omitted.\n",
    "\n",
    "Why do we need to regularize our solution? Well, we just saw why - if we don't apply this smoothing, we 'over-fit' the image. More specifically, we over-fit the noise in the image, which is what the large flux values located at the exteriors of the source reconstruction are doing. Think about it, if your sole aim is to maximize the likelihood, the best way to do this is to fit *everything* accurately, including the noise.\n",
    "\n",
    "If we change the 'normalization' variables of the plotter such that the color-map is restricted to a narrower range of values, we can see that even without regularization we are still reconstructing the actual source galaxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aplt.inversion.reconstruction(\n",
    "    inversion=no_regularization_fit.inversion, \n",
    "    plotter=aplt.Plotter(cmap=aplt.ColorMap(norm_max=0.5, norm_min=-0.5)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over-fitting is why regularization is necessary. Solutions like this completely ruin our attempts to model a strong lens. By smoothing our source reconstruction we ensure it doesn't fit the noise in the image. If we set a really high regularization coefficient we completely remove over-fitting at the expense of also fitting the image less accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_galaxy = al.Galaxy(\n",
    "    redshift=1.0,\n",
    "    pixelization=al.pix.Rectangular(shape=(40, 40)),\n",
    "    regularization=al.reg.Constant(coefficient=100.0),\n",
    ")\n",
    "\n",
    "high_regularization_fit = perform_fit_with_source_galaxy(source_galaxy=source_galaxy)\n",
    "\n",
    "aplt.fit_imaging.subplot_fit_imaging(\n",
    "    fit=high_regularization_fit, \n",
    "    include=aplt.Include(mask=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we now understand regularization and its purpose. But there is one nagging question that remains, how do I choose the regularization coefficient? We can't use our likelihood, as decreasing the regularization coefficient will always increase the likelihood, because it allows the source reconstruction to fit the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Likelihood Without Regularization:\")\n",
    "print(no_regularization_fit.likelihood_with_regularization)\n",
    "print(\"Likelihood With Normal Regularization:\")\n",
    "print(fit.likelihood_with_regularization)\n",
    "print(\"Likelihood With High Regularization:\")\n",
    "print(high_regularization_fit.likelihood_with_regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we used the likelihood we will always choose a coefficient of 0! We need a different goodness-of-fit measure. For this, we invoke the 'Bayesian evidence', which quantifies the goodness of the fit as follows:\n",
    "\n",
    "- First, it requires that the residuals of the fit are consistent with Gaussian noise (which is the noise expected in imaging). If this Gaussian pattern is not visible in the residuals, it tells us that the noise must have been over-fitted. Thus, the Bayesian evidence decreases. Obviously, if the image is poorly fitted, the residuals don't appear Gaussian either, but the poor fit will lead to a decrease in Bayesian evidence decreases all the same!\n",
    "\n",
    "- This leaves us with a large number of solutions which all fit the data equally well (e.g., to the noise level). To determine the best-fit from these solutions the Bayesian evidence quantifies the complexity of each solution's source reconstruction. If the inversion requires lots of pixels and a low level of regularization to achieve a good fit, the Bayesian evidence decreases. It penalizes solutions which are complex, which, in a Bayesian sense, are less probable (you may want to look up 'Occam's Razor').\n",
    "\n",
    "If a really complex source reconstruction is paramount to fitting the image accurately than that is probably the correct solution. However, the Bayesian evidence ensures we only invoke this more complex solution when the data necessitates it.\n",
    "\n",
    "Lets take a look at the Bayesian evidence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bayesian Evidence Without Regularization:\")\n",
    "print(no_regularization_fit.evidence)\n",
    "print(\"Bayesian Evidence With Normal Regularization:\")\n",
    "print(fit.evidence)\n",
    "print(\"Bayesian Evidence With High Regularization:\")\n",
    "print(high_regularization_fit.evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! As expected, the solution that we could see 'by-eye' was the best solution corresponds to the highest evidence solution.\n",
    "\n",
    "Before we end, lets consider which aspects of an inversion are linear and which are non-linear.\n",
    "\n",
    "The linear part of the linear inversion solves for the 'best-fit' solution. For a given regularizaton coefficient, this includes the regularization pattern. That is, we linearly reconstruct the combination of source-pixel fluxes that best-fit the image *including* the penalty term due to comparing neighboring source-pixel fluxes.\n",
    "\n",
    "However, determining the regularization coefficient that maximizes the Bayesian evidence remains a non-linear problem and this becomes part of our non-linear search. The Bayesian evidence also depends on the source resolution which means the pixel-grid resolution may also be part of our non-linear search. Nevertheless, this is only 3 parameters - there were 30+ when using light profiles to represent the source!\n",
    "\n",
    "Here are a few questions for you to think about.\n",
    "\n",
    "1) We maximize the evidence by using simpler source reconstructions. Therefore, decreasing the pixel-grid size should provide a higher evidence, provided it still has enough resolution to fit the image well (and provided that the regularization coefficient is still an appropriate value). Can you increase the evidence from the value above by changing these parameters - I've set you up with a code to do so below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_galaxy = al.Galaxy(\n",
    "    redshift=1.0,\n",
    "    pixelization=al.pix.Rectangular(shape=(40, 40)),\n",
    "    regularization=al.reg.Constant(coefficient=1.0),\n",
    ")\n",
    "\n",
    "fit = perform_fit_with_source_galaxy(source_galaxy=source_galaxy)\n",
    "\n",
    "print(\"Previous Bayesian Evidence:\")\n",
    "print(10395.370224426646)\n",
    "print(\"New Bayesian Evidence:\")\n",
    "print(fit.evidence)\n",
    "\n",
    "aplt.fit_imaging.subplot_fit_imaging(\n",
    "    fit=fit, \n",
    "    include=aplt.Include(mask=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Can you think of any other ways we might increase the evidence even further? If not - don't worry about - but you'll learn that PyAutoLens actually adapts its source reconstructions to the properties of the image that it is fitting, so as to objectively maximize the evidence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
