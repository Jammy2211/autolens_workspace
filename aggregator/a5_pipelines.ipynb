{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Aggregator 4: Advanced__\n",
        "\n",
        "The previous tutorials used beginner pipelines, whose results are distributed in one folder. Advanced pipelines results\n",
        "are output in a more complex path structure with multiple pipelines. Furthemore, advanced pipelines allow us to fit\n",
        "many different lens model parameterizations with the results output in strucuted paths depending on the phase and\n",
        "pipeline tags. This tutorial explains how to use the aggregator for such complex outputs.\n",
        "\n",
        "In `/autolens_workspace/aggregator/setup/advanced_runner.py` we fit our 3 images with the following 3 pipelines:\n",
        "\n",
        "`autolens_workspace/pipelines/advanced/no_lens_light/source/parametric/mass_sie__source_parametric.py`\n",
        "`autolens_workspace/pipelines/advanced/no_lens_light/source/inversion/from_parametric/mass_sie__source_inversion.py`\n",
        "`autolens_workspace/pipelines/advanced/no_lens_light/mass/power_law/mass_power_law__source_inversion.py`\n",
        "\n",
        "Each set of 3 images is fitted 4 separate times, with the following variants:\n",
        "\n",
        "- With the General setup hyper_galaxies=False and with with_shear=True.\n",
        "- With the General setup hyper_galaxies=True and with with_shear=True.\n",
        "- With the General setup hyper_galaxies=False and with with_shear=False.\n",
        "- With the General setup hyper_galaxies=True and with with_shear=False.\n",
        "\n",
        "The results of these fits are in the `/output/aggregator_sample_advanced` folder. Pipeline tagging has lead to many\n",
        "different results in a complex path structure that depends on the setup of the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from autoconf import conf\n",
        "import autofit as af\n",
        "import autolens as al\n",
        "import autolens.plot as aplt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below, we set up the aggregator as we did in the previous tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "workspace_path = \"/Users/Jammy/Code/PyAuto/autolens_workspace\"\n",
        "output_path = f\"{workspace_path}/output\"\n",
        "agg_results_path = f\"{output_path}/aggregator/advanced\"\n",
        "\n",
        "conf.instance = conf.Config(\n",
        "    config_path=f\"{workspace_path}/config\", output_path=output_path\n",
        ")\n",
        "\n",
        "agg = af.Aggregator(directory=str(agg_results_path))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we only fitted the 3 images using one pipeline. But suppose we used multiple pipelines, like we do in \n",
        "the advanced pipelines. In this case, the aggregator would load the NestSampless of all fits of all phases of all \n",
        "pipelines!\n",
        "\n",
        "In such circumstances, we can filter by phase name and pipeline name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pipeline_name = \"pipeline__mass_sie__source_inversion\"\n",
        "agg_filter = agg.filter(agg.phase == name, agg.pipeline == pipeline_name)\n",
        "samples_gen = agg_filter.values(\"samples\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, this list again has 3 NestSampless."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Pipeline Name Filtered MultiNest Samples: \\n\")\n",
        "print(samples_gen)\n",
        "print()\n",
        "print(\"Ttotal Samples Objects = \", len(list(agg_filter.values(\"samples\"))), \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pipeline_name = \"pipeline_mass__power_law\"\n",
        "name = \"phase[1]__lens_power_law__source\"\n",
        "\n",
        "agg_power_law = agg.filter(agg.phase == name, agg.pipeline == pipeline_name)\n",
        "\n",
        "print(\"Pipeline Name Filtered MultiNest Samples:\")\n",
        "print(list(agg_power_law.values(\"samples\")))\n",
        "print(\"Ttotal Samples Objects = \", len(list(agg_power_law.values(\"samples\"))), \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This gives 12 results, given that we fitted each of our 3 images 4 times using different pipeline settings.\n",
        "\n",
        "Lets say we want only the fits that used the hyper galaxies functionality and included a shear. To get these results, \n",
        "we require a new filtering method based on the pipeline and phase tags of a given set of results. For this, we can \n",
        "filter based on the full path of a set of results, filtering for results that contain an input string. \n",
        "\n",
        "As usual, filtering creates a new aggregator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# This gives the 6 results with hyper galaxy fitting switch on.\n",
        "agg_power_law_hyper_shear = agg_power_law.filter(\n",
        "    agg_power_law.directory.contains(\"hyper_galaxies\")\n",
        ")\n",
        "\n",
        "# This gives the 3 results from the 6 above that include a shear.\n",
        "agg_power_law_hyper_shear = agg_power_law_hyper_shear.filter(\n",
        "    agg_power_law_hyper_shear.directory.contains(\"with_shear\")\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This aggregator can now be used, as usual, to make plots of quantities like the fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fit_gen = al.agg.FitImaging(aggregator=agg_power_law_hyper_shear)\n",
        "\n",
        "for fit in fit_gen:\n",
        "    aplt.FitImaging.subplot_fit_imaging(fit=fit)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "          \n",
        "When there are many results in one directory, the best way to handle this is to create multiple aggregators using the\n",
        "filter method above. Below, we create aggregators containing the results of fits to all 3 images which we then plot \n",
        "so we can compare the different fits.\n",
        "\n",
        "Runs without hyper-galaxies or shear turned on do not tag the results with text. To get these results via path \n",
        "filtering we have to input the whole pipeline or phase tag, including the `/` to mark the end of the tag in the path \n",
        "string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg_tmp = agg_power_law.filter(agg.directory.contains(\"general/\"))\n",
        "agg_power_law_with_shear = agg_tmp.filter(agg_tmp.directory.contains(\"with_shear\"))\n",
        "\n",
        "agg_tmp = agg_power_law.filter(agg.directory.contains(\"general/\"))\n",
        "agg_power_law_shear = agg_tmp.filter(agg_power_law.directory.contains(\"with_shear\"))\n",
        "\n",
        "agg_tmp = agg_power_law.filter(agg_power_law.directory.contains(\"hyper_galaxies\"))\n",
        "agg_power_law_hyper_with_shear = agg_tmp.filter(\n",
        "    agg_tmp.directory.contains(\"source__pix_voro_mag__reg_const/\")\n",
        ")\n",
        "\n",
        "agg_tmp = agg_power_law.filter(agg_power_law.directory.contains(\"hyper_galaxies\"))\n",
        "agg_power_law_hyper_shear = agg_tmp.filter(\n",
        "    agg_tmp.directory.contains(\"source__pix_voro_mag__reg_const/\")\n",
        ")\n",
        "\n",
        "fit_gen = al.agg.FitImaging(aggregator=agg_tmp)\n",
        "\n",
        "for fit in fit_gen:\n",
        "    aplt.FitImaging.subplot_fit_imaging(fit=fit)\n",
        "\n",
        "fit_gen = al.agg.FitImaging(aggregator=agg_power_law_shear)\n",
        "\n",
        "for fit in fit_gen:\n",
        "    aplt.FitImaging.subplot_fit_imaging(fit=fit)\n",
        "\n",
        "fit_gen = al.agg.FitImaging(aggregator=agg_power_law_hyper_with_shear)\n",
        "\n",
        "for fit in fit_gen:\n",
        "    aplt.FitImaging.subplot_fit_imaging(fit=fit)\n",
        "\n",
        "fit_gen = al.agg.FitImaging(agg_power_law_hyper_shear)\n",
        "\n",
        "for fit in fit_gen:\n",
        "    aplt.FitImaging.subplot_fit_imaging(fit=fit)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "          \n",
        "What if we want an aggregator which instead contains the 4 variant fits to 1 image? \n",
        "\n",
        "We can apply directory filtering using image names to achieve this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "agg_dataset_0 = agg_power_law.filter(\n",
        "    agg_power_law.directory.contains(\"mass_sie__source_bulge__0\")\n",
        ")\n",
        "\n",
        "fit_gen = al.agg.FitImaging(aggregator=agg_dataset_0)\n",
        "\n",
        "for fit in fit_gen:\n",
        "    aplt.FitImaging.subplot_fit_imaging(fit=fit)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}