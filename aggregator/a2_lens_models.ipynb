{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Aggregator 2: Lens Models__\n",
        "\n",
        "This tutorial builds on tutorial_1 of the aggregator autolens_workspace. Here, we use the aggregator to load models\n",
        "from a non-linear search and visualize and interpret results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import autofit as af\n",
        "import autolens as al\n",
        "import autolens.plot as aplt\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Frist, we set up the aggregator as we did in the previous tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "workspace_path = \"/home/jammy/PycharmProjects/PyAuto/autolens_workspace/\"\n",
        "output_path = workspace_path + \"output\"\n",
        "agg_results_path = output_path + \"/aggregator_sample_beginner\"\n",
        "\n",
        "af.conf.instance = af.conf.Config(\n",
        "    config_path=str(workspace_path + \"/config\"), output_path=str(output_path)\n",
        ")\n",
        "\n",
        "agg = af.Aggregator(directory=str(agg_results_path))\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, lets create a list of instances of the most-likely models of the final phase of each fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pipeline_name = \"pipeline__lens_sie__source_inversion\"\n",
        "phase_name = \"phase_3__source_inversion\"\n",
        "agg_phase_3 = agg.filter(phase=phase_name)\n",
        "outputs = agg_phase_3.output\n",
        "\n",
        "ml_instances = [out.most_likely_instance for out in outputs]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A model instance is a _Galaxy_ instance of the pipeline's _GalaxyModel_'s. So, its just a list of galaxies which we can \n",
        "pass to functions in PyAutoLens. Lets create the most-likely tracer of every fit..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ml_tracers = [\n",
        "    al.Tracer.from_galaxies(galaxies=instance.galaxies) for instance in ml_instances\n",
        "]\n",
        "\n",
        "print(\"Most Likely Tracers: \\n\")\n",
        "print(ml_tracers, \"\\n\")\n",
        "print(\"Total Tracers = \", len(ml_tracers))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "... and then plot their convergences.\n",
        "\n",
        "We'll just use a grid of 100 x 100 pixels for now, and cover later how we use the actual grid of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "grid = al.Grid.uniform(shape_2d=(100, 100), pixel_scales=0.1)\n",
        "\n",
        "for tracer in ml_tracers:\n",
        "    aplt.Tracer.convergence(tracer=tracer, grid=grid)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Okay, so we can make a list of tracers and plot their convergences. However, there is a problem with using lists, what\n",
        "if we fitted a lot of lenses? Imagine we had fitted hundreds of images, with hundreds of tracers - we'd quickly \n",
        "overload the memory on our laptop. Thats not good!\n",
        "\n",
        "Therefore, for aggregator use, we will avoid using lists for any objects that could potentially be memory intensive.\n",
        "Instead, we'll use generators, as shown below. If you're not familiar with generators, they may take you a bit of time \n",
        "to get your head round - but it'll be worth it.\n",
        "\n",
        "What is a generator? It essentially stores the objects as a means to generate it using a function, mapping the \n",
        "appropriate attributes of the aggregator along the way (e.g.model instances). Crucially, unlike a list, it performs \n",
        "each operation one-by-one, freeing up the memory used by each operation before performing the next.\n",
        "\"\"\"\n",
        "\n",
        "def make_tracer_generator(agg_obj):\n",
        "\n",
        "    output = agg_obj.output\n",
        "\n",
        "    # This uses the output of one instance to generate the tracer.\n",
        "    return al.Tracer.from_galaxies(\n",
        "        galaxies=output.most_likely_instance.galaxies\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# We \"map\" the function above using our aggregator to create a tracer generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tracer_gen = agg_phase_3.map(func=make_tracer_generator)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now iterate over our tracer generator to make the plots we desire. \n",
        "\n",
        "(We'll explain how to load the grid via the aggregator in the next tutorial)\n",
        "\"\"\"\n",
        "\n",
        "grid = al.Grid.uniform(shape_2d=(100, 100), pixel_scales=0.1)\n",
        "\n",
        "for tracer in tracer_gen:\n",
        "\n",
        "    aplt.Tracer.convergence(tracer=tracer, grid=grid)\n",
        "    aplt.Tracer.potential(tracer=tracer, grid=grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Its cumbersome always have to define a 'make_tracer_generator' function to make a tracer generator - give that you'll\n",
        "probably do the exact same thing in every Jupyter Notebook you ever write!\n",
        "\n",
        "PyAutoLens's aggregator module (accessed as 'agg') has a convenience method to save you time and make your notebooks\n",
        "cleaner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "tracer_gen = al.agg.Tracer(aggregator=agg_phase_3)\n",
        "\n",
        "for tracer in tracer_gen:\n",
        "    aplt.Tracer.convergence(tracer=tracer, grid=grid)\n",
        "    aplt.Tracer.potential(tracer=tracer, grid=grid)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because instances are just lists of galaxies we can directly extract attributes of the _Galaxy_ class. Lets print \n",
        "the Einstein mass of each of our most-likely lens galaxies.\n",
        "\n",
        "The model instance uses the model defined by a pipeline. In this pipeline, we called the lens galaxy 'lens'.\n",
        "\n",
        "For illustration, lets do this with a list first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Most Likely Lens Einstein Masses:\")\n",
        "for instance in ml_instances:\n",
        "    einstein_mass = instance.galaxies.lens.einstein_mass_in_units(\n",
        "            redshift_object=instance.galaxies.lens.redshift,\n",
        "            redshift_source=instance.galaxies.source.redshift\n",
        "        )\n",
        "    print(einstein_mass)\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets use a generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "def print_most_likely_mass(agg_obj):\n",
        "\n",
        "    output = agg_obj.output\n",
        "\n",
        "    einstein_mass = output.instance.galaxies.lens.einstein_mass_in_units(\n",
        "            redshift_object=output.instance.galaxies.lens.redshift,\n",
        "            redshift_source=output.instance.galaxies.source.redshift\n",
        "        )\n",
        "    print(einstein_mass)\n",
        "\n",
        "print(\"Most Likely Lens Einstein Masses:\")\n",
        "agg_phase_3.map(func=print_most_likely_mass)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets next do something a bit more ambitious. Lets create a plot of the einstein_radius vs axis_ratio of each SIE mass \n",
        "profile.\n",
        "\n",
        "These plots don't use anything too memory intensive - like a tracer - so we are fine to go back to lists for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mp_instances = [out.most_probable_instance for out in outputs]\n",
        "mp_einstein_radii = [\n",
        "    instance.galaxies.lens.mass.einstein_radius for instance in mp_instances\n",
        "]\n",
        "mp_axis_ratios = [instance.galaxies.lens.mass.axis_ratio for instance in mp_instances]\n",
        "\n",
        "print(mp_einstein_radii)\n",
        "print(mp_axis_ratios)\n",
        "\n",
        "plt.scatter(mp_einstein_radii, mp_axis_ratios, marker=\"x\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets also include error bars at 3 sigma confidence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ue3_instances = [out.error_instance_at_upper_sigma(sigma=3.0) for out in outputs]\n",
        "le3_instances = [out.error_instance_at_lower_sigma(sigma=3.0) for out in outputs]\n",
        "\n",
        "ue3_einstein_radii = [\n",
        "    instance.galaxies.lens.mass.einstein_radius for instance in ue3_instances\n",
        "]\n",
        "le3_einstein_radii = [\n",
        "    instance.galaxies.lens.mass.einstein_radius for instance in le3_instances\n",
        "]\n",
        "ue3_axis_ratios = [instance.galaxies.lens.mass.axis_ratio for instance in ue3_instances]\n",
        "le3_axis_ratios = [instance.galaxies.lens.mass.axis_ratio for instance in le3_instances]\n",
        "\n",
        "plt.errorbar(\n",
        "    x=mp_einstein_radii,\n",
        "    y=mp_axis_ratios,\n",
        "    marker=\".\", linestyle=\"\",\n",
        "    xerr=[le3_einstein_radii, ue3_einstein_radii],\n",
        "    yerr=[le3_axis_ratios, ue3_axis_ratios],\n",
        ")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, lets compute the errors on an attribute that wasn't a free parameter in our model fit. For example, getting \n",
        "the errors on an axis_ratio is simple, because it was sampled by MultiNest during the fit. Thus, to get errors on the \n",
        "axis ratio we simply marginalize over all over parameters to produce the 1D Probability Density Function (PDF).\n",
        "\n",
        "But what if we want the errors on the Einstein Mass? This wasn't a free parameter in our model so we can't just \n",
        "marginalize over all other parameters.\n",
        "\n",
        "Instead, we need to compute the Einstein mass of every lens model sampled by MultiNest and from this determine the \n",
        "PDF of the Einstein mass. When combining the different Einstein masses we weight each value by its MultiNest sampling \n",
        "probablity. This means that models which gave a poor fit to the data are downweighted appropriately.\n",
        "\n",
        "Below, we get an instance of every MultiNest sample using the MultiNestOutput, compute that models einstein mass, \n",
        "store them in a list and find the weighted median value with errors.\n",
        "\n",
        "This function takes the list of Einstein mass values with their sample weights and computed the weighted mean and \n",
        "standard deviation of these values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def weighted_mean_and_standard_deviation(values, weights):\n",
        "    \"\"\"\n",
        "    Return the weighted average and standard deviation.\n",
        "    values, weights -- Numpy ndarrays with the same shape.\n",
        "    \"\"\"\n",
        "    values = np.asarray(values)\n",
        "    weights = np.asarray(weights)\n",
        "    average = np.average(values, weights=weights)\n",
        "    # Fast and numerically precise:\n",
        "    variance = np.average((values - average) ** 2, weights=weights)\n",
        "    return (average, np.sqrt(variance))\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we iterate over each MultiNestOutput, extracting all samples and computing ther masses and weights and compute the \n",
        "weighted mean of these samples.\n",
        "\n",
        "Computing an Einstein mass takes a bit of time, so be warned this cell could run for a few minutes! To speed things \n",
        "up, you'll notice that we only perform the loop on samples whose probably is above 1.0e-4.\n",
        "\"\"\"\n",
        "\n",
        "def mass_error(agg_obj):\n",
        "\n",
        "    output = agg_obj.output\n",
        "\n",
        "    sample_masses = []\n",
        "    sample_weights = []\n",
        "\n",
        "    for sample_index in range(output.accepted_samples-1):\n",
        "\n",
        "        sample_weight = output.weight_from_sample_index(sample_index=sample_index)\n",
        "\n",
        "        if sample_weight > 1.0e-4:\n",
        "\n",
        "            instance = output.instance_from_sample_index(sample_index=sample_index)\n",
        "\n",
        "            einstein_mass = instance.galaxies.lens.einstein_mass_in_units(\n",
        "                redshift_object=instance.galaxies.lens.redshift,\n",
        "                redshift_source=instance.galaxies.source.redshift,\n",
        "            )\n",
        "\n",
        "            sample_masses.append(einstein_mass)\n",
        "            sample_weights.append(sample_weight)\n",
        "\n",
        "    return weighted_mean_and_standard_deviation(\n",
        "        values=sample_masses, weights=sample_weights\n",
        "    )\n",
        "\n",
        "einstein_masses, einstein_mass_errors = agg_phase_3.map(func=mass_error)\n",
        "\n",
        "print(\"Einstein Masses:\\n\")\n",
        "print(einstein_masses)\n",
        "print(\"Einstein Mass Errors\\n\")\n",
        "print(einstein_mass_errors)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}