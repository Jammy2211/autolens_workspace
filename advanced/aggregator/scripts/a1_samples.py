# %%
"""
__Aggregator 1: Samples__

After fitting a large suite of data with the same pipeline, the aggregator allows us to load the results and manipulate
/ plot them using a Python script or Jupyter notebook.

This script uses the results generated by the script '/autolens_workspace/aggregator/beginner_runner.py',
which fitted 3 simulated strong lenses. We fitted each image with the pipeline:

'autolens_workspace/pipelines/beginner/no_lens_light/lens_sie__source_inversion.py'

This pipeline is composed of 3 phases.

__File Output__

The results of this fit are in the '/output/aggregator_sample_beginner' folder. First, take a look in this folder.
Provided you haven't rerun the runner, you'll notice that all the results (e.g. samples, samples_backup,
model.results, images, etc.) are in .zip files as opposed to folders that can be instantly accessed.

This is because when the pipeline was run, the 'remove_files' option in the 'config/general.ini' was set to True.
This means all results (other than the .zip file) were removed. This feature is implemented because super-computers
often have a limit on the number of files allowed per user.

Bare in mind the fact that all results are in .zip files - we'll come back to this point in a second.

__Aggregator__

We can load the results of each pipeline's analysis of each of the 3 images using the aggregator. This will allow us to
manipulate the results in this Python script (or a Jupyter notebook) to plot figures, interpret results, check specific
values, etc.
"""

# %%
from autoconf import conf
import autofit as af

# %%
"""
To begin, we setup the path to the output path we want to load results from, which in this case is the folder 
'autolens_workspace/output/aggregator_sample_beginner'.
"""

# %%
workspace_path = "/home/jammy/PycharmProjects/PyAuto/autolens_workspace"
output_path = f"{workspace_path}/output"
agg_results_path = f"{output_path}/aggregator/beginner"

# %%
"""
Now we'll use this path to explicitly set the config path and output path.
"""

# %%
conf.instance = conf.Config(
    config_path=f"{workspace_path}/config", output_path=output_path
)

# %%
"""
To set up the aggregator we simply pass it the folder of the results we want to load.
"""

# %%
agg = af.Aggregator(directory=str(agg_results_path))

# %%
"""
Before we continue, take another look at the output folder. The .zip files containing results have now all been 
unzipped, such that the results are accessible on your laptop for navigation. This means you can run fits to many 
lenses on a super computer and easily unzip all the results on your computer afterwards via the aggregator.
"""

# %%
"""
To begin, let me quickly explain what a generator is in Python, for those unaware. A generator is an object that 
iterates over a function when it is called. The aggregator creates all objects as generators, rather than lists, or 
dictionaries, or whatever.

Why? Because lists store every entry in memory simultaneously. If you fit many lenses, you'll have lots of results and 
therefore use a lot of memory. This will crash your laptop! On the other hand, a generator only stores the object in 
memory when it runs the function; it is free to overwrite it afterwards. This, your laptop won't crash!

There are two things to bare in mind with generators:

1) A generator has no length, thus to determine how many entries of data it corresponds to you first must turn it to a 
list.

2) Once we use a generator, we cannot use it again - we'll need to remake it.

We can now create a 'samples' generator of every fit. An instance of the Samples class acts as an 
interface between the results of the non-linear fit on your hard-disk and Python.
"""

# %%
samples_gen = agg.values("samples")

# %%
"""
When we print this the length of this generator converted to a list of outputs we see 9 different NestSamples 
instances. These correspond to all 3 phases of each pipeline's fit to all 3 images.
"""

# %%
print("MultiNest Samples: \n")
print(samples_gen)
print()
print("Total Samples Objects = ", len(list(samples_gen)), "\n")

# %%
"""
The Samples class contains all the parameter samples, which is a list of lists where:
 
 - The outer list is the size of the total number of samples.
 - The inner list is the size of the number of free parameters in the fit.
 
"""

# %%
for samples in agg.values("samples"):

    print("All parameters of the very first sample")
    print(samples.parameters[0])
    print("The fourth parameter of the tenth sample")
    print(samples.parameters[9][3])


print("MultiNest Samples: \n")
print(samples_gen)
print()
print("Total Samples Objects = ", len(list(samples_gen)), "\n")

# %%
"""
The Samples class all contains the log likelihood, log prior, log posterior and weights of every sample, where:

   - The log likelihood is the value evaluated from the likelihood function (e.g. -0.5 * chi_squared + the noise 
     normalized).
    
    - The log prior encodes information on how the priors on the parameters maps the log likelihood value to the log
      posterior value.
      
    - The log posterior is log_likelihood + log_prior.
    
    - The weight gives information on how samples should be combined to estimate the posterior. The weight values 
      depend on the sampler used, for example for MCMC they will all be 1's.

"""

# %%
for samples in agg.values("samples"):
    print("log(likelihood), log(prior), log(posterior) and weight of the tenth sample.")
    print(samples.log_likelihoods[9])
    print(samples.log_priors[9])
    print(samples.log_posteriors[9])
    print(samples.weights[9])

# %%
"""
If we want to remove the fits of the first 2 phases and just keep the NestSampless of the 3rd and final phase 
of the pipeline we can do so by filtering for the phase's name.
"""

# %%
phase_name = "phase_3__source_inversion"
agg_filter = agg.filter(agg.phase == phase_name)
samples_gen = agg_filter.values("samples")

# %%
"""
As expected, this list now has only 3 NestSampless, one for each image we fitted (note how in the bottom print
statement we remake the generator using the aggregator).
"""

# %%
print("Phase Name Filtered MultiNest Samples: \n")
print(list(samples_gen))
print()
print("Ttotal Samples Objects = ", len(list(agg_filter.values("samples"))), "\n")

# %%
"""
In this example, we only fitted the 3 images using one pipeline. But suppose we used multiple pipelines, like we do in 
the advanced pipelines. In this case, the aggregator would load the NestSampless of all fits of all phases of all 
pipelines!

In such circumstances, we can filter by phase name and pipeline name.
"""

# %%
pipeline_name = "pipeline__lens_sie__source_inversion"
agg_filter = agg.filter(agg.phase == phase_name, agg.pipeline == pipeline_name)
samples_gen = agg_filter.values("samples")

# %%
"""
As expected, this list again has 3 NestSampless.
"""

# %%
print("Pipeline Name Filtered MultiNest Samples: \n")
print(samples_gen)
print()
print("Ttotal Samples Objects = ", len(list(agg_filter.values("samples"))), "\n")

# %%
"""
We can use the outputs to create a list of the most-likely (e.g. highest log likelihood) model of each fit to our three 
images (in this case in phase 3).
"""

# %%
ml_vector = [samps.max_log_likelihood_vector for samps in agg_filter.values("samples")]

print("Most Likely Model Parameter Lists: \n")
print(ml_vector, "\n")

# %%
"""
This provides us with lists of all model parameters. However, this isn't that much use - which values correspond to 
which parameters?

Its more useful to create the model instance of every fit.
"""

# %%
ml_instances = [
    samps.max_log_likelihood_instance for samps in agg_filter.values("samples")
]
print("Most Likely Model Instances: \n")
print(ml_instances, "\n")

# %%
"""
A model instance contains all the model components of our fit - most importantly the list of galaxies we specified in the pipeline.
"""

# %%
print(ml_instances[0].galaxies)
print(ml_instances[1].galaxies)
print(ml_instances[2].galaxies)

# %%
"""
These galaxies will be named according to the pipeline (in this case, 'lens' and 'source').
"""

# %%
print(ml_instances[0].galaxies.lens)
print()
# print(ml_instances[1].galaxies.source)

# %%
"""
Their light and _MassProfile_s are also named according to the pipeline.
"""

# %%
print(ml_instances[0].galaxies.lens.mass)
print()
print(ml_instances[0].galaxies.lens.shear)
print()
print(ml_instances[1].galaxies.source.pixelization)
print()
print(ml_instances[1].galaxies.source.regularization)

# %%
"""
We can also access the 'most probable' model, which is the model computed by marginalizing over the MultiNest samples 
of every parameter in 1D and taking the median of this PDF.
"""

# %%
mp_vector = [samps.median_pdf_vector for samps in agg_filter.values("samples")]
mp_instances = [samps.median_pdf_instance for samps in agg_filter.values("samples")]

print("Most Probable Model Parameter Lists: \n")
print(mp_vector, "\n")
print("Most probable Model Instances: \n")
print(mp_instances, "\n")
print(mp_instances[0].galaxies.lens.mass)
print(mp_instances[0].galaxies.lens.shear)
print()

# %%
"""
We can compute the model parameters at a given sigma value (e.g. at 3.0 sigma limits).

These parameter values do not account for covariance between the model. For example if two parameters are degenerate 
this will find their values from the degeneracy in the 'same direction' (e.g. both will be positive). 

# Here, I use "uv3" to signify this is an upper value at 3 sigma confidence,, and "lv3" for the lower value.
"""

# %%
uv3_vectors = [
    samps.vector_at_upper_sigma(sigma=3.0) for samps in agg_filter.values("samples")
]

uv3_instances = [
    samps.instance_at_upper_sigma(sigma=3.0) for samps in agg_filter.values("samples")
]

lv3_vectors = [
    samps.vector_at_lower_sigma(sigma=3.0) for samps in agg_filter.values("samples")
]

lv3_instances = [
    samps.instance_at_lower_sigma(sigma=3.0) for samps in agg_filter.values("samples")
]

print("Errors Lists: \n")
print(uv3_vectors, "\n")
print(lv3_vectors, "\n")
print("Errors Instances: \n")
print(uv3_instances, "\n")
print(lv3_instances, "\n")

# %%
"""
We can compute the upper and lower errors on each parameter at a given sigma limit.

Here, "ue3" signifies the upper error at 3 sigma. 
"""

# %%
# These do not currently work due to a bug which will be fixed on Friday

ue3_vectors = [
    samps.error_vector_at_upper_sigma(sigma=3.0)
    for samps in agg_filter.values("samples")
]

ue3_instances = [
    samps.error_instance_at_upper_sigma(sigma=3.0)
    for samps in agg_filter.values("samples")
]

le3_vectors = [
    samps.error_vector_at_lower_sigma(sigma=3.0)
    for samps in agg_filter.values("samples")
]
le3_instances = [
    samps.error_instance_at_lower_sigma(sigma=3.0)
    for samps in agg_filter.values("samples")
]

print("Errors Lists: \n")
print(ue3_vectors, "\n")
print(le3_vectors, "\n")
print("Errors Instances: \n")
print(ue3_instances, "\n")
print(le3_instances, "\n")

# %%
"""
The maximum log likelihood of each model fit and its Bayesian log evidence (estimated via MultiNest) are also available.

Given each fit is to a different image, these are not very useful. However, in tutorial 5 we'll look at using the 
aggregator for images that we fit with many different models and many different pipelines, in which case comparing 
the evidences allows us to perform Bayesian model comparison!
"""

# %%
print("Likelihoods: \n")
print([samps.max_log_likelihood for samps in agg_filter.values("samples")])
print([samps.log_evidence for samps in agg_filter.values("samples")])

# %%
"""
We can also print the "model_results" of all phases, which is string that summarizes every fit's lens model providing 
quick inspection of all results.
"""

# %%
results = agg_filter.model_results
print("Model Results Summary: \n")
print(results, "\n")

# %%
